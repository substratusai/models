{"prompt": "Write YAML that defines a Kubernetes Deployment named \"iis\" with 3 replicas. The Deployment selects pods based on the label \"app=iis\". The pod template includes a container named \"iis\" that uses the \"microsoft/iis\" image and exposes port 80 within the container. Additionally, it includes an annotation specifying the isolation type as \"hyperv\" for experimental purposes.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: iis\nspec:\n  selector:\n    matchLabels:\n      app: iis\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: iis\n      annotations:\n        experimental.windows.kubernetes.io/isolation-type: hyperv\n    spec:\n      containers:\n      - name: iis\n        image: microsoft/iis\n        ports:\n        - containerPort: 80\n\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod resource named \"iis\" with the image set to \"microsoft/iis:windowsservercore-1709\" and the container listening on port 80. The Pod also includes a nodeSelector that ensures the Pod is scheduled on a node with the \"kubernetes.io/os\" label set to \"windows\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: iis\n  labels:\n    name: iis\nspec:\n  containers:\n    - name: iis\n      image: microsoft/iis:windowsservercore-1709\n      ports:\n        - containerPort: 80\n  nodeSelector:\n    \"kubernetes.io/os\": windows\n\n```"}
{"prompt": "Write YAML that describes a Deployment object for an IIS application in Kubernetes. It specifies that there should be 3 replicas of the deployment and the selector should match the labels with the \"app\" value set to \"iis\". The template within the deployment describes the containers to be deployed, with the name set to \"iis\" and the image being \"microsoft/iis\". It also sets resource limits for memory to \"128Mi\" and CPU to 2, and specifies that port 80 should be exposed within the container.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: iis\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: iis\n  template:\n    metadata:\n      labels:\n        app: iis\n    spec:\n      containers:\n      - name: iis\n        image: microsoft/iis\n        resources:\n          limits:\n            memory: \"128Mi\"\n            cpu: 2\n        ports:\n        - containerPort: 80\n\n\n```"}
{"prompt": "Write YAML that creates a DaemonSet named \"my-daemonset\" with labels \"app: foo\". The DaemonSet will only be scheduled on nodes that have the label \"kubernetes.io/os: windows\". The DaemonSet will run a container named \"foo\" using the image \"microsoft/windowsservercore:1709\".", "completion": "```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: my-daemonset\n  labels:\n    app: foo\nspec:\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - name: foo\n        image: microsoft/windowsservercore:1709\n      nodeSelector:\n        kubernetes.io/os: windows\n\n\n```"}
{"prompt": "Write YAML that creates a Secret named \"mysecret\" of type Opaque with the data stored in base64 encoding for the \"username\" and \"password\" keys. \n\nThen write YAML that creates a Pod named \"my-secret-pod\" with a container named \"my-secret-pod\" running the \"microsoft/windowsservercore:1709\" image. The container gets its USERNAME and PASSWORD environment variables from the \"mysecret\" Secret by referencing the respective keys. The Pod is scheduled on a node with the label \"kubernetes.io/os\" set to \"windows\".", "completion": "```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: MWYyZDFlMmU2N2Rm\n\n---\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-secret-pod\nspec:\n  containers:\n  - name: my-secret-pod\n    image: microsoft/windowsservercore:1709\n    env:\n      - name: USERNAME\n        valueFrom:\n          secretKeyRef:\n            name: mysecret\n            key: username\n      - name: PASSWORD\n        valueFrom:\n          secretKeyRef:\n            name: mysecret\n            key: password\n  nodeSelector:\n    kubernetes.io/os: windows\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"my-empty-dir-pod\" using the v1 API version. The Pod runs a container based on the \"microsoft/windowsservercore:1709\" image with the same name. Two emptyDir volumes are defined: \"cache-volume\" mounted to \"/cache\" and \"scratch-volume\" mounted to \"C:/scratch\". The Pod is scheduled on a Windows node based on the nodeSelector configuration.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-empty-dir-pod\nspec:\n  containers:\n  - image: microsoft/windowsservercore:1709\n    name: my-empty-dir-pod\n    volumeMounts:\n    - mountPath: /cache\n      name: cache-volume\n    - mountPath: C:/scratch\n      name: scratch-volume\n  volumes:\n  - name: cache-volume\n    emptyDir: {}\n  - name: scratch-volume\n    emptyDir: {}\n  nodeSelector:\n    kubernetes.io/os: windows\n\n```"}
{"prompt": "Write YAML that defines a ConfigMap named \"example-config\" with two properties: \"example.property.1\" that has the value \"hello\" and \"example.property.2\" that has the value \"world\". Then define a Pod named \"configmap-pod\" that runs a container using the image \"redis:3.0-nanoserver\" on a Windows node. The container has two environment variables: \"EXAMPLE_PROPERTY_1\" that takes its value from \"example.property.1\" in the ConfigMap and \"EXAMPLE_PROPERTY_2\" that takes its value from \"example.property.2\" in the ConfigMap.", "completion": "```yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: example-config\ndata:\n  example.property.1: hello\n  example.property.2: world\n\n---\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-pod\nspec:\n  containers:\n  - name: configmap-redis\n    image: redis:3.0-nanoserver\n    env:\n      - name: EXAMPLE_PROPERTY_1\n        valueFrom:\n          configMapKeyRef:\n            name: example-config\n            key: example.property.1\n      - name: EXAMPLE_PROPERTY_2\n        valueFrom:\n          configMapKeyRef:\n            name: example-config\n            key: example.property.2\n  nodeSelector:\n    kubernetes.io/os: windows\n```"}
{"prompt": "Write YAML that defines a Pod resource named \"run-as-username-container-demo\". The Pod has a security context with a windowsOptions field that sets the runAsUserName to \"ContainerUser\". It also has a container named \"run-as-username-demo\" with an image, command, and a security context that sets the runAsUserName to \"ContainerAdministrator\". The Pod is targeted to nodes with the label kubernetes.io/os set to \"windows\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: run-as-username-container-demo\nspec:\n  securityContext:\n    windowsOptions:\n      runAsUserName: \"ContainerUser\"\n  containers:\n  - name: run-as-username-demo\n    image: mcr.microsoft.com/windows/servercore:ltsc2019\n    command: [\"ping\", \"-t\", \"localhost\"]\n    securityContext:\n        windowsOptions:\n            runAsUserName: \"ContainerAdministrator\"\n  nodeSelector:\n    kubernetes.io/os: windows\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"run-as-username-pod-demo\". This Pod runs on a Windows operating system and uses the container image \"mcr.microsoft.com/windows/servercore:ltsc2019\". The Pod has a security context that specifies the user \"ContainerUser\" to run the container as. Additionally, the Pod has a node selector that ensures it is scheduled on a Windows node.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: run-as-username-pod-demo\nspec:\n  securityContext:\n    windowsOptions:\n      runAsUserName: \"ContainerUser\"\n  containers:\n  - name: run-as-username-demo\n    image: mcr.microsoft.com/windows/servercore:ltsc2019\n    command: [\"ping\", \"-t\", \"localhost\"]\n  nodeSelector:\n    kubernetes.io/os: windows\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"hostpath-volume-pod\" using the v1 API version. The Pod runs the \"microsoft/windowsservercore:1709\" image, and mounts a volume named \"foo\" at the path \"C:\\etc\\foo\" with read-only access. It also specifies that the Pod should run on a node with the label \"kubernetes.io/os: windows\" and includes the volume configuration for \"foo\" using a hostPath with the path \"C:\\etc\\foo\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath-volume-pod\nspec:\n  containers:\n  - name: my-hostpath-volume-pod\n    image: microsoft/windowsservercore:1709\n    volumeMounts:\n    - name: foo\n      mountPath: \"C:\\\\etc\\\\foo\"\n      readOnly: true\n  nodeSelector:\n    kubernetes.io/os: windows\n  volumes:\n  - name: foo\n    hostPath:\n     path: \"C:\\\\etc\\\\foo\"\n\n```"}
{"prompt": "Write YAML that defines a ClusterRole with annotations, labels, and rules. The annotations provide a description explaining the purpose of this ClusterRole and the reasons for its removal. The labels specify that this role is meant to be aggregated to the \"edit\" role. The rules define the permissions for the role, allowing the creation, deletion, patching, and updating of \"endpoints\" resources.", "completion": "```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    kubernetes.io/description: |-\n      Add endpoints write permissions to the edit and admin roles. This was\n      removed by default in 1.22 because of CVE-2021-25740. See\n      https://issue.k8s.io/103675. This can allow writers to direct LoadBalancer\n      or Ingress implementations to expose backend IPs that would not otherwise\n      be accessible, and can circumvent network policies or security controls\n      intended to prevent/isolate access to those backends.\n      EndpointSlices were never included in the edit or admin roles, so there\n      is nothing to restore for the EndpointSlice API.\n  labels:\n    rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n  name: custom:aggregate-to-edit:endpoints # you can change this if you wish\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"endpoints\"]\n    verbs: [\"create\", \"delete\", \"deletecollection\", \"patch\", \"update\"]\n\n```"}
{"prompt": "Write YAML that defines a validating admission policy with the following specifications: it has an API version of admissionregistration.k8s.io/v1alpha1, a kind of ValidatingAdmissionPolicy, a name of \"demo-policy.example.com\", a failure policy of Fail, and match constraints that allow CREATE and UPDATE operations on all resources. The policy also includes three match conditions: one to exclude lease resources, one to exclude requests made by non-node users, and one to skip RBAC requests. Lastly, the policy has a validation expression that checks if the object's metadata name does not contain 'demo' or if the metadata namespace is 'demo'.", "completion": "```yaml\napiVersion: admissionregistration.k8s.io/v1alpha1\nkind: ValidatingAdmissionPolicy\nmetadata:\n  name: \"demo-policy.example.com\"\nspec:\n  failurePolicy: Fail\n  matchConstraints:\n    resourceRules:\n      - apiGroups:   [\"*\"]\n        apiVersions: [\"*\"]\n        operations:  [\"CREATE\", \"UPDATE\"]\n        resources:   [\"*\"]\n  matchConditions:\n    - name: 'exclude-leases' # Each match condition must have a unique name\n      expression: '!(request.resource.group == \"coordination.k8s.io\" && request.resource.resource == \"leases\")' # Match non-lease resources.\n    - name: 'exclude-kubelet-requests'\n      expression: '!(\"system:nodes\" in request.userInfo.groups)' # Match requests made by non-node users.\n    - name: 'rbac' # Skip RBAC requests.\n      expression: 'request.resource.group != \"rbac.authorization.k8s.io\"'\n  validations:\n    - expression: \"!object.metadata.name.contains('demo') || object.metadata.namespace == 'demo'\"\n\n\n```"}
{"prompt": "Write YAML that defines a validating admission policy for Kubernetes. The policy is named \"demo-policy.example.com\" and specifies that failures should result in the request being rejected. It also includes a resource rule that matches deployments in the \"apps\" API group and version 1, allowing create and update operations. Additionally, it defines a validation rule called \"high-replica-count\" with an expression that checks if the number of replicas is greater than 50, and a message expression that constructs a message including the current replica count.", "completion": "```yaml\napiVersion: admissionregistration.k8s.io/v1alpha1\nkind: ValidatingAdmissionPolicy\nmetadata:\n  name: \"demo-policy.example.com\"\nspec:\n  failurePolicy: Fail\n  matchConstraints:\n    resourceRules:\n    - apiGroups:   [\"apps\"]\n      apiVersions: [\"v1\"]\n      operations:  [\"CREATE\", \"UPDATE\"]\n      resources:   [\"deployments\"]\n  validations:\n    - key: \"high-replica-count\"\n      expression: \"object.spec.replicas > 50\"\n      messageExpression: \"'Deployment spec.replicas set to ' + string(object.spec.replicas)\"\n\n```"}
{"prompt": "Write YAML that defines a validating admission policy called \"deploy-replica-policy.example.com\" for Kubernetes, using the admissionregistration.k8s.io/v1alpha1 API version. It specifies a parameter kind called \"ReplicaLimit\" with an API version of rules.example.com/v1. The policy applies to deployments in the \"apps\" API group, version \"v1\", with create and update operations. It validates that the number of replicas specified in the deployment does not exceed the maximum allowed replicas defined by the parameter. If the validation fails, an error message is returned with the reason \"Invalid\".", "completion": "```yaml\napiVersion: admissionregistration.k8s.io/v1alpha1\nkind: ValidatingAdmissionPolicy\nmetadata:\n  name: \"deploy-replica-policy.example.com\"\nspec:\n  paramKind:\n    apiVersion: rules.example.com/v1\n    kind: ReplicaLimit\n  matchConstraints:\n    resourceRules:\n    - apiGroups:   [\"apps\"]\n      apiVersions: [\"v1\"]\n      operations:  [\"CREATE\", \"UPDATE\"]\n      resources:   [\"deployments\"]\n  validations:\n  - expression: \"object.spec.replicas <= params.maxReplicas\"\n    messageExpression: \"'object.spec.replicas must be no greater than ' + string(params.maxReplicas)\"\n    reason: Invalid\n\n\n```"}
{"prompt": "Write YAML that defines a ClusterRole named \"csr-creator\" with apiVersion as rbac.authorization.k8s.io/v1. The role allows the creation, getting, listing, and watching of certificatesigningrequests resources from the certificates.k8s.io API group.", "completion": "```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: csr-creator\nrules:\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests\n  verbs:\n  - create\n  - get\n  - list\n  - watch\n\n```"}
{"prompt": "Write YAML that defines a ClusterRole named 'csr-signer' with rules for authorizing actions related to certificate signing requests (CSRs). The ClusterRole allows the user to get, list, and watch for CSRs, update the status of CSRs, and sign certificates using a specific signer named 'example.com/my-signer-name'. The YAML also specifies the version and kind as 'rbac.authorization.k8s.io/v1' and 'ClusterRole' respectively.", "completion": "```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: csr-signer\nrules:\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests/status\n  verbs:\n  - update\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - signers\n  resourceNames:\n  - example.com/my-signer-name # example.com/* can be used to authorize for all signers in the 'example.com' domain\n  verbs:\n  - sign\n\n```"}
{"prompt": "Write YAML that defines a ClusterRole named \"csr-approver\" with the following permissions: \n- Allows getting, listing, and watching certificate signing requests (CSR) from the \"certificates.k8s.io\" API group.\n- Allows updating the \"approval\" resource of certificate signing requests from the \"certificates.k8s.io\" API group.\n- Allows approving specific signers from the \"signers\" resource of the \"certificates.k8s.io\" API group, specifically the signer with the name \"example.com/my-signer-name\".", "completion": "```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: csr-approver\nrules:\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests/approval\n  verbs:\n  - update\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - signers\n  resourceNames:\n  - example.com/my-signer-name # example.com/* can be used to authorize for all signers in the 'example.com' domain\n  verbs:\n  - approve\n\n\n```"}
{"prompt": "Write YAML that specifies the API version and kind of the resource as Pod. The metadata section defines the name of the pod as \"counter\". The spec section defines the container within the pod with the name \"count\" and uses the BusyBox image with tag 1.28. The container runs the given command in an infinite loop to increment a counter and print the counter value with the current date.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox:1.28\n    args: [/bin/sh, -c,\n            'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 1; done']\n\n```"}
{"prompt": "Write YAML that creates a ServiceAccount named \"event-exporter-sa\" in the \"default\" namespace with the label \"app: event-exporter\". \nWrite YAML that creates a ClusterRoleBinding named \"event-exporter-rb\" with the label \"app: event-exporter\". It assigns the ClusterRole \"view\" to the ServiceAccount \"event-exporter-sa\" in the \"default\" namespace. \nWrite YAML that creates a Deployment named \"event-exporter-v0.2.3\" in the \"default\" namespace with the label \"app: event-exporter\". It uses the ServiceAccount \"event-exporter-sa\" and deploys a container with the image \"registry.k8s.io/event-exporter:v0.2.3\" and the command \"/event-exporter\".", "completion": "```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: event-exporter-sa\n  namespace: default\n  labels:\n    app: event-exporter\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: event-exporter-rb\n  labels:\n    app: event-exporter\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: view\nsubjects:\n- kind: ServiceAccount\n  name: event-exporter-sa\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: event-exporter-v0.2.3\n  namespace: default\n  labels:\n    app: event-exporter\nspec:\n  selector:\n    matchLabels:\n      app: event-exporter\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: event-exporter\n    spec:\n      serviceAccountName: event-exporter-sa\n      containers:\n      - name: event-exporter\n        image: registry.k8s.io/event-exporter:v0.2.3\n        command:\n        - '/event-exporter'\n      terminationGracePeriodSeconds: 30\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"termination-demo\" with a single container named \"termination-demo-container\" using the image \"debian\". The container runs the command \"/bin/sh\" with arguments \"-c\" and \"sleep 10 && echo Sleep expired > /dev/termination-log\". This command sleeps for 10 seconds and then writes \"Sleep expired\" to the file \"/dev/termination-log\" before exiting.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: termination-demo\nspec:\n  containers:\n  - name: termination-demo-container\n    image: debian\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"sleep 10 && echo Sleep expired > /dev/termination-log\"]\n\n```"}
{"prompt": "Write YAML that creates a DaemonSet object in the `kube-system` namespace called `node-problem-detector-v0.1` with the following labels: `k8s-app: node-problem-detector`, `version: v0.1`, and `kubernetes.io/cluster-service: \"true\"`. The DaemonSet runs a single container named `node-problem-detector` with the image `registry.k8s.io/node-problem-detector:v0.1`. The container has resource limits of 200m CPU and 100Mi memory, and resource requests of 20m CPU and 20Mi memory. It also mounts two volumes, `/log` and `/config`, both of which are read-only. The `log` volume is a hostPath volume that maps to the host's `/var/log/` directory, and the `config` volume is a ConfigMap volume called `node-problem-detector-config`.\n", "completion": "```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-problem-detector-v0.1\n  namespace: kube-system\n  labels:\n    k8s-app: node-problem-detector\n    version: v0.1\n    kubernetes.io/cluster-service: \"true\"\nspec:\n  selector:\n    matchLabels:\n      k8s-app: node-problem-detector  \n      version: v0.1\n      kubernetes.io/cluster-service: \"true\"\n  template:\n    metadata:\n      labels:\n        k8s-app: node-problem-detector\n        version: v0.1\n        kubernetes.io/cluster-service: \"true\"\n    spec:\n      hostNetwork: true\n      containers:\n      - name: node-problem-detector\n        image: registry.k8s.io/node-problem-detector:v0.1\n        securityContext:\n          privileged: true\n        resources:\n          limits:\n            cpu: \"200m\"\n            memory: \"100Mi\"\n          requests:\n            cpu: \"20m\"\n            memory: \"20Mi\"\n        volumeMounts:\n        - name: log\n          mountPath: /log\n          readOnly: true\n        - name: config # Overwrite the config/ directory with ConfigMap volume\n          mountPath: /config\n          readOnly: true\n      volumes:\n      - name: log\n        hostPath:\n          path: /var/log/\n      - name: config # Define ConfigMap volume\n        configMap:\n          name: node-problem-detector-config\n```"}
{"prompt": "Write YAML that describes a Kubernetes DaemonSet named \"fluentd-gcp-v2.0\" with a rolling update strategy. The DaemonSet has a single container named \"fluentd-gcp\" running the \"fluentd-gcp:2.0.2\" image. The container has resource limits and requests defined for CPU and memory. It also has volume mounts for /var/log, /var/lib/docker/containers, /host/lib, and /etc/fluent/config.d. The DaemonSet also sets a liveness probe that checks if fluentd is properly functioning. Additionally, it has nodeSelector defined and tolerations set for fluentd-ds-ready. The YAML also includes volume definitions for /var/log, /var/lib/docker/containers, /usr/lib64, and a configMap named \"fluentd-gcp-config\".", "completion": "```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd-gcp-v2.0\n  labels:\n    k8s-app: fluentd-gcp\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    version: v2.0\nspec:\n  selector:\n    matchLabels:\n      k8s-app: fluentd-gcp\n      kubernetes.io/cluster-service: \"true\"\n      version: v2.0\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        k8s-app: fluentd-gcp\n        kubernetes.io/cluster-service: \"true\"\n        version: v2.0\n      # This annotation ensures that fluentd does not get evicted if the node\n      # supports critical pod annotation based priority scheme.\n      # Note that this does not guarantee admission on the nodes (#40573).\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      dnsPolicy: Default\n      containers:\n      - name: fluentd-gcp\n        image: registry.k8s.io/fluentd-gcp:2.0.2\n        # If fluentd consumes its own logs, the following situation may happen:\n        # fluentd fails to send a chunk to the server => writes it to the log =>\n        # tries to send this message to the server => fails to send a chunk and so on.\n        # Writing to a file, which is not exported to the back-end prevents it.\n        # It also allows to increase the fluentd verbosity by default.\n        command:\n          - '/bin/sh'\n          - '-c'\n          - '/run.sh $FLUENTD_ARGS 2>&1 >>/var/log/fluentd.log'\n        env:\n        - name: FLUENTD_ARGS\n          value: --no-supervisor\n        resources:\n          limits:\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: libsystemddir\n          mountPath: /host/lib\n          readOnly: true\n        - name: config-volume\n          mountPath: /etc/fluent/config.d\n        # Liveness probe is aimed to help in situations where fluentd\n        # silently hangs for no apparent reasons until manual restart.\n        # The idea of this probe is that if fluentd is not queueing or\n        # flushing chunks for 5 minutes, something is not right. If\n        # you want to change the fluentd configuration, reducing amount of\n        # logs fluentd collects, consider changing the threshold or turning\n        # liveness probe off completely.\n        livenessProbe:\n          initialDelaySeconds: 600\n          periodSeconds: 60\n          exec:\n            command:\n            - '/bin/sh'\n            - '-c'\n            - >\n              LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300};\n              STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900};\n              if [ ! -e /var/log/fluentd-buffers ];\n              then\n                exit 1;\n              fi;\n              LAST_MODIFIED_DATE=`stat /var/log/fluentd-buffers | grep Modify | sed -r \"s/Modify: (.*)/\\1/\"`;\n              LAST_MODIFIED_TIMESTAMP=`date -d \"$LAST_MODIFIED_DATE\" +%s`;\n              if [ `date +%s` -gt `expr $LAST_MODIFIED_TIMESTAMP + $STUCK_THRESHOLD_SECONDS` ];\n              then\n                rm -rf /var/log/fluentd-buffers;\n                exit 1;\n              fi;\n              if [ `date +%s` -gt `expr $LAST_MODIFIED_TIMESTAMP + $LIVENESS_THRESHOLD_SECONDS` ];\n              then\n                exit 1;\n              fi;\n      nodeSelector:\n        beta.kubernetes.io/fluentd-ds-ready: \"true\"\n      tolerations:\n      - key: \"node.alpha.kubernetes.io/ismaster\"\n        effect: \"NoSchedule\"\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: libsystemddir\n        hostPath:\n          path: /usr/lib64\n      - name: config-volume\n        configMap:\n          name: fluentd-gcp-config\n\n```"}
{"prompt": "Write YAML that defines a DaemonSet named \"node-problem-detector-v0.1\" running in the \"kube-system\" namespace with labels \"k8s-app: node-problem-detector\", \"version: v0.1\", and \"kubernetes.io/cluster-service: true\". The DaemonSet should have a selector that matches labels \"k8s-app: node-problem-detector\", \"version: v0.1\", and \"kubernetes.io/cluster-service: true\". The DaemonSet should run a container named \"node-problem-detector\" using the image \"registry.k8s.io/node-problem-detector:v0.1\". The container should have securityContext set to \"privileged: true\", resource limits of \"200m\" CPU and \"100Mi\" memory, and resource requests of \"20m\" CPU and \"20Mi\" memory. It should mount the \"log\" volume at \"/log\" as read-only. The volume should use the hostPath with path \"/var/log/\".", "completion": "```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-problem-detector-v0.1\n  namespace: kube-system\n  labels:\n    k8s-app: node-problem-detector\n    version: v0.1\n    kubernetes.io/cluster-service: \"true\"\nspec:\n  selector:\n    matchLabels:\n      k8s-app: node-problem-detector  \n      version: v0.1\n      kubernetes.io/cluster-service: \"true\"\n  template:\n    metadata:\n      labels:\n        k8s-app: node-problem-detector\n        version: v0.1\n        kubernetes.io/cluster-service: \"true\"\n    spec:\n      hostNetwork: true\n      containers:\n      - name: node-problem-detector\n        image: registry.k8s.io/node-problem-detector:v0.1\n        securityContext:\n          privileged: true\n        resources:\n          limits:\n            cpu: \"200m\"\n            memory: \"100Mi\"\n          requests:\n            cpu: \"20m\"\n            memory: \"20Mi\"\n        volumeMounts:\n        - name: log\n          mountPath: /log\n          readOnly: true\n      volumes:\n      - name: log\n        hostPath:\n          path: /var/log/\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod resource named \"example-conflict-with-limitrange-cpu\". The Pod has one container named \"demo\" which uses the image \"registry.k8s.io/pause:2.0\". The container has a resource request of 700 milliCPU units.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-conflict-with-limitrange-cpu\nspec:\n  containers:\n  - name: demo\n    image: registry.k8s.io/pause:2.0\n    resources:\n      requests:\n        cpu: 700m\n\n```"}
{"prompt": "Write YAML that defines a LimitRange with the following specifications: default CPU limit of 500 milliCPU, default CPU request of 500 milliCPU, maximum CPU limit of 1 CPU, minimum CPU limit of 100 milliCPU, and the type of the LimitRange is set to Container. The LimitRange is named \"cpu-resource-constraint\".", "completion": "```yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-resource-constraint\nspec:\n  limits:\n  - default: # this section defines default limits\n      cpu: 500m\n    defaultRequest: # this section defines default requests\n      cpu: 500m\n    max: # max and min define the limit range\n      cpu: \"1\"\n    min:\n      cpu: 100m\n    type: Container\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod with the name 'example-no-conflict-with-limitrange-cpu'. The Pod contains one container named 'demo' that uses the 'pause:2.0' image from the Kubernetes registry. The Pod has resource limits and requests set for CPU, with a request of 700 milliCPU and a limit of 700 milliCPU.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-no-conflict-with-limitrange-cpu\nspec:\n  containers:\n  - name: demo\n    image: registry.k8s.io/pause:2.0\n    resources:\n      requests:\n        cpu: 700m\n      limits:\n        cpu: 700m\n\n```"}
{"prompt": "Write YAML that defines a Pod object with the name \"nginx\". It specifies that the pod should be scheduled to run on a specific node with the name \"foo-node\". The pod also includes a container named \"nginx\" which uses the \"nginx\" image and has an image pull policy of \"IfNotPresent\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  nodeName: foo-node # schedule pod to specific node\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"command-demo\" with a label \"purpose\" set to \"demonstrate-command\". This Pod will have a container named \"command-demo-container\" running the \"debian\" image. The container will execute the command \"printenv\" with arguments \"HOSTNAME\" and \"KUBERNETES_PORT\". The Pod's restart policy is set to \"OnFailure\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: command-demo\n  labels:\n    purpose: demonstrate-command\nspec:\n  containers:\n  - name: command-demo-container\n    image: debian\n    command: [\"printenv\"]\n    args: [\"HOSTNAME\", \"KUBERNETES_PORT\"]\n  restartPolicy: OnFailure\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"test-pod\" with a single container named \"pause\". The container will use the \"registry.k8s.io/pause:3.6\" image.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: pause\n    image: registry.k8s.io/pause:3.6\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod resource named \"nginx\" with the nginx container. The pod has a label \"env\" with the value \"test\". The container uses the nginx image and has the \"IfNotPresent\" image pull policy. Additionally, the pod has a toleration for the \"example-key\" key, where the operator is set to \"Exists\" and the effect is set to \"NoSchedule\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  tolerations:\n  - key: \"example-key\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named `private-reg` which will run a single container named `private-reg-container`. The container will use a private image specified by `<your-private-image>`. Additionally, the YAML defines an image pull secret named `regcred` to authenticate with the private registry for pulling the image.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: private-reg\nspec:\n  containers:\n  - name: private-reg-container\n    image: <your-private-image>\n  imagePullSecrets:\n  - name: regcred\n\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod with a container named \"test-container\". The container uses the image \"registry.k8s.io/busybox\" and runs the command \"env\" inside the container. The Pod also includes two environment variables: SPECIAL_LEVEL_KEY, which gets its value from a ConfigMap named \"special-config\" using the key \"special.how\", and LOG_LEVEL, which gets its value from a ConfigMap named \"env-config\" using the key \"log_level\". The Pod has a restart policy of \"Never\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: registry.k8s.io/busybox\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.how\n        - name: LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: env-config\n              key: log_level\n  restartPolicy: Never\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod object called \"nginx\" using the v1 API version. The Pod contains a single container called \"nginx\" that uses the \"nginx:1.14.2\" image. The container exposes port 80 for network traffic.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.2\n    ports:\n    - containerPort: 80\n\n```"}
{"prompt": "Write YAML that specifies the creation of a Pod named \"lifecycle-demo\" with a single container named \"lifecycle-demo-container\" that uses the \"nginx\" image. The YAML also defines lifecycle hooks for the container: a postStart hook that runs a command to echo a message to a file, and a preStop hook that runs a command to stop the nginx process gracefully and waits until it is no longer running.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: lifecycle-demo\nspec:\n  containers:\n  - name: lifecycle-demo-container\n    image: nginx\n    lifecycle:\n      postStart:\n        exec:\n          command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler > /usr/share/message\"]\n      preStop:\n        exec:\n          command: [\"/bin/sh\",\"-c\",\"nginx -s quit; while killall -0 nginx; do sleep 1; done\"]\n\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod with affinity and anti-affinity rules. The Pod requires the nodes to have the label \"kubernetes.io/os\" set to \"linux\" for scheduling. It also prefers nodes with the label \"label-1\" set to \"key-1\" with a weight of 1, and nodes with the label \"label-2\" set to \"key-2\" with a weight of 50. The Pod contains a single container named \"with-node-affinity\" that uses the \"pause:2.0\" image.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-affinity-anti-affinity\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/os\n            operator: In\n            values:\n            - linux\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: label-1\n            operator: In\n            values:\n            - key-1\n      - weight: 50\n        preference:\n          matchExpressions:\n          - key: label-2\n            operator: In\n            values:\n            - key-2\n  containers:\n  - name: with-node-affinity\n    image: registry.k8s.io/pause:2.0\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes pod with the following characteristics: \n- The pod uses the v1 API version.\n- The pod's kind is \"Pod\".\n- The pod has a metadata section with the name \"dapi-test-pod\".\n- The pod has a spec section that includes a single container named \"test-container\", which uses the \"registry.k8s.io/busybox\" image.\n- The container's command is set to \"/bin/echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\".\n- The pod has environment variables named \"SPECIAL_LEVEL_KEY\" and \"SPECIAL_TYPE_KEY\" that are sourced from a config map named \"special-config\", with keys \"SPECIAL_LEVEL\" and \"SPECIAL_TYPE\" respectively.\n- The pod's restart policy is set to \"Never\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: registry.k8s.io/busybox\n      command: [ \"/bin/echo\", \"$(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\" ]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: SPECIAL_LEVEL\n        - name: SPECIAL_TYPE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: SPECIAL_TYPE\n  restartPolicy: Never\n\n```"}
{"prompt": "Write YAML that defines a Pod resource with the metadata set to \"userns\" and the API version set to \"v1\". The Pod will have a single container named \"shell\" that uses the Debian image and runs the command \"sleep infinity\". The hostUsers field is set to false, indicating that the Pod does not require access to the host user namespace.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: userns\nspec:\n  hostUsers: false\n  containers:\n  - name: shell\n    command: [\"sleep\", \"infinity\"]\n    image: debian\n\n```"}
{"prompt": "Write YAML that creates a Pod named \"dapi-test-pod\" with a container named \"test-container\" using the image \"registry.k8s.io/busybox\". The container runs the command \"cat /etc/config/keys\" in a shell. The Pod also includes a volumeMount named \"config-volume\" that mounts the \"special-config\" ConfigMap to the path \"/etc/config\" within the container. The Pod has a restart policy of \"Never\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: registry.k8s.io/busybox\n      command: [ \"/bin/sh\",\"-c\",\"cat /etc/config/keys\" ]\n      volumeMounts:\n      - name: config-volume\n        mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: special-config\n        items:\n        - key: SPECIAL_LEVEL\n          path: keys\n  restartPolicy: Never\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod with two containers. The Pod has a volume named \"shared-data\" of type emptyDir. The first container named \"nginx-container\" uses the nginx image and mounts the \"shared-data\" volume to the path \"/usr/share/nginx/html\". The second container named \"debian-container\" uses the debian image, mounts the same \"shared-data\" volume to the path \"/pod-data\", and runs a command to echo a message into a file named \"index.html\" within the volume.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: two-containers\nspec:\n\n  restartPolicy: Never\n\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n\n  containers:\n\n  - name: nginx-container\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n\n  - name: debian-container\n    image: debian\n    volumeMounts:\n    - name: shared-data\n      mountPath: /pod-data\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"echo Hello from the debian container > /pod-data/index.html\"]\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod called 'nginx' with two containers - one running the 'nginx' image and another running 'busybox:1.28' image. The Pod is configured to share the process namespace and the 'busybox' container has added capabilities for SYS_PTRACE. The 'busybox' container is also configured to allow for interactive shell access with stdin and tty enabled.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  shareProcessNamespace: true\n  containers:\n  - name: nginx\n    image: nginx\n  - name: shell\n    image: busybox:1.28\n    securityContext:\n      capabilities:\n        add:\n        - SYS_PTRACE\n    stdin: true\n    tty: true\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"test-pod\" with two scheduling gates, \"example.com/foo\" and \"example.com/bar\". The Pod also includes a container named \"pause\" that uses the image \"registry.k8s.io/pause:3.6\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  schedulingGates:\n  - name: example.com/foo\n  - name: example.com/bar\n  containers:\n  - name: pause\n    image: registry.k8s.io/pause:3.6\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"init-demo\" with an Nginx container running on port 80. In addition, the Pod has an init container named \"install\" which retrieves an HTML file from \"http://info.cern.ch\" and saves it in a volume named \"workdir\". The Pod's DNS policy is set to \"Default\" and it has a volume named \"workdir\" with an empty directory.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: init-demo\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: workdir\n      mountPath: /usr/share/nginx/html\n  # These containers are run during pod initialization\n  initContainers:\n  - name: install\n    image: busybox:1.28\n    command:\n    - wget\n    - \"-O\"\n    - \"/work-dir/index.html\"\n    - http://info.cern.ch\n    volumeMounts:\n    - name: workdir\n      mountPath: \"/work-dir\"\n  dnsPolicy: Default\n  volumes:\n  - name: workdir\n    emptyDir: {}\n\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod object called 'dapi-test-pod'. The Pod has one container named 'test-container' using the busybox image and executes the command 'ls /etc/config/'. It also mounts a volume called 'config-volume' at the path '/etc/config'. Moreover, it includes a volume named 'config-volume' that references a ConfigMap named 'special-config' to add files to the container. The Pod has a restart policy of 'Never', meaning it won't restart automatically if it fails.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: registry.k8s.io/busybox\n      command: [ \"/bin/sh\", \"-c\", \"ls /etc/config/\" ]\n      volumeMounts:\n      - name: config-volume\n        mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        # Provide the name of the ConfigMap containing the files you want\n        # to add to the container\n        name: special-config\n  restartPolicy: Never\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod with node affinity. The Pod has a required node affinity which ensures it only gets scheduled on nodes in the antarctica-east1 or antarctica-west1 zones. It also has a preferred node affinity which indicates a preference to be scheduled on a node that has a label with key \"another-node-label-key\" and value \"another-node-label-value\". The Pod uses the \"pause:2.0\" image from the Kubernetes registry.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-node-affinity\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - antarctica-east1\n            - antarctica-west1\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  containers:\n  - name: with-node-affinity\n    image: registry.k8s.io/pause:2.0\n```"}
{"prompt": "Write YAML that defines a Kubernetes pod resource called `nginx`. This pod runs the NGINX container with the image `nginx` and the imagePullPolicy set to `IfNotPresent`. The pod is also labeled with `env: test`. Additionally, the pod is scheduled on nodes with the label `disktype: ssd`.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  nodeSelector:\n    disktype: ssd\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"nginx\" with a single container using the image \"nginx\". The container mounts a volume at \"/var/run/secrets/tokens\" and references a volume named \"vault-token\". The Pod also specifies a service account named \"build-robot\" and a projected volume source for a service account token stored in \"/var/run/secrets/tokens\" with an expiration of 7200 seconds and an audience of \"vault\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n    volumeMounts:\n    - mountPath: /var/run/secrets/tokens\n      name: vault-token\n  serviceAccountName: build-robot\n  volumes:\n  - name: vault-token\n    projected:\n      sources:\n      - serviceAccountToken:\n          path: vault-token\n          expirationSeconds: 7200\n          audience: vault\n\n```"}
{"prompt": "Write YAML that represents a Kubernetes Pod named \"nginx\". It specifies a node affinity rule that prefers nodes with a disktype value of \"ssd\" during scheduling. The Pod also includes a container named \"nginx\" that uses the nginx image and has an imagePullPolicy of IfNotPresent.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  affinity:\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - ssd          \n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n\n```"}
{"prompt": "Write YAML that creates a Kubernetes pod with a single container. The container is based on the `registry.k8s.io/busybox` image and runs the command `env`. The pod also sets an environment variable `SPECIAL_LEVEL_KEY` using a value from a ConfigMap named `special-config` and with the key `special.how`.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: registry.k8s.io/busybox\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\n      env:\n        # Define the environment variable\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY\n              name: special-config\n              # Specify the key associated with the value\n              key: special.how\n  restartPolicy: Never\n\n```"}
{"prompt": "Write YAML that describes two different Kubernetes Pods. Each Pod has a name, labels, and a single container. The first Pod is named \"pod1\" and has a label \"tier: frontend\". It uses the container named \"hello1\" with the image \"gcr.io/google-samples/hello-app:2.0\". The second Pod is named \"pod2\" and also has a label \"tier: frontend\". It uses the container named \"hello2\" with the image \"gcr.io/google-samples/hello-app:1.0\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\n  labels:\n    tier: frontend\nspec:\n  containers:\n  - name: hello1\n    image: gcr.io/google-samples/hello-app:2.0\n\n---\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\n  labels:\n    tier: frontend\nspec:\n  containers:\n  - name: hello2\n    image: gcr.io/google-samples/hello-app:1.0\n\n```"}
{"prompt": "Write YAML that creates a Pod in Kubernetes with pod affinity and anti-affinity rules. The pod affinity ensures that pods with the label \"security: S1\" are only scheduled on nodes in the same zone, defined by the \"topology.kubernetes.io/zone\" label. The pod anti-affinity specifies a preference for pods with the label \"security: S2\" to be scheduled on different nodes in the same zone. The Pod has one container named \"with-pod-affinity\" using the \"registry.k8s.io/pause:2.0\" image.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-pod-affinity\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: topology.kubernetes.io/zone\n    podAntiAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: security\n              operator: In\n              values:\n              - S2\n          topologyKey: topology.kubernetes.io/zone\n  containers:\n  - name: with-pod-affinity\n    image: registry.k8s.io/pause:2.0\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod with the name \"dapi-test-pod\". The Pod contains a single container named \"test-container\" that runs the \"busybox\" image. The container runs the command \"env\" inside a shell. The container also gets its environment variables from a ConfigMap named \"special-config\". The Pod has a restart policy of \"Never\", meaning it will not be restarted if it fails or terminates.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: registry.k8s.io/busybox\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\n      envFrom:\n      - configMapRef:\n          name: special-config\n  restartPolicy: Never\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes pod named `nginx`. The pod has an affinity rule that requires it to be scheduled on nodes with the label `disktype=ssd`. The pod also includes a container named `nginx` that uses the `nginx` image and pulls it if it is not already present.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: disktype\n            operator: In\n            values:\n            - ssd            \n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod called \"goproxy\" with a label of \"app=goproxy\". The Pod has one container named \"goproxy\" using the image \"registry.k8s.io/goproxy:0.1\" and exposes port 8080. The Pod also has a readiness probe and a liveness probe, both using a TCP socket to check if the container is healthy. The readiness probe starts after a delay of 5 seconds and checks every 10 seconds, while the liveness probe starts after a delay of 15 seconds and checks every 20 seconds.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: goproxy\n  labels:\n    app: goproxy\nspec:\n  containers:\n  - name: goproxy\n    image: registry.k8s.io/goproxy:0.1\n    ports:\n    - containerPort: 8080\n    readinessProbe:\n      tcpSocket:\n        port: 8080\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 8080\n      initialDelaySeconds: 15\n      periodSeconds: 20\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named `liveness-exec` with one container named `liveness` using the `busybox` image. The container runs a command that creates a file `/tmp/healthy`, waits for 30 seconds, removes the file, and then sleeps for 600 seconds. The Pod has a liveness probe that periodically executes the `cat` command on the file `/tmp/healthy` with an initial delay of 5 seconds and a probing interval of 5 seconds.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-exec\nspec:\n  containers:\n  - name: liveness\n    image: registry.k8s.io/busybox\n    args:\n    - /bin/sh\n    - -c\n    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 5\n      periodSeconds: 5\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"etcd-with-grpc\" using the v1 API version. The Pod will have one container named \"etcd\" which will use the image \"registry.k8s.io/etcd:3.5.1-0\". The container will run the command \"/usr/local/bin/etcd\" with multiple arguments for configuring its behavior. The container will also expose port 2379 and have a liveness probe that checks the availability of the etcd service through gRPC port 2379 after an initial delay of 10 seconds.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: etcd-with-grpc\nspec:\n  containers:\n  - name: etcd\n    image: registry.k8s.io/etcd:3.5.1-0\n    command: [ \"/usr/local/bin/etcd\", \"--data-dir\",  \"/var/lib/etcd\", \"--listen-client-urls\", \"http://0.0.0.0:2379\", \"--advertise-client-urls\", \"http://127.0.0.1:2379\", \"--log-level\", \"debug\"]\n    ports:\n    - containerPort: 2379\n    livenessProbe:\n      grpc:\n        port: 2379\n      initialDelaySeconds: 10\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"pod-with-tcp-socket-healthcheck\" which runs a container named \"redis\" using the Redis image. The Pod has a liveness probe defined with a TCP socket probe that checks the availability of port 6379. It has an initial delay of 30 seconds after startup before applying the health check, and a timeout of 1 second. The container also exposes port 6379.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-tcp-socket-healthcheck\nspec:\n  containers:\n  - name: redis\n    image: redis\n    # defines the health checking\n    livenessProbe:\n      # a TCP socket probe\n      tcpSocket:\n        port: 6379\n      # length of time to wait for a pod to initialize\n      # after pod startup, before applying health checking\n      initialDelaySeconds: 30\n      timeoutSeconds: 1\n    ports:\n    - containerPort: 6379\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"pod-with-http-healthcheck\". The Pod has one container named \"nginx\" with the Nginx image. It also has a livenessProbe that uses an HTTP probe to check the health of the container by sending an HTTP GET request to path \"/_status/healthz\" on port 80. The initial delay before applying health checking is set to 30 seconds, and the timeout for the probe is set to 1 second. The container also exposes port 80.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-http-healthcheck\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    # defines the health checking\n    livenessProbe:\n      # an http probe\n      httpGet:\n        path: /_status/healthz\n        port: 80\n      # length of time to wait for a pod to initialize\n      # after pod startup, before applying health checking\n      initialDelaySeconds: 30\n      timeoutSeconds: 1\n    ports:\n    - containerPort: 80\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"liveness-http\" with a container named \"liveness\" using the image \"registry.k8s.io/liveness\". The Pod includes a livenessProbe that performs an HTTP GET request to \"/healthz\" on port 8080 with a custom header \"Custom-Header: Awesome\". The livenessProbe has an initial delay of 3 seconds and checks the health every 3 seconds.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: registry.k8s.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n\n```"}
{"prompt": "Write YAML that creates a ConfigMap object named \"example-redis-config\" with a single key-value pair. The key is \"redis-config\" and the value is a multi-line string that contains the Redis configuration settings \"maxmemory 2mb\" and \"maxmemory-policy allkeys-lru\". This YAML can be used with Kubernetes to create the ConfigMap resource.", "completion": "```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: example-redis-config\ndata:\n  redis-config: |\n    maxmemory 2mb\n    maxmemory-policy allkeys-lru\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod called \"redis\" using the v1 API version. This Pod will have one container named \"redis\" using the \"redis:5.0.4\" image with the command \"redis-server\" followed by \"/redis-master/redis.conf\". It will have an environment variable named \"MASTER\" with the value \"true\", and will expose port 6379. The container will have a CPU limit of 0.1, and two volume mounts at \"/redis-master-data\" and \"/redis-master\" named \"data\" and \"config\" respectively. The Pod will have two volumes, an emptyDir named \"data\" and a configMap named \"config\" referencing the \"example-redis-config\" config map with a single item having the key \"redis-config\" and the path \"redis.conf\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: redis\nspec:\n  containers:\n  - name: redis\n    image: redis:5.0.4\n    command:\n      - redis-server\n      - \"/redis-master/redis.conf\"\n    env:\n    - name: MASTER\n      value: \"true\"\n    ports:\n    - containerPort: 6379\n    resources:\n      limits:\n        cpu: \"0.1\"\n    volumeMounts:\n    - mountPath: /redis-master-data\n      name: data\n    - mountPath: /redis-master\n      name: config\n  volumes:\n    - name: data\n      emptyDir: {}\n    - name: config\n      configMap:\n        name: example-redis-config\n        items:\n        - key: redis-config\n          path: redis.conf\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"memory-demo\" in the namespace \"mem-example\". The Pod contains a container named \"memory-demo-ctr\" with the image \"polinux/stress\". The container has memory resource requests of 100Mi and limits of 200Mi. The container will run the \"stress\" command with arguments \"--vm\", \"1\", \"--vm-bytes\", \"150M\", and \"--vm-hang\", \"1\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: memory-demo\n  namespace: mem-example\nspec:\n  containers:\n  - name: memory-demo-ctr\n    image: polinux/stress\n    resources:\n      requests:\n        memory: \"100Mi\"\n      limits:\n        memory: \"200Mi\"\n    command: [\"stress\"]\n    args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"]\n\n```"}
{"prompt": "Write YAML that specifies the following:\n- The API version is v1 and the kind is Pod.\n- The metadata of the pod includes its name as \"memory-demo-3\" and it belongs to the namespace \"mem-example\".\n- The pod has one container named \"memory-demo-3-ctr\" with the image \"polinux/stress\".\n- The container has specified resource requests and limits for memory, both set to \"1000Gi\".\n- The container runs the command \"stress\" with the arguments \"--vm 1 --vm-bytes 150M --vm-hang 1\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: memory-demo-3\n  namespace: mem-example\nspec:\n  containers:\n  - name: memory-demo-3-ctr\n    image: polinux/stress\n    resources:\n      requests:\n        memory: \"1000Gi\"\n      limits:\n        memory: \"1000Gi\"\n    command: [\"stress\"]\n    args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"]\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"extended-resource-demo\" with a single container named \"extended-resource-demo-ctr\" running the nginx image. The Pod specifies resource requests and limits for a custom resource named \"example.com/dongle\", with both the request and limit set to 3 units.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: extended-resource-demo\nspec:\n  containers:\n  - name: extended-resource-demo-ctr\n    image: nginx\n    resources:\n      requests:\n        example.com/dongle: 3\n      limits:\n        example.com/dongle: 3\n\n```"}
{"prompt": "Write YAML that specifies the API version as v1, defines a Pod object, sets the name of the Pod to \"memory-demo-2\", and places it in the \"mem-example\" namespace. The Pod contains a single container named \"memory-demo-2-ctr\" with the image \"polinux/stress\". The container has resource requests for memory set to \"50Mi\" and limits set to \"100Mi\". The container runs the command \"stress\" with arguments \"--vm\", \"1\", \"--vm-bytes\", \"250M\", and \"--vm-hang\", \"1\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: memory-demo-2\n  namespace: mem-example\nspec:\n  containers:\n  - name: memory-demo-2-ctr\n    image: polinux/stress\n    resources:\n      requests:\n        memory: \"50Mi\"\n      limits:\n        memory: \"100Mi\"\n    command: [\"stress\"]\n    args: [\"--vm\", \"1\", \"--vm-bytes\", \"250M\", \"--vm-hang\", \"1\"]\n\n```"}
{"prompt": "Write YAML that describes a Pod named \"cpu-demo-2\" in the \"cpu-example\" namespace. The Pod includes a single container named \"cpu-demo-ctr-2\" with the image \"vish/stress\". The container is configured to have a CPU limit and request of 100 units, and the container's command is set to run with 2 CPUs.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cpu-demo-2\n  namespace: cpu-example\nspec:\n  containers:\n  - name: cpu-demo-ctr-2\n    image: vish/stress\n    resources:\n      limits:\n        cpu: \"100\"\n      requests:\n        cpu: \"100\"\n    args:\n    - -cpus\n    - \"2\"\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes pod named \"extended-resource-demo-2\" with a single container named \"extended-resource-demo-2-ctr\" using the nginx image. The pod also specifies resource requests and limits for the container, setting the value of the \"example.com/dongle\" resource to 2 for both requests and limits.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: extended-resource-demo-2\nspec:\n  containers:\n  - name: extended-resource-demo-2-ctr\n    image: nginx\n    resources:\n      requests:\n        example.com/dongle: 2\n      limits:\n        example.com/dongle: 2\n\n```"}
{"prompt": "Write YAML that creates a pod named \"cpu-demo\" in the \"cpu-example\" namespace. The pod will run a container named \"cpu-demo-ctr\" with the image \"vish/stress\". The container is limited to using 1 CPU and requests 0.5 CPU, and the container's command line arguments are set to use 2 CPUs.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cpu-demo\n  namespace: cpu-example\nspec:\n  containers:\n  - name: cpu-demo-ctr\n    image: vish/stress\n    resources:\n      limits:\n        cpu: \"1\"\n      requests:\n        cpu: \"0.5\"\n    args:\n    - -cpus\n    - \"2\"\n\n```"}
{"prompt": "Write YAML that describes a Pod with the name \"mypod\" and a label \"foo\" with a value of \"bar\". The Pod has topology spread constraints set, where the maximum skew is 1, and it should be scheduled based on the \"zone\" topology key. If the constraints are not satisfied, the Pod should not be scheduled. Additionally, the Pod has node affinity where it is required to be scheduled on nodes that do not have the label \"zoneC\". The Pod contains a container named \"pause\" with the image \"registry.k8s.io/pause:3.1\".", "completion": "```yaml\nkind: Pod\napiVersion: v1\nmetadata:\n  name: mypod\n  labels:\n    foo: bar\nspec:\n  topologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: zone\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        foo: bar\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: zone\n            operator: NotIn\n            values:\n            - zoneC\n  containers:\n  - name: pause\n    image: registry.k8s.io/pause:3.1\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"mypod\" with a label \"foo\" set to \"bar\".\nThe Pod has a single container named \"pause\" which pulls the image \"registry.k8s.io/pause:3.1\".\nAdditionally, the YAML includes a topology spread constraint that ensures the Pod is scheduled with a maximum skew of 1 across different zones, and if unsatisfiable, the Pod should not be scheduled.\n", "completion": "```yaml\nkind: Pod\napiVersion: v1\nmetadata:\n  name: mypod\n  labels:\n    foo: bar\nspec:\n  topologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: zone\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        foo: bar\n  containers:\n  - name: pause\n    image: registry.k8s.io/pause:3.1\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod with the name \"mypod\" and a label of \"foo\" with a value of \"bar\". It also specifies topology spread constraints to ensure that the maximum skew is 1 for both the \"zone\" and \"node\" topologies. Finally, it includes a container named \"pause\" that uses the image \"registry.k8s.io/pause:3.1\".", "completion": "```yaml\nkind: Pod\napiVersion: v1\nmetadata:\n  name: mypod\n  labels:\n    foo: bar\nspec:\n  topologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: zone\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        foo: bar\n  - maxSkew: 1\n    topologyKey: node\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        foo: bar\n  containers:\n  - name: pause\n    image: registry.k8s.io/pause:3.1\n```"}
{"prompt": "Write YAML that defines a Pod with a container named \"test-projected-volume\" using the Busybox:1.28 image, which sleeps for 86400 seconds. The container mounts a volume named \"all-in-one\" at the path \"/projected-volume\" with read-only access. The Pod also has a volume named \"all-in-one\" that is projected with two secrets, \"user\" and \"pass\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-projected-volume\nspec:\n  containers:\n  - name: test-projected-volume\n    image: busybox:1.28\n    args:\n    - sleep\n    - \"86400\"\n    volumeMounts:\n    - name: all-in-one\n      mountPath: \"/projected-volume\"\n      readOnly: true\n  volumes:\n  - name: all-in-one\n    projected:\n      sources:\n      - secret:\n          name: user\n      - secret:\n          name: pass\n\n```"}
{"prompt": "Write YAML that creates a Pod named \"volume-test\" with a container named \"container-test\" running the busybox:1.28 image. The container mounts a volume named \"all-in-one\" at the path \"/projected-volume\" with read-only access. The volume \"all-in-one\" is projected with two secrets, \"mysecret\" and \"mysecret2\", each containing specific key-value pairs with their corresponding paths. The second secret also has a mode of 511.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-test\nspec:\n  containers:\n  - name: container-test\n    image: busybox:1.28\n    volumeMounts:\n    - name: all-in-one\n      mountPath: \"/projected-volume\"\n      readOnly: true\n  volumes:\n  - name: all-in-one\n    projected:\n      sources:\n      - secret:\n          name: mysecret\n          items:\n            - key: username\n              path: my-group/my-username\n      - secret:\n          name: mysecret2\n          items:\n            - key: password\n              path: my-group/my-password\n              mode: 511\n\n```"}
{"prompt": "Write YAML that defines a PersistentVolumeClaim resource with the name 'task-pv-claim'. It has a storageClassName set to 'manual' and access mode set to 'ReadWriteOnce'. Additionally, it requests 3 gigabytes of storage.\n\nIn this YAML, we specify the apiVersion as 'v1' and the kind as 'PersistentVolumeClaim' to indicate the type of resource we are creating. The metadata section defines the name of the resource, while the spec section specifies its characteristics. The storageClassName determines the storage class to be used, accessModes specifies the access mode, and resources.requests.storage sets the amount of storage requested.", "completion": "```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: task-pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n\n```"}
{"prompt": "Write YAML that defines a Pod resource with the container named \"task-pv-container\" using the nginx image. The Pod has a volume named \"task-pv-storage\" which is associated with a persistentVolumeClaim named \"task-pv-claim\". The container's port 80 is exposed as \"http-server\" and the \"/usr/share/nginx/html\" directory is mounted to the volume \"task-pv-storage\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n    - name: task-pv-storage\n      persistentVolumeClaim:\n        claimName: task-pv-claim\n  containers:\n    - name: task-pv-container\n      image: nginx\n      ports:\n        - containerPort: 80\n          name: \"http-server\"\n      volumeMounts:\n        - mountPath: \"/usr/share/nginx/html\"\n          name: task-pv-storage\n\n\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"volume-test\" with a single container named \"container-test\" using the busybox:1.28 image. The container is mounted to a volume named \"all-in-one\" at the \"/projected-volume\" path, with read-only access. The volume \"all-in-one\" is a projected volume that combines data from a secret named \"mysecret\" (specifically, the \"username\" key at the \"my-group/my-username\" path), data from the Pod's labels, and data from a configMap named \"myconfigmap\" (specifically, the \"config\" key at the \"my-group/my-config\" path).", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-test\nspec:\n  containers:\n  - name: container-test\n    image: busybox:1.28\n    volumeMounts:\n    - name: all-in-one\n      mountPath: \"/projected-volume\"\n      readOnly: true\n  volumes:\n  - name: all-in-one\n    projected:\n      sources:\n      - secret:\n          name: mysecret\n          items:\n            - key: username\n              path: my-group/my-username\n      - downwardAPI:\n          items:\n            - path: \"labels\"\n              fieldRef:\n                fieldPath: metadata.labels\n            - path: \"cpu_limit\"\n              resourceFieldRef:\n                containerName: container-test\n                resource: limits.cpu\n      - configMap:\n          name: myconfigmap\n          items:\n            - key: config\n              path: my-group/my-config\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"sa-token-test\" with one container named \"container-test\" using the busybox:1.28 image. The container has a volume mount named \"token-vol\" at the path \"/service-account\", which is read-only. The Pod is associated with the default service account and has a volume named \"token-vol\" with a projected source that includes a service account token with an audience of \"api\", an expiration time of 3600 seconds, and a path of \"token\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sa-token-test\nspec:\n  containers:\n  - name: container-test\n    image: busybox:1.28\n    volumeMounts:\n    - name: token-vol\n      mountPath: \"/service-account\"\n      readOnly: true\n  serviceAccountName: default\n  volumes:\n  - name: token-vol\n    projected:\n      sources:\n      - serviceAccountToken:\n          audience: api\n          expirationSeconds: 3600\n          path: token\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"redis\" with a single container running the Redis image. The Pod also includes a volumeMount that mounts the \"redis-storage\" volume to the container's \"/data/redis\" mount path. Lastly, the YAML specifies a volume called \"redis-storage\" of type \"emptyDir\" which creates an empty directory within the Pod's memory.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: redis\nspec:\n  containers:\n  - name: redis\n    image: redis\n    volumeMounts:\n    - name: redis-storage\n      mountPath: /data/redis\n  volumes:\n  - name: redis-storage\n    emptyDir: {}\n\n```"}
{"prompt": "Write YAML that defines a Persistent Volume called \"task-pv-volume\" with a label of \"type: local\". The volume has a storage class of \"manual\" and a capacity of 10Gi. The access mode is set to \"ReadWriteOnce\" and it is mounted on the host at the path \"/mnt/data\".", "completion": "```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: task-pv-volume\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/mnt/data\"\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"test\" with a container named \"test\" using the nginx image. It mounts two volumes: one named \"config\" with subPath \"html\" at mountPath \"/usr/share/nginx/html\" for site data, and another with subPath \"nginx.conf\" at mountPath \"/etc/nginx/nginx.conf\" for nginx config. The Pod also has a volume named \"config\" created from a persistentVolumeClaim named \"test-nfs-claim\".", "completion": "```yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test\nspec:\n  containers:\n    - name: test\n      image: nginx\n      volumeMounts:\n        # a mount for site-data\n        - name: config\n          mountPath: /usr/share/nginx/html\n          subPath: html\n        # another mount for nginx config\n        - name: config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n  volumes:\n    - name: config\n      persistentVolumeClaim:\n        claimName: test-nfs-claim\n\n```"}
{"prompt": "Write YAML that specifies the creation of a Pod named \"security-context-demo\". The Pod has a security context that sets the user ID to 1000, group ID to 3000, and file system group ID to 2000. It also sets an emptyDir volume named \"sec-ctx-vol\". The Pod contains one container named \"sec-ctx-demo\" with the busybox:1.28 image, running the command to sleep for 1 hour. The container also mounts the \"sec-ctx-vol\" volume to the \"/data/demo\" path and has a security context that disallows privilege escalation.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  securityContext:\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n  volumes:\n  - name: sec-ctx-vol\n    emptyDir: {}\n  containers:\n  - name: sec-ctx-demo\n    image: busybox:1.28\n    command: [ \"sh\", \"-c\", \"sleep 1h\" ]\n    volumeMounts:\n    - name: sec-ctx-vol\n      mountPath: /data/demo\n    securityContext:\n      allowPrivilegeEscalation: false\n\n```"}
{"prompt": "Write YAML that creates a Pod named \"security-context-demo-2\" with a security context specifying that it should run as user ID 1000. Inside the Pod, there is a container named \"sec-ctx-demo-2\" that uses the image \"gcr.io/google-samples/node-hello:1.0\". The container has its own security context that overrides the Pod's settings and specifies that it should run as user ID 2000 and disallows privilege escalation.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo-2\nspec:\n  securityContext:\n    runAsUser: 1000\n  containers:\n  - name: sec-ctx-demo-2\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      runAsUser: 2000\n      allowPrivilegeEscalation: false\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"hello-apparmor\" with an annotation specifying the AppArmor profile \"k8s-apparmor-example-deny-write\". The Pod contains a single container named \"hello\" that runs the image busybox:1.28 and executes the command \"sh -c 'echo 'Hello AppArmor!' && sleep 1h'\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-apparmor\n  annotations:\n    # Tell Kubernetes to apply the AppArmor profile \"k8s-apparmor-example-deny-write\".\n    # Note that this is ignored if the Kubernetes node is not running version 1.4 or greater.\n    container.apparmor.security.beta.kubernetes.io/hello: localhost/k8s-apparmor-example-deny-write\nspec:\n  containers:\n  - name: hello\n    image: busybox:1.28\n    command: [ \"sh\", \"-c\", \"echo 'Hello AppArmor!' && sleep 1h\" ]\n\n```"}
{"prompt": "Write YAML that specifies a Kubernetes Pod with the name \"security-context-demo-3\". The Pod has one container named \"sec-ctx-3\" and it uses the image \"gcr.io/google-samples/node-hello:1.0\". The YAML also includes metadata for the Pod.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo-3\nspec:\n  containers:\n  - name: sec-ctx-3\n    image: gcr.io/google-samples/node-hello:1.0\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"security-context-demo-4\", with one container named \"sec-ctx-4\" using the image \"gcr.io/google-samples/node-hello:1.0\". In the container's security context, it adds the capabilities \"NET_ADMIN\" and \"SYS_TIME\" to allow network administration and system time manipulation within the container.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo-4\nspec:\n  containers:\n  - name: sec-ctx-4\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add: [\"NET_ADMIN\", \"SYS_TIME\"]\n\n```"}
{"prompt": "Write YAML that specifies the API version as `kind.x-k8s.io/v1alpha4` and defines a Kubernetes cluster. It includes a single node with the role of `control-plane` and an additional mount for the container at path `/var/lib/kubelet/seccomp/profiles`, which is linked to the host path `./profiles`.", "completion": "```yaml\napiVersion: kind.x-k8s.io/v1alpha4\nkind: Cluster\nnodes:\n- role: control-plane\n  extraMounts:\n  - hostPath: \"./profiles\"\n    containerPath: \"/var/lib/kubelet/seccomp/profiles\"\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"fine-pod\" with labels and annotations. The Pod contains a single container named \"test-container\" that runs the image \"hashicorp/http-echo:0.2.3\" with a specific command argument. The security context of the container is set to disallow privilege escalation.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fine-pod\n  labels:\n    app: fine-pod\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: localhost/profiles/fine-grained.json\nspec:\n  containers:\n  - name: test-container\n    image: hashicorp/http-echo:0.2.3\n    args:\n    - \"-text=just made some syscalls!\"\n    securityContext:\n      allowPrivilegeEscalation: false\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"violation-pod\" with a label \"app\" set to \"violation-pod\" and an annotation specifying a seccomp profile located at \"localhost/profiles/violation.json\". The Pod has one container named \"test-container\" running the image \"hashicorp/http-echo:0.2.3\", with an argument set to \"-text=just made some syscalls!\". The securityContext specifies that privilege escalation is not allowed.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: violation-pod\n  labels:\n    app: violation-pod\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: localhost/profiles/violation.json\nspec:\n  containers:\n  - name: test-container\n    image: hashicorp/http-echo:0.2.3\n    args:\n    - \"-text=just made some syscalls!\"\n    securityContext:\n      allowPrivilegeEscalation: false\n```"}
{"prompt": "Write YAML that defines a Pod configuration with the name \"audit-pod\" and labels it as \"app: audit-pod\". The Pod specifies an annotation for seccomp security using a JSON profile located at \"localhost/profiles/audit.json\". The Pod also includes a container named \"test-container\" with the image \"hashicorp/http-echo:0.2.3\" and sets the command-line argument to \"-text=just made some syscalls!\". The security context is configured to disallow privilege escalation.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: audit-pod\n  labels:\n    app: audit-pod\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: localhost/profiles/audit.json\nspec:\n  containers:\n  - name: test-container\n    image: hashicorp/http-echo:0.2.3\n    args:\n    - \"-text=just made some syscalls!\"\n    securityContext:\n      allowPrivilegeEscalation: false\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod resource with the specified metadata and a single container. The Pod has the name \"default-pod\" and the label \"app=default-pod\". It also has an annotation for the seccomp security profile. The container is named \"test-container\" and uses the \"hashicorp/http-echo:0.2.3\" image. It has an argument \"-text=just made some syscalls!\" and a security context that disallows privilege escalation.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: default-pod\n  labels:\n    app: default-pod\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: runtime/default\nspec:\n  containers:\n  - name: test-container\n    image: hashicorp/http-echo:0.2.3\n    args:\n    - \"-text=just made some syscalls!\"\n    securityContext:\n      allowPrivilegeEscalation: false\n```"}
{"prompt": "Write YAML that defines a Pod named \"fine-pod\" with labels indicating it is an app called \"fine-pod\". The Pod has a security context with a seccomp profile that specifies a type of \"Localhost\" and a localhost profile located at \"profiles/fine-grained.json\". The Pod also has a container named \"test-container\" which uses the image \"hashicorp/http-echo:0.2.3\" and has an argument of \"-text=just made some syscalls!\". The container has a security context that prevents privilege escalation.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fine-pod\n  labels:\n    app: fine-pod\nspec:\n  securityContext:\n    seccompProfile:\n      type: Localhost\n      localhostProfile: profiles/fine-grained.json\n  containers:\n  - name: test-container\n    image: hashicorp/http-echo:0.2.3\n    args:\n    - \"-text=just made some syscalls!\"\n    securityContext:\n      allowPrivilegeEscalation: false\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod called \"violation-pod\" with a label of \"app: violation-pod\". The Pod has a securityContext that specifies a seccompProfile which is set to \"Localhost\" and has a localhostProfile file set to \"profiles/violation.json\". The Pod also includes a container called \"test-container\", which uses the image \"hashicorp/http-echo:0.2.3\" and specifies an argument of \"-text=just made some syscalls!\". The container's securityContext disables privilege escalation.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: violation-pod\n  labels:\n    app: violation-pod\nspec:\n  securityContext:\n    seccompProfile:\n      type: Localhost\n      localhostProfile: profiles/violation.json\n  containers:\n  - name: test-container\n    image: hashicorp/http-echo:0.2.3\n    args:\n    - \"-text=just made some syscalls!\"\n    securityContext:\n      allowPrivilegeEscalation: false\n```"}
{"prompt": "Write YAML that defines a Pod named \"audit-pod\" with the label \"app: audit-pod\". The Pod specifies a seccomp security profile, where the type is set to \"Localhost\" and the localhostProfile is set to \"profiles/audit.json\". The Pod also includes a container named \"test-container\" with the image \"hashicorp/http-echo:0.2.3\". The container's args are set to \"-text=just made some syscalls!\" and its securityContext has allowPrivilegeEscalation set to false.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: audit-pod\n  labels:\n    app: audit-pod\nspec:\n  securityContext:\n    seccompProfile:\n      type: Localhost\n      localhostProfile: profiles/audit.json\n  containers:\n  - name: test-container\n    image: hashicorp/http-echo:0.2.3\n    args:\n    - \"-text=just made some syscalls!\"\n    securityContext:\n      allowPrivilegeEscalation: false\n```"}
{"prompt": "Write YAML that defines a Kubernetes pod with the API version \"v1\" and kind \"Pod\". The pod has a metadata section that includes the name \"default-pod\" and a label \"app\" set to \"default-pod\". The pod specification includes a security context with a seccomp profile of type \"RuntimeDefault\". The pod also has a container named \"test-container\" with an image of \"hashicorp/http-echo:0.2.3\". The container has an argument set to \"-text=just made some more syscalls!\" and a security context that disables privilege escalation.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: default-pod\n  labels:\n    app: default-pod\nspec:\n  securityContext:\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: test-container\n    image: hashicorp/http-echo:0.2.3\n    args:\n    - \"-text=just made some more syscalls!\"\n    securityContext:\n      allowPrivilegeEscalation: false\n```"}
{"prompt": "Write YAML that specifies a Secret object named \"test-secret\" with two data fields: \"username\" and \"password\". The values for these fields are base64-encoded strings: \"bXktYXBw\" and \"Mzk1MjgkdmRnN0pi\" respectively. This YAML can be used to create or update a Secret resource in Kubernetes.", "completion": "```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: test-secret\ndata:\n  username: bXktYXBw\n  password: Mzk1MjgkdmRnN0pi\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod configuration with a container named \"test-container\" running the BusyBox image. The container runs a shell command that continuously prints the values of CPU request, CPU limit, memory request, and memory limit environment variables every 10 seconds. These environment variables are populated using resource field references to the CPU and memory requests and limits of the \"test-container\" itself. The Pod has a restart policy of \"Never\".\n", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-envars-resourcefieldref\nspec:\n  containers:\n    - name: test-container\n      image: registry.k8s.io/busybox:1.24\n      command: [ \"sh\", \"-c\"]\n      args:\n      - while true; do\n          echo -en '\\n';\n          printenv MY_CPU_REQUEST MY_CPU_LIMIT;\n          printenv MY_MEM_REQUEST MY_MEM_LIMIT;\n          sleep 10;\n        done;\n      resources:\n        requests:\n          memory: \"32Mi\"\n          cpu: \"125m\"\n        limits:\n          memory: \"64Mi\"\n          cpu: \"250m\"\n      env:\n        - name: MY_CPU_REQUEST\n          valueFrom:\n            resourceFieldRef:\n              containerName: test-container\n              resource: requests.cpu\n        - name: MY_CPU_LIMIT\n          valueFrom:\n            resourceFieldRef:\n              containerName: test-container\n              resource: limits.cpu\n        - name: MY_MEM_REQUEST\n          valueFrom:\n            resourceFieldRef:\n              containerName: test-container\n              resource: requests.memory\n        - name: MY_MEM_LIMIT\n          valueFrom:\n            resourceFieldRef:\n              containerName: test-container\n              resource: limits.memory\n  restartPolicy: Never\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"envar-demo\" with a single container named \"envar-demo-container\" running the image \"gcr.io/google-samples/node-hello:1.0\". The Pod has two environment variables defined: \"DEMO_GREETING\" with the value \"Hello from the environment\" and \"DEMO_FAREWELL\" with the value \"Such a sweet sorrow\". The purpose of this YAML is to demonstrate how to define environment variables for a container within a Pod.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: envar-demo\n  labels:\n    purpose: demonstrate-envars\nspec:\n  containers:\n  - name: envar-demo-container\n    image: gcr.io/google-samples/node-hello:1.0\n    env:\n    - name: DEMO_GREETING\n      value: \"Hello from the environment\"\n    - name: DEMO_FAREWELL\n      value: \"Such a sweet sorrow\"\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"kubernetes-downwardapi-volume-example-2\". The Pod contains a single container named \"client-container\" with an image from the \"busybox\" registry. The container runs a shell command that outputs the values of CPU and memory limits and requests from the downward API volume mounted at \"/etc/podinfo\". The container has resource limits and requests defined for memory and CPU, and the volume is configured with downwardAPI items to obtain these values.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kubernetes-downwardapi-volume-example-2\nspec:\n  containers:\n    - name: client-container\n      image: registry.k8s.io/busybox:1.24\n      command: [\"sh\", \"-c\"]\n      args:\n      - while true; do\n          echo -en '\\n';\n          if [[ -e /etc/podinfo/cpu_limit ]]; then\n            echo -en '\\n'; cat /etc/podinfo/cpu_limit; fi;\n          if [[ -e /etc/podinfo/cpu_request ]]; then\n            echo -en '\\n'; cat /etc/podinfo/cpu_request; fi;\n          if [[ -e /etc/podinfo/mem_limit ]]; then\n            echo -en '\\n'; cat /etc/podinfo/mem_limit; fi;\n          if [[ -e /etc/podinfo/mem_request ]]; then\n            echo -en '\\n'; cat /etc/podinfo/mem_request; fi;\n          sleep 5;\n        done;\n      resources:\n        requests:\n          memory: \"32Mi\"\n          cpu: \"125m\"\n        limits:\n          memory: \"64Mi\"\n          cpu: \"250m\"\n      volumeMounts:\n        - name: podinfo\n          mountPath: /etc/podinfo\n  volumes:\n    - name: podinfo\n      downwardAPI:\n        items:\n          - path: \"cpu_limit\"\n            resourceFieldRef:\n              containerName: client-container\n              resource: limits.cpu\n              divisor: 1m\n          - path: \"cpu_request\"\n            resourceFieldRef:\n              containerName: client-container\n              resource: requests.cpu\n              divisor: 1m\n          - path: \"mem_limit\"\n            resourceFieldRef:\n              containerName: client-container\n              resource: limits.memory\n              divisor: 1Mi\n          - path: \"mem_request\"\n            resourceFieldRef:\n              containerName: client-container\n              resource: requests.memory\n              divisor: 1Mi\n\n\n```"}
{"prompt": "Write YAML that creates a pod with a container named \"test-container\" running the \"busybox\" image. The container executes a shell command that continuously prints the values of various environment variables such as MY_NODE_NAME, MY_POD_NAME, MY_POD_NAMESPACE, MY_POD_IP, and MY_POD_SERVICE_ACCOUNT. These environment variables are obtained from different fields within the Kubernetes cluster, such as spec.nodeName, metadata.name, metadata.namespace, status.podIP, and spec.serviceAccountName. The pod has a restart policy of \"Never\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-envars-fieldref\nspec:\n  containers:\n    - name: test-container\n      image: registry.k8s.io/busybox\n      command: [ \"sh\", \"-c\"]\n      args:\n      - while true; do\n          echo -en '\\n';\n          printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;\n          printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;\n          sleep 10;\n        done;\n      env:\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: MY_POD_SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n  restartPolicy: Never\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod resource named \"secret-envars-test-pod\" with a single container named \"envars-test-container\" running the \"nginx\" image. This Pod also sets two environment variables named \"SECRET_USERNAME\" and \"SECRET_PASSWORD\" by referencing the values from a Kubernetes Secret named \"test-secret\" with the keys \"username\" and \"password\" respectively.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-envars-test-pod\nspec:\n  containers:\n  - name: envars-test-container\n    image: nginx\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: test-secret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: test-secret\n          key: password\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod called \"secret-test-pod\" with a single container using the nginx image. The container has a volume mount called \"secret-volume\" at path \"/etc/secret-volume\" with read-only access. The Pod also has a volume named \"secret-volume\" that mounts a secret named \"test-secret\" into the container.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: nginx\n      volumeMounts:\n        # name must match the volume name below\n        - name: secret-volume\n          mountPath: /etc/secret-volume\n          readOnly: true\n  # The secret data is exposed to Containers in the Pod through a Volume.\n  volumes:\n    - name: secret-volume\n      secret:\n        secretName: test-secret\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod with the following specifications:\n- The Pod is a containerized instance of busybox:1.28.\n- The container runs a command that continuously prints the values of three environment variables - UNCHANGED_REFERENCE, SERVICE_ADDRESS, and ESCAPED_REFERENCE.\n- The values of these variables are obtained by combining other environment variables - PROTOCOL, SERVICE_IP, and SERVICE_PORT.\n- The PROTOCOL variable is set to \"https\", SERVICE_IP is set to \"172.17.0.1\", and SERVICE_PORT is set to \"80\".\n- The ESCAPED_REFERENCE variable has an additional \"$\" symbol to escape special characters.\n\nNote that this YAML is written to deploy this Pod to a Kubernetes cluster.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dependent-envars-demo\nspec:\n  containers:\n    - name: dependent-envars-demo\n      args:\n        - while true; do echo -en '\\n'; printf UNCHANGED_REFERENCE=$UNCHANGED_REFERENCE'\\n'; printf SERVICE_ADDRESS=$SERVICE_ADDRESS'\\n';printf ESCAPED_REFERENCE=$ESCAPED_REFERENCE'\\n'; sleep 30; done;\n      command:\n        - sh\n        - -c\n      image: busybox:1.28\n      env:\n        - name: SERVICE_PORT\n          value: \"80\"\n        - name: SERVICE_IP\n          value: \"172.17.0.1\"\n        - name: UNCHANGED_REFERENCE\n          value: \"$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)\"\n        - name: PROTOCOL\n          value: \"https\"\n        - name: SERVICE_ADDRESS\n          value: \"$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)\"\n        - name: ESCAPED_REFERENCE\n          value: \"$$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)\"\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod with the name \"kubernetes-downwardapi-volume-example\", labels \"zone: us-est-coast\", \"cluster: test-cluster1\", and \"rack: rack-22\", and annotations \"build: two\" and \"builder: john-doe\". The Pod includes a container with the name \"client-container\" and an image from the \"busybox\" registry. The container runs a command that continuously checks and prints the labels and annotations of the Pod every 5 seconds. The Pod also has a volume named \"podinfo\" that is mounted at the path \"/etc/podinfo\" and includes the labels and annotations fields from the Pod's metadata using the downwardAPI.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kubernetes-downwardapi-volume-example\n  labels:\n    zone: us-est-coast\n    cluster: test-cluster1\n    rack: rack-22\n  annotations:\n    build: two\n    builder: john-doe\nspec:\n  containers:\n    - name: client-container\n      image: registry.k8s.io/busybox\n      command: [\"sh\", \"-c\"]\n      args:\n      - while true; do\n          if [[ -e /etc/podinfo/labels ]]; then\n            echo -en '\\n\\n'; cat /etc/podinfo/labels; fi;\n          if [[ -e /etc/podinfo/annotations ]]; then\n            echo -en '\\n\\n'; cat /etc/podinfo/annotations; fi;\n          sleep 5;\n        done;\n      volumeMounts:\n        - name: podinfo\n          mountPath: /etc/podinfo\n  volumes:\n    - name: podinfo\n      downwardAPI:\n        items:\n          - path: \"labels\"\n            fieldRef:\n              fieldPath: metadata.labels\n          - path: \"annotations\"\n            fieldRef:\n              fieldPath: metadata.annotations\n\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"envfrom-secret\" with one container named \"envars-test-container\" using the \"nginx\" image. This container should inherit environment variables from a secret named \"test-secret\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: envfrom-secret\nspec:\n  containers:\n  - name: envars-test-container\n    image: nginx\n    envFrom:\n    - secretRef:\n        name: test-secret\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"env-single-secret\". This Pod has a single container named \"envars-test-container\" with the image \"nginx\". The container has an environment variable named \"SECRET_USERNAME\" which gets its value from a secret named \"backend-user\" and the specific key within the secret is \"backend-username\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: env-single-secret\nspec:\n  containers:\n  - name: envars-test-container\n    image: nginx\n    env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: backend-user\n          key: backend-username\n\n```"}
{"prompt": "Write YAML that creates a Pod named \"envvars-multiple-secrets\" using the \"v1\" API version and kind \"Pod\". The Pod should have one container named \"envars-test-container\" using the NGINX image. The container should have two environment variables: \"BACKEND_USERNAME\" sourced from a secret named \"backend-user\" with the key \"backend-username\", and \"DB_USERNAME\" sourced from a secret named \"db-user\" with the key \"db-username\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: envvars-multiple-secrets\nspec:\n  containers:\n  - name: envars-test-container\n    image: nginx\n    env:\n    - name: BACKEND_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: backend-user\n          key: backend-username\n    - name: DB_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: db-user\n          key: db-username\n\n```"}
{"prompt": "Write YAML that defines a Pod resource named qos-demo-2 in the namespace qos-example. The Pod contains a single container named qos-demo-2-ctr, which is based on the nginx image. The container has resource limits of 200Mi for memory and requests 100Mi of memory.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: qos-demo-2\n  namespace: qos-example\nspec:\n  containers:\n  - name: qos-demo-2-ctr\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n      requests:\n        memory: \"100Mi\"\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"qos-demo-5\" in the \"qos-example\" namespace. The Pod contains a single container named \"qos-demo-ctr-5\" with the \"nginx\" image. The container has memory and CPU resource limits set to 200Mi and 700m respectively, and resource requests set to the same values.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: qos-demo-5\n  namespace: qos-example\nspec:\n  containers:\n  - name: qos-demo-ctr-5\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"700m\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"700m\"\n\n```"}
{"prompt": "Write YAML that creates a Pod named \"qos-demo-4\" in the \"qos-example\" namespace. The Pod has two containers: \"qos-demo-4-ctr-1\" with an image of Nginx and a requested memory of 200Mi, and \"qos-demo-4-ctr-2\" with an image of Redis.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: qos-demo-4\n  namespace: qos-example\nspec:\n  containers:\n\n  - name: qos-demo-4-ctr-1\n    image: nginx\n    resources:\n      requests:\n        memory: \"200Mi\"\n\n  - name: qos-demo-4-ctr-2\n    image: redis\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"qos-demo\" in the \"qos-example\" namespace. The Pod will run an nginx container with specified resource limits and requests. The resource limits are set to 200Mi of memory and 700m of CPU, while the resource requests are also set to 200Mi of memory and 700m of CPU.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: qos-demo\n  namespace: qos-example\nspec:\n  containers:\n  - name: qos-demo-ctr\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"700m\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"700m\"\n\n```"}
{"prompt": "Write YAML that describes a Pod resource with the name \"qos-demo-3\" and namespace \"qos-example\". The Pod contains a single container named \"qos-demo-3-ctr\" which uses the \"nginx\" image.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: qos-demo-3\n  namespace: qos-example\nspec:\n  containers:\n  - name: qos-demo-3-ctr\n    image: nginx\n\n```"}
{"prompt": "Write YAML that specifies a Deployment resource for an nginx application with 2 replicas. The Deployment selects pods with the label \"app: nginx\" and the pods use the nginx image with version 1.16.1. The pods are exposed on port 80.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.16.1 # Update the version of nginx from 1.14.2 to 1.16.1\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that defines a Pod object named \"shell-demo\" using the API version \"v1\". The Pod has one container named \"nginx\" and uses the \"nginx\" image. It mounts a volume named \"shared-data\" to the container's \"/usr/share/nginx/html\" directory. The Pod's network is shared with the host and its DNS policy is set to Default.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: shell-demo\nspec:\n  volumes:\n  - name: shared-data\n    emptyDir: {}\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: shared-data\n      mountPath: /usr/share/nginx/html\n  hostNetwork: true\n  dnsPolicy: Default\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Deployment for an nginx application. The Deployment will create and manage two pods with the nginx image version 1.14.2. The pods will expose port 80 for communication.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2 # tells deployment to run 2 pods matching the template\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes deployment named \"php-apache\" using the \"apps/v1\" API version. It specifies a PHP Apache container with an image from the \"registry.k8s.io/hpa-example\" repository, exposes port 80, and sets resource limits for CPU usage at 500m and CPU requests at 200m. Additionally, the YAML includes a service named \"php-apache\" that exposes port 80 and selects the deployment with the label \"run: php-apache\".", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: php-apache\nspec:\n  selector:\n    matchLabels:\n      run: php-apache\n  template:\n    metadata:\n      labels:\n        run: php-apache\n    spec:\n      containers:\n      - name: php-apache\n        image: registry.k8s.io/hpa-example\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 200m\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: php-apache\n  labels:\n    run: php-apache\nspec:\n  ports:\n  - port: 80\n  selector:\n    run: php-apache\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Deployment called \"nginx-deployment\" using the apps/v1 API version. It specifies that there should be 2 replicas of the deployment, and they should be managed by selecting pods with the label \"app: nginx\". The template for creating the pods includes a container named \"nginx\" using the nginx image, with resource limits of 128Mi of memory and 500m of CPU. The container also exposes port 80.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Deployment resource called \"nginx-deployment\" using the \"apps/v1\" API version. It specifies that there should be 4 replicas of the Pod template, which uses the label selector \"app=nginx\". The template also includes a container named \"nginx\" that uses the \"nginx:1.16.1\" image and exposes port 80.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 4 # Update the replicas from 2 to 4\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.16.1\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that specifies a Deployment resource of version \"apps/v1\" with the name \"retainkeys-demo\". The Deployment should use the rolling update strategy with a maximum surge of 30%. The Deployment should also have a template that includes a container named \"retainkeys-demo-ctr\" with the image \"nginx\".", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: retainkeys-demo\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  strategy:\n    rollingUpdate:\n      maxSurge: 30%\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: retainkeys-demo-ctr\n        image: nginx\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Deployment resource called \"nginx-deployment\" with an API version of \"apps/v1\". The deployment should have a selector that matches pods with the label \"app: nginx\" and a minimum readiness delay of 5 seconds. The template for the deployment should include a container named \"nginx\" with the image \"nginx:1.14.2\" that listens on port 80.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  minReadySeconds: 5\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Deployment resource named \"nginx-deployment\" using the apps/v1 API version. The Deployment will select Pods with the label \"app=nginx\" and use a template to create Pods with the same label. Within the Pod template, there is a container named \"nginx\" using the image \"nginx:1.16.1\" and exposing port 80.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.16.1 # update the image\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes deployment named \"patch-demo\" with 2 replicas. The deployment selects pods with the label \"app: nginx\" and creates pods based on the specified template. The template includes a single container named \"patch-demo-ctr\" with the nginx image. Additionally, it includes a toleration that allows the pods to be scheduled on nodes labeled with \"dedicated=test-team\" despite any NoSchedule taints.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: patch-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: patch-demo-ctr\n        image: nginx\n      tolerations:\n      - effect: NoSchedule\n        key: dedicated\n        value: test-team\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service object called \"my-nginx-svc\" with the label \"app: nginx\", specifying that it should be a LoadBalancer type and should listen on port 80. Also, define a Deployment object called \"my-nginx\" with the label \"app: nginx\", specifying that it should have 3 replicas and use the \"nginx:1.14.2\" image. The Deployment should expose port 80 in its containers. Both the Service and Deployment should have the same selector label (\"app: nginx\") to ensure they are associated with each other.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-nginx-svc\n  labels:\n    app: nginx\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that creates a CronJob resource in Kubernetes with the name \"hello\". This CronJob will run a job at every minute. The job will use the busybox:1.28 image and execute the command \"date; echo Hello from the Kubernetes cluster\" inside the container. The restart policy is set to \"OnFailure\", meaning the job will restart only if it fails.", "completion": "```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"* * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox:1.28\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Job with the following specifications: It uses the API version `batch/v1` and the kind `Job`. The name of the job is set to `'indexed-job'`. The job has `5` completions and allows `3` parallel executions. The completion mode is set to `Indexed`. The template of the job consists of a single container named `'worker'` that uses the `busybox` image from the `docker.io/library` repository. The container runs the command `\"rev /input/data.txt\"`. It also mounts a volume named `'input'` at the path `/input` and uses the downwardAPI to retrieve the value of the annotation `metadata.annotations['batch.kubernetes.io/job-completion-index']` and saves it as `data.txt` in the mounted volume.", "completion": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 'indexed-job'\nspec:\n  completions: 5\n  parallelism: 3\n  completionMode: Indexed\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: 'worker'\n        image: 'docker.io/library/busybox'\n        command:\n        - \"rev\"\n        - \"/input/data.txt\"\n        volumeMounts:\n        - mountPath: /input\n          name: input\n      volumes:\n      - name: input\n        downwardAPI:\n          items:\n          - path: \"data.txt\"\n            fieldRef:\n              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']\n```"}
{"prompt": "Write YAML that specifies a Job resource with the apiVersion set to batch/v1 and kind set to Job. The metadata section includes the name of the Job as 'indexed-job'. The spec section includes the completions set to 5, parallelism set to 3, completionMode set to Indexed. The template section includes the spec for the container, with restartPolicy set to Never. The template also includes an initContainer named 'input' with an image specified as 'docker.io/library/bash'. The initContainer has a command defined to execute a bash script, which sets an array 'items' and writes the element at the index defined by $JOB_COMPLETION_INDEX to a file called data.txt. The template also includes a container named 'worker' with an image specified as 'docker.io/library/busybox'. The worker container has a command to perform the 'rev' operation on the data.txt file. Finally, the template includes a volume named 'input' with emptyDir as its type.", "completion": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: 'indexed-job'\nspec:\n  completions: 5\n  parallelism: 3\n  completionMode: Indexed\n  template:\n    spec:\n      restartPolicy: Never\n      initContainers:\n      - name: 'input'\n        image: 'docker.io/library/bash'\n        command:\n        - \"bash\"\n        - \"-c\"\n        - |\n          items=(foo bar baz qux xyz)\n          echo ${items[$JOB_COMPLETION_INDEX]} > /input/data.txt\n        volumeMounts:\n        - mountPath: /input\n          name: input\n      containers:\n      - name: 'worker'\n        image: 'docker.io/library/busybox'\n        command:\n        - \"rev\"\n        - \"/input/data.txt\"\n        volumeMounts:\n        - mountPath: /input\n          name: input\n      volumes:\n      - name: input\n        emptyDir: {}\n\n```"}
{"prompt": "Write YAML that defines a job with the apiVersion batch/v1 and kind Job. The metadata includes the name of the job, which includes a placeholder variable $ITEM. The spec includes the definition of a container named \"c\" with an image, command, and restart policy.", "completion": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: process-item-$ITEM\n  labels:\n    jobgroup: jobexample\nspec:\n  template:\n    metadata:\n      name: jobexample\n      labels:\n        jobgroup: jobexample\n    spec:\n      containers:\n      - name: c\n        image: busybox:1.28\n        command: [\"sh\", \"-c\", \"echo Processing item $ITEM && sleep 5\"]\n      restartPolicy: Never\n\n```"}
{"prompt": "Write YAML that creates a Job object named \"job-wq-1\" in the batch/v1 API version with 8 completions and a parallelism of 2. The Job's template contains a container named \"c\" with an image specified as \"gcr.io/<project>/job-wq-1\". The container also has two environment variables, BROKER_URL and QUEUE, with their respective values set. The restart policy for the Job is set to \"OnFailure\".", "completion": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-wq-1\nspec:\n  completions: 8\n  parallelism: 2\n  template:\n    metadata:\n      name: job-wq-1\n    spec:\n      containers:\n      - name: c\n        image: gcr.io/<project>/job-wq-1\n        env:\n        - name: BROKER_URL\n          value: amqp://guest:guest@rabbitmq-service:5672\n        - name: QUEUE\n          value: job1\n      restartPolicy: OnFailure\n\n```"}
{"prompt": "Write YAML that defines a Job resource with the apiVersion set to \"batch/v1\" and kind set to \"Job\". The metadata section sets the name of the job to \"job-wq-2\". The spec section sets the parallelism to 2 and includes a template for the job's containers, where one container named \"c\" is defined with the image \"gcr.io/myproject/job-wq-2\". The restartPolicy is set to \"OnFailure\".", "completion": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-wq-2\nspec:\n  parallelism: 2\n  template:\n    metadata:\n      name: job-wq-2\n    spec:\n      containers:\n      - name: c\n        image: gcr.io/myproject/job-wq-2\n      restartPolicy: OnFailure\n\n```"}
{"prompt": "Write YAML that creates a Service object in Kubernetes with the name \"redis\". It specifies that the Service should listen on port 6379 and forward the traffic to the same port of the target (6379). The Service is associated with the pods that have the label \"app: redis\".\n\nIn this YAML, the \"apiVersion\" field specifies the version of the Kubernetes API being used. The \"kind\" field defines the type of object being created, which is a Service in this case. The \"metadata\" field contains additional information about the Service, such as its name. The \"spec\" field defines the desired state of the Service, including its ports and selector.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis\nspec:\n  ports:\n    - port: 6379\n      targetPort: 6379\n  selector:\n    app: redis\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"redis-master\" with an image of \"redis\". The Pod has one container named \"master\" with an environment variable named \"MASTER\" set to \"true\". The container also exposes a port on 6379.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  containers:\n    - name: master\n      image: redis\n      env:\n        - name: MASTER\n          value: \"true\"\n      ports:\n        - containerPort: 6379\n\n```"}
{"prompt": "Write YAML that represents a HorizontalPodAutoscaler resource in Kubernetes. The resource is named \"php-apache\" and it is set to scale the \"Deployment\" named \"php-apache\". It is configured to have a minimum of 1 replica and a maximum of 10 replicas, targeting a CPU utilization of 50%.", "completion": "```yaml\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: php-apache\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: php-apache\n  minReplicas: 1\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 50\n\n```"}
{"prompt": "Write YAML that creates a StatefulSet named \"cassandra\" with 3 replicas. The StatefulSet uses the image \"gcr.io/google-samples/cassandra:v13\" and has ports 7000, 7001, 7199, and 9042 exposed. It also sets resource limits for CPU and memory, adds capabilities, specifies preStop lifecycle command, defines environment variables, and sets readinessProbe.\n\nAdditionally, the YAML includes a volumeClaimTemplates section that defines a persistent storage volume for the StatefulSet, with a storageClassName of \"fast\" and a storage size of 1Gi.\n\nLastly, the YAML creates a StorageClass named \"fast\" with the provisioner \"k8s.io/minikube-hostpath\" and specifies that it uses pd-ssd as the type of storage.", "completion": "```yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: cassandra\n  labels:\n    app: cassandra\nspec:\n  serviceName: cassandra\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cassandra\n  template:\n    metadata:\n      labels:\n        app: cassandra\n    spec:\n      terminationGracePeriodSeconds: 1800\n      containers:\n      - name: cassandra\n        image: gcr.io/google-samples/cassandra:v13\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 7000\n          name: intra-node\n        - containerPort: 7001\n          name: tls-intra-node\n        - containerPort: 7199\n          name: jmx\n        - containerPort: 9042\n          name: cql\n        resources:\n          limits:\n            cpu: \"500m\"\n            memory: 1Gi\n          requests:\n            cpu: \"500m\"\n            memory: 1Gi\n        securityContext:\n          capabilities:\n            add:\n              - IPC_LOCK\n        lifecycle:\n          preStop:\n            exec:\n              command: \n              - /bin/sh\n              - -c\n              - nodetool drain\n        env:\n          - name: MAX_HEAP_SIZE\n            value: 512M\n          - name: HEAP_NEWSIZE\n            value: 100M\n          - name: CASSANDRA_SEEDS\n            value: \"cassandra-0.cassandra.default.svc.cluster.local\"\n          - name: CASSANDRA_CLUSTER_NAME\n            value: \"K8Demo\"\n          - name: CASSANDRA_DC\n            value: \"DC1-K8Demo\"\n          - name: CASSANDRA_RACK\n            value: \"Rack1-K8Demo\"\n          - name: POD_IP\n            valueFrom:\n              fieldRef:\n                fieldPath: status.podIP\n        readinessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - -c\n            - /ready-probe.sh\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n        # These volume mounts are persistent. They are like inline claims,\n        # but not exactly because the names need to match exactly one of\n        # the stateful pod volumes.\n        volumeMounts:\n        - name: cassandra-data\n          mountPath: /cassandra_data\n  # These are converted to volume claims by the controller\n  # and mounted at the paths mentioned above.\n  # do not use these in production until ssd GCEPersistentDisk or other ssd pd\n  volumeClaimTemplates:\n  - metadata:\n      name: cassandra-data\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      storageClassName: fast\n      resources:\n        requests:\n          storage: 1Gi\n---\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: fast\nprovisioner: k8s.io/minikube-hostpath\nparameters:\n  type: pd-ssd\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service object named 'cassandra' with the label 'app' set to 'cassandra'. The service has no cluster IP and exposes port 9042. It selects pods with the label 'app' set to 'cassandra'.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: cassandra\n  name: cassandra\nspec:\n  clusterIP: None\n  ports:\n  - port: 9042\n  selector:\n    app: cassandra\n\n```"}
{"prompt": "Write YAML that defines a PersistentVolume named \"mysql-pv-volume\" with a capacity of 20Gi and using a hostPath of \"/mnt/data\". It is labeled as \"local\" and has a storageClassName of \"manual\". \n\nThen, write YAML that defines a PersistentVolumeClaim named \"mysql-pv-claim\" with accessModes set to ReadWriteOnce and a storageClassName of \"manual\". It requests a storage capacity of 20Gi.", "completion": "```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: mysql-pv-volume\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 20Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/mnt/data\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n\n```"}
{"prompt": "Write YAML that defines a Service named \"mysql\" that runs on port 3306 and has a selector for an app named \"mysql\". The service has a clusterIP of \"None\". Then, define a Deployment also named \"mysql\" that has a selector for the same app. The deployment uses a Recreate strategy and runs a container with the image \"mysql:5.6\". The container has an environment variable MYSQL_ROOT_PASSWORD with a value of \"password\". The container exposes port 3306 and mounts a persistent volume at /var/lib/mysql.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  ports:\n  - port: 3306\n  selector:\n    app: mysql\n  clusterIP: None\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n          # Use secret in real usage\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pv-claim\n\n```"}
{"prompt": "Write YAML that creates a headless service named \"mysql\" with no cluster IP and label selectors for the mysql application. Additionally, write YAML that creates a client service named \"mysql-read\" with label selectors for the mysql application and specifies that it is read-only. Both services use the MySQL default port 3306.", "completion": "```yaml\n# Headless service for stable DNS entries of StatefulSet members.\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\n    app.kubernetes.io/name: mysql\nspec:\n  ports:\n  - name: mysql\n    port: 3306\n  clusterIP: None\n  selector:\n    app: mysql\n---\n# Client service for connecting to any MySQL instance for reads.\n# For writes, you must instead connect to the primary: mysql-0.mysql.\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-read\n  labels:\n    app: mysql\n    app.kubernetes.io/name: mysql\n    readonly: \"true\"\nspec:\n  ports:\n  - name: mysql\n    port: 3306\n  selector:\n    app: mysql\n\n```"}
{"prompt": "Write YAML that represents a Kubernetes ConfigMap named \"mysql\" with two key-value pairs in the \"data\" section. The first key is \"primary.cnf\" with the value set as a multiline string containing MySQL configuration specific to the primary instance. The second key is \"replica.cnf\" with the value set as another multiline string containing MySQL configuration specific to replicas.", "completion": "```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\n    app.kubernetes.io/name: mysql\ndata:\n  primary.cnf: |\n    # Apply this config only on the primary.\n    [mysqld]\n    log-bin\n  replica.cnf: |\n    # Apply this config only on replicas.\n    [mysqld]\n    super-read-only\n\n\n```"}
{"prompt": "Write YAML that defines a StatefulSet resource named \"mysql\" with 3 replicas. It uses initContainers to generate the mysql server-id based on the pod's ordinal index and copies appropriate conf.d files from a config-map to an emptyDir. It also specifies containers for the MySQL database and xtrabackup, with volume mounts for data and conf directories, resource limits, and liveness and readiness probes. Finally, it includes a volumeClaimTemplate for data with a storage request of 10Gi.", "completion": "```yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n      app.kubernetes.io/name: mysql\n  serviceName: mysql\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mysql\n        app.kubernetes.io/name: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:5.7\n        command:\n        - bash\n        - \"-c\"\n        - |\n          set -ex\n          # Generate mysql server-id from pod ordinal index.\n          [[ $HOSTNAME =~ -([0-9]+)$ ]] || exit 1\n          ordinal=${BASH_REMATCH[1]}\n          echo [mysqld] > /mnt/conf.d/server-id.cnf\n          # Add an offset to avoid reserved server-id=0 value.\n          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\n          # Copy appropriate conf.d files from config-map to emptyDir.\n          if [[ $ordinal -eq 0 ]]; then\n            cp /mnt/config-map/primary.cnf /mnt/conf.d/\n          else\n            cp /mnt/config-map/replica.cnf /mnt/conf.d/\n          fi\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n        - name: config-map\n          mountPath: /mnt/config-map\n      - name: clone-mysql\n        image: gcr.io/google-samples/xtrabackup:1.0\n        command:\n        - bash\n        - \"-c\"\n        - |\n          set -ex\n          # Skip the clone if data already exists.\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\n          # Skip the clone on primary (ordinal index 0).\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n          ordinal=${BASH_REMATCH[1]}\n          [[ $ordinal -eq 0 ]] && exit 0\n          # Clone data from previous peer.\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n          # Prepare the backup.\n          xtrabackup --prepare --target-dir=/var/lib/mysql\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      containers:\n      - name: mysql\n        image: mysql:5.7\n        env:\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n          value: \"1\"\n        ports:\n        - name: mysql\n          containerPort: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n        livenessProbe:\n          exec:\n            command: [\"mysqladmin\", \"ping\"]\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            # Check we can execute queries over TCP (skip-networking is off).\n            command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"SELECT 1\"]\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          timeoutSeconds: 1\n      - name: xtrabackup\n        image: gcr.io/google-samples/xtrabackup:1.0\n        ports:\n        - name: xtrabackup\n          containerPort: 3307\n        command:\n        - bash\n        - \"-c\"\n        - |\n          set -ex\n          cd /var/lib/mysql\n\n          # Determine binlog position of cloned data, if any.\n          if [[ -f xtrabackup_slave_info && \"x$(<xtrabackup_slave_info)\" != \"x\" ]]; then\n            # XtraBackup already generated a partial \"CHANGE MASTER TO\" query\n            # because we're cloning from an existing replica. (Need to remove the tailing semicolon!)\n            cat xtrabackup_slave_info | sed -E 's/;$//g' > change_master_to.sql.in\n            # Ignore xtrabackup_binlog_info in this case (it's useless).\n            rm -f xtrabackup_slave_info xtrabackup_binlog_info\n          elif [[ -f xtrabackup_binlog_info ]]; then\n            # We're cloning directly from primary. Parse binlog position.\n            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\n            rm -f xtrabackup_binlog_info xtrabackup_slave_info\n            echo \"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\n                  MASTER_LOG_POS=${BASH_REMATCH[2]}\" > change_master_to.sql.in\n          fi\n\n          # Check if we need to complete a clone by starting replication.\n          if [[ -f change_master_to.sql.in ]]; then\n            echo \"Waiting for mysqld to be ready (accepting connections)\"\n            until mysql -h 127.0.0.1 -e \"SELECT 1\"; do sleep 1; done\n\n            echo \"Initializing replication from clone position\"\n            mysql -h 127.0.0.1 \\\n                  -e \"$(<change_master_to.sql.in), \\\n                          MASTER_HOST='mysql-0.mysql', \\\n                          MASTER_USER='root', \\\n                          MASTER_PASSWORD='', \\\n                          MASTER_CONNECT_RETRY=10; \\\n                        START SLAVE;\" || exit 1\n            # In case of container restart, attempt this at-most-once.\n            mv change_master_to.sql.in change_master_to.sql.orig\n          fi\n\n          # Start a server to send backups when requested by peers.\n          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\\n            \"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n          subPath: mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: conf\n        emptyDir: {}\n      - name: config-map\n        configMap:\n          name: mysql\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 10Gi\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes service named \"mongo\" with labels specifying its name as \"mongo\" and component as \"backend\". The service exposes port 27017 and redirects traffic to the same target port. The service is configured to route requests to backend components that have the labels \"mongo\" as the name and \"backend\" as the component.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongo\n  labels:\n    app.kubernetes.io/name: mongo\n    app.kubernetes.io/component: backend\nspec:\n  ports:\n  - port: 27017\n    targetPort: 27017\n  selector:\n    app.kubernetes.io/name: mongo\n    app.kubernetes.io/component: backend\n\n```"}
{"prompt": "Write YAML that specifies a Deployment resource in Kubernetes with the name \"mongo\" and labels indicating that it is part of the backend component. The Deployment has replicas set to 1 and the pod template has labels matching those of the Deployment. The pod template also includes a container named \"mongo\" with the image \"mongo:4.2\", arguments specifying that the container should bind to IP 0.0.0.0, resource requests set to 100m CPU and 100Mi memory, and a port mapping for container port 27017.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app.kubernetes.io/name: mongo\n    app.kubernetes.io/component: backend\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mongo\n      app.kubernetes.io/component: backend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mongo\n        app.kubernetes.io/component: backend\n    spec:\n      containers:\n      - name: mongo\n        image: mongo:4.2\n        args:\n          - --bind_ip\n          - 0.0.0.0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 27017\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Service named \"zk-hs\" with two ports: port 2888 named \"server\" and port 3888 named \"leader-election\". The Service has no cluster IP and is selected by the labels \"app: zk\".\nWrite YAML that describes a Kubernetes Service named \"zk-cs\" with port 2181 named \"client\". The Service is selected by the labels \"app: zk\".\nWrite YAML that describes a Kubernetes Pod Disruption Budget named \"zk-pdb\" which allows a maximum of 1 unavailable pod. The PDB selects pods with the label \"app: zk\".", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: zk-hs\n  labels:\n    app: zk\nspec:\n  ports:\n  - port: 2888\n    name: server\n  - port: 3888\n    name: leader-election\n  clusterIP: None\n  selector:\n    app: zk\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: zk-cs\n  labels:\n    app: zk\nspec:\n  ports:\n  - port: 2181\n    name: client\n  selector:\n    app: zk\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: zk-pdb\nspec:\n  selector:\n    matchLabels:\n      app: zk\n  maxUnavailable: 1\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zk\nspec:\n  selector:\n    matchLabels:\n      app: zk\n  serviceName: zk-hs\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  podManagementPolicy: OrderedReady\n  template:\n    metadata:\n      labels:\n        app: zk\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - zk\n              topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: kubernetes-zookeeper\n        imagePullPolicy: Always\n        image: \"registry.k8s.io/kubernetes-zookeeper:1.0-3.4.10\"\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"0.5\"\n        ports:\n        - containerPort: 2181\n          name: client\n        - containerPort: 2888\n          name: server\n        - containerPort: 3888\n          name: leader-election\n        command:\n        - sh\n        - -c\n        - \"start-zookeeper \\\n          --servers=3 \\\n          --data_dir=/var/lib/zookeeper/data \\\n          --data_log_dir=/var/lib/zookeeper/data/log \\\n          --conf_dir=/opt/zookeeper/conf \\\n          --client_port=2181 \\\n          --election_port=3888 \\\n          --server_port=2888 \\\n          --tick_time=2000 \\\n          --init_limit=10 \\\n          --sync_limit=5 \\\n          --heap=512M \\\n          --max_client_cnxns=60 \\\n          --snap_retain_count=3 \\\n          --purge_interval=12 \\\n          --max_session_timeout=40000 \\\n          --min_session_timeout=4000 \\\n          --log_level=INFO\"\n        readinessProbe:\n          exec:\n            command:\n            - sh\n            - -c\n            - \"zookeeper-ready 2181\"\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - sh\n            - -c\n            - \"zookeeper-ready 2181\"\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/zookeeper\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 10Gi\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Deployment named \"my-nginx\" with 3 replicas. The Deployment selects Pods with the label \"app: nginx\". The Pod template specifies a container named \"nginx\" using the image \"nginx:1.14.2\" and exposes port 80.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Service resource named \"my-nginx-svc\" labeled as \"nginx\". It specifies the resource type as \"LoadBalancer\" and exposes port 80. The service is associated with pods that have the label \"app: nginx\", allowing the traffic to be directed to those pods.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-nginx-svc\n  labels:\n    app: nginx\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n  selector:\n    app: nginx\n\n```"}
{"prompt": "Write YAML that creates a Kubernetes Service resource named \"nginx\" with label \"app: nginx\", exposing port 80 and having no cluster IP address. Then, create a Kubernetes StatefulSet resource named \"web\" with serviceName as \"nginx\", running 2 replicas in parallel pod management policy, and matching labels \"app: nginx\". The StatefulSet includes a container named \"nginx\" running the image \"registry.k8s.io/nginx-slim:0.8\", exposing port 80, and mounting a volume named \"www\" to the path \"/usr/share/nginx/html\" with a volumeClaimTemplate named \"www\" having access mode \"ReadWriteOnce\" and requesting storage of 1Gi.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  clusterIP: None\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: \"nginx\"\n  podManagementPolicy: \"Parallel\"\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: registry.k8s.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Service named \"nginx\" with label \"app: nginx\", exposing port 80 and with no clusterIP assigned. The Service is used to select the StatefulSet using the \"app: nginx\" label.\n\nWrite YAML that describes a Kubernetes StatefulSet named \"web\" with serviceName \"nginx\", deploying 2 replicas. The StatefulSet selects pods with the \"app: nginx\" label and uses a template to create pods with the nginx container from the \"registry.k8s.io/nginx-slim:0.8\" image. The container listens on port 80 and mounts a volume named \"www\" to the path /usr/share/nginx/html. The volume has a volume claim template named \"www\" with a storage request of 1Gi and \"ReadWriteOnce\" access mode.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  clusterIP: None\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: \"nginx\"\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: registry.k8s.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n\n\n```"}
{"prompt": "Write YAML that defines three Kubernetes resources: a Service, a PersistentVolumeClaim, and a Deployment. The Service resource is named \"wordpress\" and uses port 80, with a LoadBalancer type. The PersistentVolumeClaim is named \"wp-pv-claim\" and requests 20Gi of storage. The Deployment is also named \"wordpress\" and uses the wordpress:4.8-apache image, with environment variables for the database host and password, and mounts a persistent storage volume at /var/www/html.\n", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: wordpress\n  labels:\n    app: wordpress\nspec:\n  ports:\n    - port: 80\n  selector:\n    app: wordpress\n    tier: frontend\n  type: LoadBalancer\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: wp-pv-claim\n  labels:\n    app: wordpress\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress\n  labels:\n    app: wordpress\nspec:\n  selector:\n    matchLabels:\n      app: wordpress\n      tier: frontend\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: wordpress\n        tier: frontend\n    spec:\n      containers:\n      - image: wordpress:4.8-apache\n        name: wordpress\n        env:\n        - name: WORDPRESS_DB_HOST\n          value: wordpress-mysql\n        - name: WORDPRESS_DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 80\n          name: wordpress\n        volumeMounts:\n        - name: wordpress-persistent-storage\n          mountPath: /var/www/html\n      volumes:\n      - name: wordpress-persistent-storage\n        persistentVolumeClaim:\n          claimName: wp-pv-claim\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service called wordpress-mysql with a label app: wordpress. This Service exposes port 3306, and it selects pods with the labels app: wordpress and tier: mysql. The Service has a clusterIP of None, meaning it is not accessible from outside the cluster.\n\nWrite YAML that defines a PersistentVolumeClaim called mysql-pv-claim with a label app: wordpress. This claim requests a storage capacity of 20Gi and supports ReadWriteOnce access mode.\n\nWrite YAML that defines a Kubernetes Deployment called wordpress-mysql with a label app: wordpress. This Deployment selects pods with the labels app: wordpress and tier: mysql. The Deployment uses the Recreate strategy and deploys a container using MySQL 5.6 image. The container is named mysql and mounts the persistent volume claim mysql-pv-claim to the directory /var/lib/mysql. The container also sets the environment variable MYSQL_ROOT_PASSWORD using a secret named mysql-pass and key password. The container exposes port 3306.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: wordpress-mysql\n  labels:\n    app: wordpress\nspec:\n  ports:\n    - port: 3306\n  selector:\n    app: wordpress\n    tier: mysql\n  clusterIP: None\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pv-claim\n  labels:\n    app: wordpress\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wordpress-mysql\n  labels:\n    app: wordpress\nspec:\n  selector:\n    matchLabels:\n      app: wordpress\n      tier: mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: wordpress\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql:5.6\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pv-claim\n\n```"}
{"prompt": "Write YAML that defines a deployment called \"nginx-deployment\" using the apps/v1 API version. It specifies that the deployment should have 3 replicas and the selector should match labels with \"app: nginx\". The template section defines a container named \"nginx\" using the nginx:1.14.2 image.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Deployment resource for an nginx application. The Deployment is defined under the `apps/v1` API version and is named \"nginx-deployment\" with the label \"app: nginx\". The Deployment creates a single replica of the nginx container using the \"nginx:1.14.2\" image.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service object named \"redis-leader\" with labels for app, role, and tier. The Service will expose port 6379 and route traffic to pods that have the same labels for app, role, and tier (app: redis, role: leader, tier: backend). This YAML can be used to deploy the Redis leader service in a Kubernetes cluster.", "completion": "```yaml\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-leader\n  labels:\n    app: redis\n    role: leader\n    tier: backend\nspec:\n  ports:\n  - port: 6379\n    targetPort: 6379\n  selector:\n    app: redis\n    role: leader\n    tier: backend\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service named \"redis-follower\" with the labels \"app: redis\", \"role: follower\", and \"tier: backend\". This Service listens on port 6379 and selects pods with the labels \"app: redis\", \"role: follower\", and \"tier: backend\".", "completion": "```yaml\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-follower\n  labels:\n    app: redis\n    role: follower\n    tier: backend\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 6379\n  selector:\n    app: redis\n    role: follower\n    tier: backend\n```"}
{"prompt": "Write YAML that defines a Deployment named \"frontend\" with 3 replicas. The Deployment selects pods with the labels \"app: guestbook\" and \"tier: frontend\". The Deployment's template specifies a container named \"php-redis\" with the image \"gcr.io/google_samples/gb-frontend:v5\". The container has an environment variable named \"GET_HOSTS_FROM\" with the value set to \"dns\". It also defines resource requests for CPU and memory and exposes port 80.", "completion": "```yaml\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n        app: guestbook\n        tier: frontend\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v5\n        env:\n        - name: GET_HOSTS_FROM\n          value: \"dns\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n```"}
{"prompt": "Write YAML that creates a Kubernetes Deployment object called \"redis-leader\" with one replica. The Deployment has a selector that matches the labels \"app: redis\" and a container named \"leader\" which uses the Redis 6.0.5 image from Docker Hub. The container has resource requests for 100m CPU and 100Mi memory, and exposes port 6379.", "completion": "```yaml\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-leader\n  labels:\n    app: redis\n    role: leader\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: leader\n        tier: backend\n    spec:\n      containers:\n      - name: leader\n        image: \"docker.io/redis:6.0.5\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n```"}
{"prompt": "Write YAML that defines a Deployment resource in Kubernetes called \"redis-follower\" with two replicas. The deployment has a selector that matches labels with app: redis and the template has a container named \"follower\" running the image gcr.io/google_samples/gb-redis-follower:v2 with resource requests for cpu and memory, and exposes port 6379.", "completion": "```yaml\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-follower\n  labels:\n    app: redis\n    role: follower\n    tier: backend\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: follower\n        tier: backend\n    spec:\n      containers:\n      - name: follower\n        image: gcr.io/google_samples/gb-redis-follower:v2\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service named \"frontend\" with labels \"app: guestbook\" and \"tier: frontend\". The Service has a port specified as 80 and selects Pods with labels \"app: guestbook\" and \"tier: frontend\" to distribute traffic to.", "completion": "```yaml\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\nspec:\n  # if your cluster supports it, uncomment the following to automatically create\n  # an external load-balanced IP for the frontend service.\n  # type: LoadBalancer\n  #type: LoadBalancer\n  ports:\n    # the port that this service should serve on\n  - port: 80\n  selector:\n    app: guestbook\n    tier: frontend\n```"}
{"prompt": "Write YAML that describes a Kubernetes Service named \"nginx-service\" with the following specifications: it should serve on port 8000 and forward traffic to the container on each pod using port 80. The protocol used should be TCP. The Service should load balance traffic to pods with the label \"app: nginx\".\n", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  ports:\n  - port: 8000 # the port that this service should serve on\n    # the container on each pod to connect to, can be a name\n    # (e.g. 'www') or a number (e.g. 80)\n    targetPort: 80\n    protocol: TCP\n  # just like the selector in the deployment,\n  # but this time it identifies the set of pods to load balance\n  # traffic to.\n  selector:\n    app: nginx\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Deployment with the name \"hello-world\" and creates 5 replicas. The Deployment selector matches the labels \"app.kubernetes.io/name: load-balancer-example\". The Deployment template includes a container called \"hello-world\" with the image gcr.io/google-samples/node-hello:1.0, and exposes port 8080.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: load-balancer-example\n  name: hello-world\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: load-balancer-example\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: load-balancer-example\n    spec:\n      containers:\n      - image: gcr.io/google-samples/node-hello:1.0\n        name: hello-world\n        ports:\n        - containerPort: 8080\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Deployment object named \"nginx-deployment\" with one replica and uses the \"nginx\" image. The Deployment has a terminationGracePeriodSeconds of 120 to allow for a longer grace period before terminating the container. The container runs on port 80 and has a preStop lifecycle hook that runs a command to sleep for 180 seconds before terminating the container.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      terminationGracePeriodSeconds: 120 # extra long grace period\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        lifecycle:\n          preStop:\n            exec:\n              # Real life termination may take any time up to terminationGracePeriodSeconds.\n              # In this example - just hang around for at least the duration of terminationGracePeriodSeconds,\n              # at 120 seconds container will be forcibly terminated.\n              # Note, all this time nginx will keep processing requests.\n              command: [\n                \"/bin/sh\", \"-c\", \"sleep 180\"\n              ]\n\n```"}
{"prompt": "Write YAML that defines a Deployment object named \"backend\" with 3 replicas. It uses a selector to match labels with values \"app: hello\", \"tier: backend\", and \"track: stable\". The Deployment object includes a Pod template with a container named \"hello\" using the image \"gcr.io/google-samples/hello-go-gke:1.0\" and exposing port 80.", "completion": "```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\nspec:\n  selector:\n    matchLabels:\n      app: hello\n      tier: backend\n      track: stable\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: hello\n        tier: backend\n        track: stable\n    spec:\n      containers:\n        - name: hello\n          image: \"gcr.io/google-samples/hello-go-gke:1.0\"\n          ports:\n            - name: http\n              containerPort: 80\n...\n```"}
{"prompt": "Write YAML that specifies a Deployment resource called \"hello-world\" with 2 replicas. The Deployment should select pods with the label \"run: load-balancer-example\" and the pods should run a container named \"hello-world\" with the image \"gcr.io/google-samples/node-hello:1.0\". The container should expose port 8080 using the TCP protocol.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\nspec:\n  selector:\n    matchLabels:\n      run: load-balancer-example\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: load-balancer-example\n    spec:\n      containers:\n        - name: hello-world\n          image: gcr.io/google-samples/node-hello:1.0\n          ports:\n            - containerPort: 8080\n              protocol: TCP\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Deployment object named \"frontend\" with a single replica. The Deployment should have a selector with matchLabels set to app: hello, tier: frontend, and track: stable. The template within the Deployment should have a container named \"nginx\" that uses the \"gcr.io/google-samples/hello-frontend:1.0\" image. Additionally, the container should have a preStop lifecycle hook that runs the command \"/usr/sbin/nginx -s quit\" before the container is terminated.", "completion": "```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: hello\n      tier: frontend\n      track: stable\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: hello\n        tier: frontend\n        track: stable\n    spec:\n      containers:\n        - name: nginx\n          image: \"gcr.io/google-samples/hello-frontend:1.0\"\n          lifecycle:\n            preStop:\n              exec:\n                command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"]\n...\n```"}
{"prompt": "Write YAML that describes a Kubernetes Service resource named \"hello\" with a selector that matches pods with labels \"app: hello\" and \"tier: backend\". The service listens on port 80 using TCP protocol and forwards the incoming traffic to pods targeted by the label \"http\".\nThis YAML defines a Kubernetes Service resource using the v1 API version. It specifies the metadata of the service, including its name. The selector field is used to match pods with the specified labels, and the ports field describes the port configuration, including the protocol, port number, and the target port name or number.", "completion": "```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hello\nspec:\n  selector:\n    app: hello\n    tier: backend\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: http\n...\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service named \"frontend\" with a selector to match pods labeled as \"app: hello\" and \"tier: frontend\". The service exposes port 80 and forwards traffic to pods on port 80. The service is of type \"LoadBalancer\", allowing it to be accessible from outside the cluster.", "completion": "```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\nspec:\n  selector:\n    app: hello\n    tier: frontend\n  ports:\n  - protocol: \"TCP\"\n    port: 80\n    targetPort: 80\n  type: LoadBalancer\n...\n```"}
{"prompt": "Write YAML that defines an Ingress resource named \"test-ingress\" with the API version \"networking.k8s.io/v1\". This Ingress resource has a default backend service named \"test\" with port number 80.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: test-ingress\nspec:\n  defaultBackend:\n    service:\n      name: test\n      port:\n        number: 80\n\n```"}
{"prompt": "Write YAML that defines a NetworkPolicy resource in Kubernetes. The NetworkPolicy is named \"access-nginx\" and its purpose is to control network traffic to pods with the label \"app: nginx\". The policy allows ingress (incoming) traffic only from pods with the label \"access: true\".", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: access-nginx\nspec:\n  podSelector:\n    matchLabels:\n      app: nginx\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          access: \"true\"\n\n```"}
{"prompt": "Write YAML that creates a Kubernetes service named \"my-service\" with the label \"app.kubernetes.io/name: MyApp\". The service has a preferred IP family policy of \"PreferDualStack\" and the selector is set to \"app.kubernetes.io/name: MyApp\". It also exposes port 80 using TCP protocol.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  ipFamilyPolicy: PreferDualStack\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n\n```"}
{"prompt": "Write YAML that defines an Ingress resource named \"example-ingress\" with an annotation for nginx-ingress to rewrite the target path. It specifies a rule for the host \"hello-world.info\" and a path of \"/\" with a pathType of \"Prefix\". The backend is a service named \"web\" with port 8080.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\nspec:\n  rules:\n    - host: hello-world.info\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: web\n                port:\n                  number: 8080\n```"}
{"prompt": "Write YAML that represents a Kubernetes Deployment resource object with the name \"my-nginx\", with two replicas. The deployment targets pods with the label \"run=my-nginx\". Inside the pod template, there is a container named \"my-nginx\" that runs the nginx image and exposes port 80.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n\n\n```"}
{"prompt": "Write YAML that defines an Ingress Class resource in Kubernetes. The resource has the `nginx-example` as its name and `networking.k8s.io/v1` as its API version. It also has a label indicating that it is a controller and an annotation indicating that it is the default class. The `controller` field specifies that the Ingress controller used is `k8s.io/ingress-nginx`.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n  name: nginx-example\n  annotations:\n    ingressclass.kubernetes.io/is-default-class: \"true\"\nspec:\n  controller: k8s.io/ingress-nginx\n\n```"}
{"prompt": "Write YAML that describes an Ingress resource with three rules. The first rule directs traffic with the host \"first.bar.com\" to the service named \"service1\" on port 80. The second rule directs traffic with the host \"second.bar.com\" to the service named \"service2\" on port 80. The third rule directs all other traffic to the service named \"service3\" on port 80.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: name-virtual-host-ingress-no-third-host\nspec:\n  rules:\n  - host: first.bar.com\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: service1\n            port:\n              number: 80\n  - host: second.bar.com\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: service2\n            port:\n              number: 80\n  - http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: service3\n            port:\n              number: 80\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service named \"my-nginx\" with type NodePort, exposing ports 8080 and 443 for TCP traffic. The Service selects for Pods with the label \"run: my-nginx\". \n\nAdditionally, write YAML that defines a Deployment named \"my-nginx\" with 1 replica. The Deployment selects for Pods with the label \"run: my-nginx\" and mounts a secret named \"nginxsecret\" to the volume \"secret-volume\" and a config map named \"nginxconfigmap\" to the volume \"configmap-volume\". The Deployment also specifies a container named \"nginxhttps\" using the image \"bprashanth/nginxhttps:1.0\" with ports 443 and 80 exposed. The secret and config map volumes are mounted to specific paths in the container.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-nginx\n  labels:\n    run: my-nginx\nspec:\n  type: NodePort\n  ports:\n  - port: 8080\n    targetPort: 80\n    protocol: TCP\n    name: http\n  - port: 443\n    protocol: TCP\n    name: https\n  selector:\n    run: my-nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      volumes:\n      - name: secret-volume\n        secret:\n          secretName: nginxsecret\n      - name: configmap-volume\n        configMap:\n          name: nginxconfigmap\n      containers:\n      - name: nginxhttps\n        image: bprashanth/nginxhttps:1.0\n        ports:\n        - containerPort: 443\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx/ssl\n          name: secret-volume\n        - mountPath: /etc/nginx/conf.d\n          name: configmap-volume\n\n```"}
{"prompt": "Write YAML that defines a Deployment resource named \"curl-deployment\" with one replica. The Deployment selects Pods labeled with \"app: curlpod\". The Pod template contains a volume named \"secret-volume\" that mounts a secret named \"nginxsecret\" and a container named \"curlpod\" that runs the command \"sh -c 'while true; do sleep 1; done'\" in an image of \"radial/busyboxplus:curl\" and mounts the \"secret-volume\" at the path \"/etc/nginx/ssl\".", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: curl-deployment\nspec:\n  selector:\n    matchLabels:\n      app: curlpod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: curlpod\n    spec:\n      volumes:\n      - name: secret-volume\n        secret:\n          secretName: nginxsecret\n      containers:\n      - name: curlpod\n        command:\n        - sh\n        - -c\n        - while true; do sleep 1; done\n        image: radial/busyboxplus:curl\n        volumeMounts:\n        - mountPath: /etc/nginx/ssl\n          name: secret-volume\n\n```"}
{"prompt": "Write YAML that specifies the API version and kind of the resource, which in this case is a Service. It also includes metadata that defines a name and labels for the Service. The spec section defines the IP family policy as PreferDualStack and includes an IPv6 IP family. The Service type is set to LoadBalancer and it is selected based on the label app.kubernetes.io/name with a value of MyApp. Finally, there is a port section which specifies that the Service should listen on port 80 with a TCP protocol.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  ipFamilyPolicy: PreferDualStack\n  ipFamilies:\n  - IPv6\n  type: LoadBalancer\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n\n```"}
{"prompt": "Write YAML that creates a NetworkPolicy object with the name \"default-deny-ingress\" in the \"metadata\" section. The \"spec\" section specifies that the policy applies to all pods, indicated by an empty \"podSelector\". The policy type is set to only allow ingress traffic.", "completion": "```yaml\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n\n```"}
{"prompt": "Write YAML that defines an Ingress resource for Kubernetes with the API version `networking.k8s.io/v1`. The Ingress is named `tls-example-ingress` and specifies a TLS secret named `testsecret-tls` for the host `https-example.foo.com`. The Ingress also includes a rule that forwards incoming HTTP traffic for the host `https-example.foo.com` to a service named `service1` on port `80` with a path of `/` and `pathType` set to `Prefix`.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tls-example-ingress\nspec:\n  tls:\n  - hosts:\n      - https-example.foo.com\n    secretName: testsecret-tls\n  rules:\n  - host: https-example.foo.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: service1\n            port:\n              number: 80\n\n```"}
{"prompt": "Write YAML that defines an Ingress resource in the Kubernetes cluster with the name \"minimal-ingress\". It uses the nginx ingress controller to rewrite the URL path to \"/\". The Ingress resource is associated with the \"nginx-example\" ingress class and has a rule that forwards requests with the \"/testpath\" prefix to a backend service named \"test\" on port 80.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: minimal-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx-example\n  rules:\n  - http:\n      paths:\n      - path: /testpath\n        pathType: Prefix\n        backend:\n          service:\n            name: test\n            port:\n              number: 80\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes NetworkPolicy resource named \"allow-all-ingress\" with a specification to allow all incoming traffic. This NetworkPolicy applies to all pods using the empty podSelector {}. The policyTypes field is set to \"Ingress\", indicating that only traffic coming into the pods is allowed.", "completion": "```yaml\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-all-ingress\nspec:\n  podSelector: {}\n  ingress:\n  - {}\n  policyTypes:\n  - Ingress\n\n```"}
{"prompt": "Write YAML that defines an Ingress resource for Kubernetes with the name \"simple-fanout-example\". It has two rules, one for the host \"foo.bar.com\" and another for the path \"/bar\". The first rule forwards requests to a service named \"service1\" on port 4200, while the second rule forwards requests to a service named \"service2\" on port 8080.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: simple-fanout-example\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - path: /foo\n        pathType: Prefix\n        backend:\n          service:\n            name: service1\n            port:\n              number: 4200\n      - path: /bar\n        pathType: Prefix\n        backend:\n          service:\n            name: service2\n            port:\n              number: 8080\n\n```"}
{"prompt": "Write YAML that defines an Ingress resource named \"ingress-resource-backend\" using the networking.k8s.io/v1 API version. The Ingress resource has a default backend that points to a resource of kind StorageBucket from the k8s.example.com API group, with the name \"static-assets\". The Ingress resource also has a rule defined for the path \"/icons\" with a path type of ImplementationSpecific, which directs traffic to another StorageBucket resource with the name \"icon-assets\" from the k8s.example.com API group.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-resource-backend\nspec:\n  defaultBackend:\n    resource:\n      apiGroup: k8s.example.com\n      kind: StorageBucket\n      name: static-assets\n  rules:\n    - http:\n        paths:\n          - path: /icons\n            pathType: ImplementationSpecific\n            backend:\n              resource:\n                apiGroup: k8s.example.com\n                kind: StorageBucket\n                name: icon-assets\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes service named \"my-service\" with the labels \"app.kubernetes.io/name: MyApp\". The service should have an IP family policy set to \"PreferDualStack\" and support both IPv6 and IPv4. The service should be selected by pods with the label \"app.kubernetes.io/name: MyApp\" and expose port 80 using the TCP protocol.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  ipFamilyPolicy: PreferDualStack\n  ipFamilies:\n  - IPv6\n  - IPv4\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n\n```"}
{"prompt": "Write YAML that creates a Kubernetes Service object named \"my-nginx\" with labels \"run: my-nginx\". This Service will expose port 80 using TCP protocol. The Service will be associated with Pods that have the label \"run: my-nginx\".", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-nginx\n  labels:\n    run: my-nginx\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n  selector:\n    run: my-nginx\n\n```"}
{"prompt": "Write YAML that creates a pod in the default namespace named \"dns-example\" with a container named \"test\" using the nginx image. The DNS policy for the pod is set to \"None\" which means it won't use the cluster DNS. The DNS configuration for the pod specifies a custom nameserver of 192.0.2.1 and a search list of \"ns1.svc.cluster-domain.example\" and \"my.dns.search.suffix\". Additionally, two DNS options are set: \"ndots\" with a value of \"2\" and \"edns0\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: dns-example\nspec:\n  containers:\n    - name: test\n      image: nginx\n  dnsPolicy: \"None\"\n  dnsConfig:\n    nameservers:\n      - 192.0.2.1 # this is an example\n    searches:\n      - ns1.svc.cluster-domain.example\n      - my.dns.search.suffix\n    options:\n      - name: ndots\n        value: \"2\"\n      - name: edns0\n\n```"}
{"prompt": "Write YAML that creates a Pod named \"hostaliases-pod\" with a restart policy of \"Never\". The Pod includes two host aliases, mapping the IP addresses 127.0.0.1 and 10.1.2.3 to the respective hostnames \"foo.local\" and \"bar.local\", and \"foo.remote\" and \"bar.remote\". The Pod also contains a container named \"cat-hosts\" that uses the busybox:1.28 image and runs the command \"cat\" with the argument \"/etc/hosts\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: hostaliases-pod\nspec:\n  restartPolicy: Never\n  hostAliases:\n  - ip: \"127.0.0.1\"\n    hostnames:\n    - \"foo.local\"\n    - \"bar.local\"\n  - ip: \"10.1.2.3\"\n    hostnames:\n    - \"foo.remote\"\n    - \"bar.remote\"\n  containers:\n  - name: cat-hosts\n    image: busybox:1.28\n    command:\n    - cat\n    args:\n    - \"/etc/hosts\"\n\n```"}
{"prompt": "Write YAML that creates an IngressClass resource with the name \"external-lb\" and specifies the API version as networking.k8s.io/v1. The resource's kind is set to IngressClass. The spec section defines the controller as \"example.com/ingress-controller\" and includes parameters specifying the API group, kind, and name as k8s.example.com, IngressParameters, and external-lb respectively.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: external-lb\nspec:\n  controller: example.com/ingress-controller\n  parameters:\n    apiGroup: k8s.example.com\n    kind: IngressParameters\n    name: external-lb\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service named \"my-service\" with the API version v1. The Service should have an IP family of IPv6 and should be selected by an app with the label \"MyApp\". It should expose port 80 using TCP protocol and forward the traffic to port 9376.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  ipFamily: IPv6\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n\n```"}
{"prompt": "Write YAML that creates an IngressClass resource with the metadata name set to \"external-lb\" and the apiVersion set to \"networking.k8s.io/v1\". The spec section should include a controller field with a value of \"example.com/ingress-controller\". Additionally, it should have a parameters field with apiGroup, kind, name, namespace, and scope fields set to \"k8s.example.com\", \"IngressParameters\", \"external-lb\", \"external-configuration\", and \"Namespace\" respectively.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: external-lb\nspec:\n  controller: example.com/ingress-controller\n  parameters:\n    apiGroup: k8s.example.com\n    kind: IngressParameters\n    name: external-lb\n    namespace: external-configuration\n    scope: Namespace\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes network policy called \"default-deny-egress\" using the \"networking.k8s.io/v1\" API version. It has no specific pod selector, meaning it applies to all pods, and it allows only egress traffic to flow out from the pods.", "completion": "```yaml\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-egress\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service called \"my-service\" with the label \"app.kubernetes.io/name: MyApp\". The Service will use IPv6 and will select pods with the label \"app.kubernetes.io/name: MyApp\". It will expose port 80 using the TCP protocol.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  ipFamilies:\n  - IPv6\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n\n```"}
{"prompt": "Write YAML that defines an Ingress resource named \"name-virtual-host-ingress\" with two rules. The first rule is for the host \"foo.bar.com\" and directs all traffic to service1 on port 80. The second rule is for the host \"bar.foo.com\" and directs all traffic to service2 on port 80.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: name-virtual-host-ingress\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: service1\n            port:\n              number: 80\n  - host: bar.foo.com\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: service2\n            port:\n              number: 80\n\n```"}
{"prompt": "Write YAML that creates a NetworkPolicy object named \"test-network-policy\" in the default namespace. This policy will allow ingress traffic from IP range 172.17.0.0/16, excluding 172.17.1.0/24, from all pods in a namespace labeled \"project: myproject\" and from pods labeled \"role: frontend\", on TCP port 6379. It also allows egress traffic to IP range 10.0.0.0/24 on TCP port 5978.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: test-network-policy\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    - from:\n        - ipBlock:\n            cidr: 172.17.0.0/16\n            except:\n              - 172.17.1.0/24\n        - namespaceSelector:\n            matchLabels:\n              project: myproject\n        - podSelector:\n            matchLabels:\n              role: frontend\n      ports:\n        - protocol: TCP\n          port: 6379\n  egress:\n    - to:\n        - ipBlock:\n            cidr: 10.0.0.0/24\n      ports:\n        - protocol: TCP\n          port: 5978\n\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes NetworkPolicy called \"multi-port-egress\" in the \"default\" namespace. This NetworkPolicy is applied to pods with the label \"role: db\". It allows egress traffic to the IP range 10.0.0.0/24 with TCP ports ranging from 32000 to 32768.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: multi-port-egress\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n    - Egress\n  egress:\n    - to:\n        - ipBlock:\n            cidr: 10.0.0.0/24\n      ports:\n        - protocol: TCP\n          port: 32000\n          endPort: 32768\n\n\n```"}
{"prompt": "Write YAML that defines an Ingress resource with two rules. The first rule is set to host \"foo.bar.com\" and redirects requests with path \"/bar\" to \"service1\" on port 80. The second rule uses a wildcard host \"*.foo.com\" and redirects requests with path \"/foo\" to \"service2\" on port 80.", "completion": "```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-wildcard-host\nspec:\n  rules:\n  - host: \"foo.bar.com\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/bar\"\n        backend:\n          service:\n            name: service1\n            port:\n              number: 80\n  - host: \"*.foo.com\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/foo\"\n        backend:\n          service:\n            name: service2\n            port:\n              number: 80\n\n```"}
{"prompt": "Write YAML that defines a NetworkPolicy resource in Kubernetes API version networking.k8s.io/v1. The name of this NetworkPolicy is \"allow-all-egress\". The spec section specifies that all pods are selected to apply this NetworkPolicy and allows all egress traffic.", "completion": "```yaml\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-all-egress\nspec:\n  podSelector: {}\n  egress:\n  - {}\n  policyTypes:\n  - Egress\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Service called \"my-service\" for an application named \"MyApp\". This Service will listen on port 80 using the TCP protocol and will select pods with the label \"app.kubernetes.io/name: MyApp\" to forward traffic to.", "completion": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n\n```"}
{"prompt": "Write YAML that declares a NetworkPolicy resource called \"default-deny-all\" with the specified API version and kind. The NetworkPolicy resource is configured to apply a default deny-all rule, meaning it will block all incoming and outgoing traffic to and from pods.", "completion": "```yaml\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n\n```"}
{"prompt": "Write YAML that defines a FlowSchema named \"health-for-strangers\" with a matching precedence of 1000. The FlowSchema has a priority level configuration named \"exempt\". The rules allow all verbs on the \"/healthz\", \"/livez\", and \"/readyz\" nonResourceURLs for the \"system:unauthenticated\" group.", "completion": "```yaml\napiVersion: flowcontrol.apiserver.k8s.io/v1beta3\nkind: FlowSchema\nmetadata:\n  name: health-for-strangers\nspec:\n  matchingPrecedence: 1000\n  priorityLevelConfiguration:\n    name: exempt\n  rules:\n    - nonResourceRules:\n      - nonResourceURLs:\n          - \"/healthz\"\n          - \"/livez\"\n          - \"/readyz\"\n        verbs:\n          - \"*\"\n      subjects:\n        - kind: Group\n          group:\n            name: \"system:unauthenticated\"\n\n```"}
{"prompt": "Write YAML that creates a Kubernetes Secret called \"mysecretname\" for a service account token. The Secret has an annotation that specifies the name of the service account as \"myserviceaccount\".", "completion": "```yaml\napiVersion: v1\nkind: Secret\ntype: kubernetes.io/service-account-token\nmetadata:\n  name: mysecretname\n  annotations:\n    kubernetes.io/service-account.name: myserviceaccount\n\n```"}
{"prompt": "Write YAML that defines a PodDisruptionBudget with the name \"zk-pdb\". The PodDisruptionBudget ensures that at least 2 replicas of the specified app (zookeeper) are available at all times. This YAML can be used to create or update a PodDisruptionBudget resource in a Kubernetes cluster.", "completion": "```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: zk-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: zookeeper\n\n```"}
{"prompt": "Write YAML that creates a Pod Security Policy named \"baseline\". The policy allows the default AppArmor profile and all seccomp profiles, while disabling privileged access and setting a limited set of allowed capabilities. It also allows various volume types except for hostpath, and disables host network, IPC, and PID namespaces. Finally, it sets the runAsUser, seLinux, supplementalGroups, and fsGroup rules to \"RunAsAny\".", "completion": "```yaml\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: baseline\n  annotations:\n    # Optional: Allow the default AppArmor profile, requires setting the default.\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'\n    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\nspec:\n  privileged: false\n  # The moby default capability set, minus NET_RAW\n  allowedCapabilities:\n    - 'CHOWN'\n    - 'DAC_OVERRIDE'\n    - 'FSETID'\n    - 'FOWNER'\n    - 'MKNOD'\n    - 'SETGID'\n    - 'SETUID'\n    - 'SETFCAP'\n    - 'SETPCAP'\n    - 'NET_BIND_SERVICE'\n    - 'SYS_CHROOT'\n    - 'KILL'\n    - 'AUDIT_WRITE'\n  # Allow all volume types except hostpath\n  volumes:\n    # 'core' volume types\n    - 'configMap'\n    - 'emptyDir'\n    - 'projected'\n    - 'secret'\n    - 'downwardAPI'\n    # Assume that ephemeral CSI drivers & persistentVolumes set up by the cluster admin are safe to use.\n    - 'csi'\n    - 'persistentVolumeClaim'\n    - 'ephemeral'\n    # Allow all other non-hostpath volume types.\n    - 'awsElasticBlockStore'\n    - 'azureDisk'\n    - 'azureFile'\n    - 'cephFS'\n    - 'cinder'\n    - 'fc'\n    - 'flexVolume'\n    - 'flocker'\n    - 'gcePersistentDisk'\n    - 'gitRepo'\n    - 'glusterfs'\n    - 'iscsi'\n    - 'nfs'\n    - 'photonPersistentDisk'\n    - 'portworxVolume'\n    - 'quobyte'\n    - 'rbd'\n    - 'scaleIO'\n    - 'storageos'\n    - 'vsphereVolume'\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  readOnlyRootFilesystem: false\n  runAsUser:\n    rule: 'RunAsAny'\n  seLinux:\n    # This policy assumes the nodes are using AppArmor rather than SELinux.\n    # The PSP SELinux API cannot express the SELinux Pod Security Standards,\n    # so if using SELinux, you must choose a more restrictive default.\n    rule: 'RunAsAny'\n  supplementalGroups:\n    rule: 'RunAsAny'\n  fsGroup:\n    rule: 'RunAsAny'\n\n```"}
{"prompt": "Write YAML that creates a PodSecurityPolicy named \"privileged\" with the following settings: allow any Seccomp profile, allow privileged containers with privilege escalation, allow all capabilities, allow all volumes, use host network, allow all host ports, use host IPC and host PID namespaces, allow running as any user, allow using any SELinux context, allow any supplemental group, and allow any fsGroup.", "completion": "```yaml\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: privileged\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\nspec:\n  privileged: true\n  allowPrivilegeEscalation: true\n  allowedCapabilities:\n  - '*'\n  volumes:\n  - '*'\n  hostNetwork: true\n  hostPorts:\n  - min: 0\n    max: 65535\n  hostIPC: true\n  hostPID: true\n  runAsUser:\n    rule: 'RunAsAny'\n  seLinux:\n    rule: 'RunAsAny'\n  supplementalGroups:\n    rule: 'RunAsAny'\n  fsGroup:\n    rule: 'RunAsAny'\n\n```"}
{"prompt": "Write YAML that defines a PodSecurityPolicy resource called \"example\". This resource sets the privileged field to false, indicating that privileged pods are not allowed. It also sets rules for seLinux, supplementalGroups, runAsUser, and fsGroup, all of which are set to \"RunAsAny\". Finally, the volumes field is set to allow any volume.", "completion": "```yaml\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: example\nspec:\n  privileged: false  # Don't allow privileged pods!\n  # The rest fills in some required fields.\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  runAsUser:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n  volumes:\n  - '*'\n\n```"}
{"prompt": "Write YAML that defines a PodSecurityPolicy named \"restricted\" with the following specifications: seccomp and apparmor profiles allowed are \"docker/default\", \"runtime/default\", and \"runtime/default\" respectively, privileged mode is set to false, allowPrivilegeEscalation is set to false, requiredDropCapabilities is set to \"ALL\", allowed volumes include \"configMap\", \"emptyDir\", \"projected\", \"secret\", \"downwardAPI\", \"csi\", \"persistentVolumeClaim\", and \"ephemeral\", hostNetwork, hostIPC, and hostPID are all set to false, runAsUser rule is set to \"MustRunAsNonRoot\", seLinux rule is set to \"RunAsAny\", supplementalGroups are required to run and the range is set to forbid adding the root group, fsGroup is required to run and the range is set to forbid adding the root group, and readOnlyRootFilesystem is set to false.", "completion": "```yaml\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: restricted\n  annotations:\n    # docker/default identifies a profile for seccomp, but it is not particularly tied to the Docker runtime\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default'\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'\n    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'\nspec:\n  privileged: false\n  # Required to prevent escalations to root.\n  allowPrivilegeEscalation: false\n  requiredDropCapabilities:\n    - ALL\n  # Allow core volume types.\n  volumes:\n    - 'configMap'\n    - 'emptyDir'\n    - 'projected'\n    - 'secret'\n    - 'downwardAPI'\n    # Assume that ephemeral CSI drivers & persistentVolumes set up by the cluster admin are safe to use.\n    - 'csi'\n    - 'persistentVolumeClaim'\n    - 'ephemeral'\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  runAsUser:\n    # Require the container to run without root privileges.\n    rule: 'MustRunAsNonRoot'\n  seLinux:\n    # This policy assumes the nodes are using AppArmor rather than SELinux.\n    rule: 'RunAsAny'\n  supplementalGroups:\n    rule: 'MustRunAs'\n    ranges:\n      # Forbid adding the root group.\n      - min: 1\n        max: 65535\n  fsGroup:\n    rule: 'MustRunAs'\n    ranges:\n      # Forbid adding the root group.\n      - min: 1\n        max: 65535\n  readOnlyRootFilesystem: false\n\n```"}
{"prompt": "Write YAML that declares a PodDisruptionBudget resource named \"zk-pdb\" with the policy/v1 API version. The spec section specifies that only one replica of the PodDisruptionBudget's associated Pods can be unavailable at a time. The selector section matches Pods with the label \"app: zookeeper\".", "completion": "```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: zk-pdb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: zookeeper\n\n```"}
{"prompt": "Write YAML that specifies a ResourceQuota object of kind \"ResourceQuota\" with the name \"pods-cluster-services\". The scopeSelector field is used to specify the scope of the quota. Here, it is set to match the PriorityClass scope with the operator being \"In\" and the value being \"cluster-services\".", "completion": "```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: pods-cluster-services\nspec:\n  scopeSelector:\n    matchExpressions:\n      - operator : In\n        scopeName: PriorityClass\n        values: [\"cluster-services\"]\n```"}
{"prompt": "Write YAML that creates a Kubernetes Namespace called \"my-privileged-namespace\" with the labels \"pod-security.kubernetes.io/enforce\" set to \"privileged\" and \"pod-security.kubernetes.io/enforce-version\" set to \"latest\". This YAML also specifies that the API version used is \"v1\" and the resource kind is \"Namespace\". \n\nThe YAML can be used with the `kubectl apply -f` command to create the Namespace in a Kubernetes cluster.", "completion": "```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-privileged-namespace\n  labels:\n    pod-security.kubernetes.io/enforce: privileged\n    pod-security.kubernetes.io/enforce-version: latest\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"nginx\" running the nginx image. The Pod has a single container named \"nginx\" and it exposes port 80.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n    - image: nginx\n      name: nginx\n      ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Namespace called \"my-restricted-namespace\" with labels to enforce pod security restrictions. The labels \"pod-security.kubernetes.io/enforce\" and \"pod-security.kubernetes.io/enforce-version\" specify that the namespace should enforce restricted security policies using the latest version. The labels \"pod-security.kubernetes.io/warn\" and \"pod-security.kubernetes.io/warn-version\" set a warning for restricted security policies using the latest version.", "completion": "```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-restricted-namespace\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/enforce-version: latest\n    pod-security.kubernetes.io/warn: restricted\n    pod-security.kubernetes.io/warn-version: latest\n```"}
{"prompt": "Write YAML that defines a Kubernetes Namespace resource named \"my-baseline-namespace\" and sets specific labels for pod security enforcement and warning. The labels specify that baseline security policies should be enforced and the latest versions of those policies should be used for both enforcement and warning.", "completion": "```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-baseline-namespace\n  labels:\n    pod-security.kubernetes.io/enforce: baseline\n    pod-security.kubernetes.io/enforce-version: latest\n    pod-security.kubernetes.io/warn: baseline\n    pod-security.kubernetes.io/warn-version: latest\n```"}
{"prompt": "Write YAML that defines two ConfigMap resources. The first ConfigMap is named \"special-config\" and has a data field with a key-value pair of \"special.how: very\". The second ConfigMap is named \"env-config\" and has a data field with a key-value pair of \"log_level: INFO\". Both ConfigMaps are in the \"default\" namespace.", "completion": "```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: env-config\n  namespace: default\ndata:\n  log_level: INFO\n\n```"}
{"prompt": "Write YAML that defines a ConfigMap named \"special-config\" in the \"default\" namespace with two key-value pairs: \"SPECIAL_LEVEL\" with a value of \"very\" and \"SPECIAL_TYPE\" with a value of \"charm\". This YAML represents a Kubernetes resource object used to store configuration data that can be accessed by other pods or containers in the cluster.\n", "completion": "```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  SPECIAL_LEVEL: very\n  SPECIAL_TYPE: charm\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"configmap-demo-pod\" with a single container named \"demo\", using the Alpine image. The container has a command to sleep for 3600 seconds. The container also has two environment variables, \"PLAYER_INITIAL_LIVES\" and \"UI_PROPERTIES_FILE_NAME\", which are both sourced from a ConfigMap named \"game-demo\". The Pod also has a volume named \"config\" which is mounted to the \"/config\" path in the container, and this volume is populated with two files from the ConfigMap, \"game.properties\" and \"user-interface.properties\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-demo-pod\nspec:\n  containers:\n    - name: demo\n      image: alpine\n      command: [\"sleep\", \"3600\"]\n      env:\n        # Define the environment variable\n        - name: PLAYER_INITIAL_LIVES # Notice that the case is different here\n                                     # from the key name in the ConfigMap.\n          valueFrom:\n            configMapKeyRef:\n              name: game-demo           # The ConfigMap this value comes from.\n              key: player_initial_lives # The key to fetch.\n        - name: UI_PROPERTIES_FILE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: game-demo\n              key: ui_properties_file_name\n      volumeMounts:\n      - name: config\n        mountPath: \"/config\"\n        readOnly: true\n  volumes:\n  # You set volumes at the Pod level, then mount them into containers inside that Pod\n  - name: config\n    configMap:\n      # Provide the name of the ConfigMap you want to mount.\n      name: game-demo\n      # An array of keys from the ConfigMap to create as files\n      items:\n      - key: \"game.properties\"\n        path: \"game.properties\"\n      - key: \"user-interface.properties\"\n        path: \"user-interface.properties\"\n        \n```"}
{"prompt": "Write YAML that defines an audit policy for Kubernetes. This policy specifies which events to log and at what level. It includes rules to log pod changes at the RequestResponse level, configmap changes in the kube-system namespace at the Request level, and all other resources at the Metadata level. It also specifies exceptions to logging, such as excluding requests to a configmap called \"controller-leader\" and watch requests by the \"system:kube-proxy\" on endpoints or services.", "completion": "```yaml\napiVersion: audit.k8s.io/v1 # This is required.\nkind: Policy\n# Don't generate audit events for all requests in RequestReceived stage.\nomitStages:\n  - \"RequestReceived\"\nrules:\n  # Log pod changes at RequestResponse level\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      # Resource \"pods\" doesn't match requests to any subresource of pods,\n      # which is consistent with the RBAC policy.\n      resources: [\"pods\"]\n  # Log \"pods/log\", \"pods/status\" at Metadata level\n  - level: Metadata\n    resources:\n    - group: \"\"\n      resources: [\"pods/log\", \"pods/status\"]\n\n  # Don't log requests to a configmap called \"controller-leader\"\n  - level: None\n    resources:\n    - group: \"\"\n      resources: [\"configmaps\"]\n      resourceNames: [\"controller-leader\"]\n\n  # Don't log watch requests by the \"system:kube-proxy\" on endpoints or services\n  - level: None\n    users: [\"system:kube-proxy\"]\n    verbs: [\"watch\"]\n    resources:\n    - group: \"\" # core API group\n      resources: [\"endpoints\", \"services\"]\n\n  # Don't log authenticated requests to certain non-resource URL paths.\n  - level: None\n    userGroups: [\"system:authenticated\"]\n    nonResourceURLs:\n    - \"/api*\" # Wildcard matching.\n    - \"/version\"\n\n  # Log the request body of configmap changes in kube-system.\n  - level: Request\n    resources:\n    - group: \"\" # core API group\n      resources: [\"configmaps\"]\n    # This rule only applies to resources in the \"kube-system\" namespace.\n    # The empty string \"\" can be used to select non-namespaced resources.\n    namespaces: [\"kube-system\"]\n\n  # Log configmap and secret changes in all other namespaces at the Metadata level.\n  - level: Metadata\n    resources:\n    - group: \"\" # core API group\n      resources: [\"secrets\", \"configmaps\"]\n\n  # Log all other resources in core and extensions at the Request level.\n  - level: Request\n    resources:\n    - group: \"\" # core API group\n    - group: \"extensions\" # Version of group should NOT be included.\n\n  # A catch-all rule to log all other requests at the Metadata level.\n  - level: Metadata\n    # Long-running requests like watches that fall under this rule will not\n    # generate an audit event in RequestReceived.\n    omitStages:\n      - \"RequestReceived\"\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Deployment named \"snowflake\" with 2 replicas. The Deployment will use an image from the \"registry.k8s.io/serve_hostname\" repository and always pull the latest image. The Deployment will match pods with the label \"app=snowflake\" and apply the \"snowflake\" label to both the deployment and the pods.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: snowflake\n  name: snowflake\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: snowflake\n  template:\n    metadata:\n      labels:\n        app: snowflake\n    spec:\n      containers:\n      - image: registry.k8s.io/serve_hostname\n        imagePullPolicy: Always\n        name: snowflake\n\n```"}
{"prompt": "Write YAML that represents a Kubernetes namespace called \"production\". It specifies the API version v1 and the kind \"Namespace\". The metadata section sets the name of the namespace to \"production\" and assigns a label with the name \"production\" to it.", "completion": "```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  labels:\n    name: production\n\n```"}
{"prompt": "Write YAML that defines a Namespace resource in Kubernetes with the apiVersion set to v1 and the kind set to Namespace. The metadata section provides additional information about the resource, including the name which is set to \"development\" and the labels which also have the name set to \"development\".", "completion": "```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: development\n  labels:\n    name: development\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"counter\". This Pod has three containers: count, count-log-1, and count-log-2. The \"count\" container uses the busybox:1.28 image and runs a shell script that continuously increments a counter and logs the counter value along with the current date and time to /var/log/1.log and /var/log/2.log. The \"count-log-1\" and \"count-log-2\" containers use the busybox:1.28 image and continuously tail the logs from /var/log/1.log and /var/log/2.log respectively. There is also a volumeMount named \"varlog\" that mounts the /var/log directory inside all the containers, and a volume named \"varlog\" that is an emptyDir volume.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox:1.28\n    args:\n    - /bin/sh\n    - -c\n    - >\n      i=0;\n      while true;\n      do\n        echo \"$i: $(date)\" >> /var/log/1.log;\n        echo \"$(date) INFO $i\" >> /var/log/2.log;\n        i=$((i+1));\n        sleep 1;\n      done\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-log-1\n    image: busybox:1.28\n    args: [/bin/sh, -c, 'tail -n+1 -F /var/log/1.log']\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-log-2\n    image: busybox:1.28\n    args: [/bin/sh, -c, 'tail -n+1 -F /var/log/2.log']\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  volumes:\n  - name: varlog\n    emptyDir: {}\n\n```"}
{"prompt": "Write YAML that creates a ConfigMap object named \"fluentd-config\" with the data field \"fluentd.conf\". The \"fluentd.conf\" file contains two tail sources, each with a specified path and tag. The ConfigMap also has a match block that specifies the type as \"google_cloud\".", "completion": "```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentd-config\ndata:\n  fluentd.conf: |\n    <source>\n      type tail\n      format none\n      path /var/log/1.log\n      pos_file /var/log/1.log.pos\n      tag count.format1\n    </source>\n\n    <source>\n      type tail\n      format none\n      path /var/log/2.log\n      pos_file /var/log/2.log.pos\n      tag count.format2\n    </source>\n\n    <match **>\n      type google_cloud\n    </match>\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named \"counter\" which runs a container with the image \"busybox:1.28\". The container executes a series of shell commands to increment a counter and log the counter value and the current date to two separate log files every second. The container also has a volume mount that maps the \"/var/log\" directory to a volume named \"varlog\". The volume is defined as an empty directory.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox:1.28\n    args:\n    - /bin/sh\n    - -c\n    - >\n      i=0;\n      while true;\n      do\n        echo \"$i: $(date)\" >> /var/log/1.log;\n        echo \"$(date) INFO $i\" >> /var/log/2.log;\n        i=$((i+1));\n        sleep 1;\n      done\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  volumes:\n  - name: varlog\n    emptyDir: {}\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"counter\". The Pod consists of two containers, one named \"count\" and the other named \"count-agent\". The \"count\" container runs a shell script that continuously logs the current date and informational messages to two separate log files every second. The \"count-agent\" container runs a Fluentd agent that reads logs from the same logs directory and forwards them to a Fluentd server. The Pod also defines two volumes, one called \"varlog\" for sharing logs between the containers, and another called \"config-volume\" which is a ConfigMap named \"fluentd-config\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox:1.28\n    args:\n    - /bin/sh\n    - -c\n    - >\n      i=0;\n      while true;\n      do\n        echo \"$i: $(date)\" >> /var/log/1.log;\n        echo \"$(date) INFO $i\" >> /var/log/2.log;\n        i=$((i+1));\n        sleep 1;\n      done\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: registry.k8s.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n\n```"}
{"prompt": "Write YAML that sets up cloud-controller-manager as a Daemonset in the cluster. It creates a ServiceAccount for the cloud-controller-manager in the kube-system namespace and binds it to the cluster-admin ClusterRole. It also creates a DaemonSet with a container running the cloud-controller-manager image, configured with specific flags for the cloud provider and other settings.", "completion": "```yaml\n# This is an example of how to set up cloud-controller-manager as a Daemonset in your cluster.\n# It assumes that your masters can run pods and has the role node-role.kubernetes.io/master\n# Note that this Daemonset will not work straight out of the box for your cloud, this is\n# meant to be a guideline.\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: cloud-controller-manager\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:cloud-controller-manager\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: cloud-controller-manager\n  namespace: kube-system\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cloud-controller-manager\n  name: cloud-controller-manager\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cloud-controller-manager\n  template:\n    metadata:\n      labels:\n        k8s-app: cloud-controller-manager\n    spec:\n      serviceAccountName: cloud-controller-manager\n      containers:\n      - name: cloud-controller-manager\n        # for in-tree providers we use registry.k8s.io/cloud-controller-manager\n        # this can be replaced with any other image for out-of-tree providers\n        image: registry.k8s.io/cloud-controller-manager:v1.8.0\n        command:\n        - /usr/local/bin/cloud-controller-manager\n        - --cloud-provider=[YOUR_CLOUD_PROVIDER]  # Add your own cloud provider here!\n        - --leader-elect=true\n        - --use-service-account-credentials\n        # these flags will vary for every cloud provider\n        - --allocate-node-cidrs=true\n        - --configure-cloud-routes=true\n        - --cluster-cidr=172.17.0.0/16\n      tolerations:\n      # this is required so CCM can bootstrap itself\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        value: \"true\"\n        effect: NoSchedule\n      # these tolerations are to have the daemonset runnable on control plane nodes\n      # remove them if your control plane nodes should not run pods\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n        effect: NoSchedule\n      # this is to restrict CCM to only run on master nodes\n      # the node selector may vary depending on your cluster setup\n      nodeSelector:\n        node-role.kubernetes.io/master: \"\"\n\n```"}
{"prompt": "Write YAML that declares a ResourceQuota object with the name \"object-quota-demo\", specifying the resource limitations. The specified hard limitations for this resource quota are 1 persistent volume claim, 2 load balancer services, and 0 node ports. This YAML can be used to create or update the ResourceQuota object in a Kubernetes cluster.", "completion": "```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: object-quota-demo\nspec:\n  hard:\n    persistentvolumeclaims: \"1\"\n    services.loadbalancers: \"2\"\n    services.nodeports: \"0\"\n\n```"}
{"prompt": "Write YAML that defines a ResourceQuota object with the name \"mem-cpu-demo\". The ResourceQuota object has hard limits specified for CPU and memory usage, where the CPU requests are set to 1 core and memory requests are set to 1 GiB. The CPU limits are set to 2 cores and memory limits are set to 2 GiB.", "completion": "```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: mem-cpu-demo\nspec:\n  hard:\n    requests.cpu: \"1\"\n    requests.memory: 1Gi\n    limits.cpu: \"2\"\n    limits.memory: 2Gi\n\n```"}
{"prompt": "Write YAML that describes a Pod object with the name \"default-mem-demo-3\" running the nginx image. The Pod has one container named \"default-mem-demo-3-ctr\" and it requests a minimum memory allocation of 128Mi.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: default-mem-demo-3\nspec:\n  containers:\n  - name: default-mem-demo-3-ctr\n    image: nginx\n    resources:\n      requests:\n        memory: \"128Mi\"\n\n```"}
{"prompt": "Write YAML that defines a LimitRange resource in Kubernetes. This resource is named \"mem-min-max-demo-lr\" and it specifies limits for the memory resource of containers. The maximum memory allowed is 1Gi and the minimum memory allowed is 500Mi.", "completion": "```yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-min-max-demo-lr\nspec:\n  limits:\n  - max:\n      memory: 1Gi\n    min:\n      memory: 500Mi\n    type: Container\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod with the name \"constraints-cpu-demo-2\". The Pod will run a container named \"constraints-cpu-demo-2-ctr\" using the nginx image. The container is configured with CPU resource limits of 1.5 cores and a CPU request of 500 milli-cores.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: constraints-cpu-demo-2\nspec:\n  containers:\n  - name: constraints-cpu-demo-2-ctr\n    image: nginx\n    resources:\n      limits:\n        cpu: \"1.5\"\n      requests:\n        cpu: \"500m\"\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"constraints-cpu-demo-4\" using the v1 API version and specifies that the object is of kind Pod. The Pod should have one container named \"constraints-cpu-demo-4-ctr\" that uses the \"vish/stress\" image.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: constraints-cpu-demo-4\nspec:\n  containers:\n  - name: constraints-cpu-demo-4-ctr\n    image: vish/stress\n\n```"}
{"prompt": "Write YAML that defines a LimitRange object with the name \"mem-limit-range\". Inside the object, specify two limits: default and defaultRequest. Set the default memory limit to 512Mi and the default memory request to 256Mi. Also, specify that these limits should apply to containers.", "completion": "```yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-limit-range\nspec:\n  limits:\n  - default:\n      memory: 512Mi\n    defaultRequest:\n      memory: 256Mi\n    type: Container\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"constraints-mem-demo-2\" with a container named \"constraints-mem-demo-2-ctr\" running an nginx image. The container has memory limits set to 1.5 gigabytes (\"1.5Gi\") and memory requests set to 800 megabytes (\"800Mi\").", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: constraints-mem-demo-2\nspec:\n  containers:\n  - name: constraints-mem-demo-2-ctr\n    image: nginx\n    resources:\n      limits:\n        memory: \"1.5Gi\"\n      requests:\n        memory: \"800Mi\"\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named `default-cpu-demo-2` with a container named `default-cpu-demo-2-ctr` running the `nginx` image. The Pod has a CPU resource limit set to 1 unit.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: default-cpu-demo-2\nspec:\n  containers:\n  - name: default-cpu-demo-2-ctr\n    image: nginx\n    resources:\n      limits:\n        cpu: \"1\"\n\n```"}
{"prompt": "Write YAML that defines a LimitRange object with the apiVersion set to v1 and kind set to LimitRange. The name of the object is \"limit-memory-ratio-pod\", and it specifies a limit of 2 for the maxLimitRequestRatio of the memory type for Pods. This YAML configuration is used to set resource limits for memory usage in a Kubernetes cluster.", "completion": "```yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: limit-memory-ratio-pod\nspec:\n  limits:\n  - maxLimitRequestRatio:\n      memory: 2\n    type: Pod\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"default-mem-demo-2\" using the v1 API version. The Pod contains a single container named \"default-mem-demo-2-ctr\" that uses the nginx image. The container has a memory limit set to 1GB.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: default-mem-demo-2\nspec:\n  containers:\n  - name: default-mem-demo-2-ctr\n    image: nginx\n    resources:\n      limits:\n        memory: \"1Gi\"\n\n```"}
{"prompt": "Write YAML that declares a LimitRange resource in Kubernetes API version v1. This resource is of kind LimitRange and has metadata with the name \"limit-mem-cpu-per-pod\". The LimitRange specifies a limit of maximum 2 CPUs and 2GB of memory for each Pod.", "completion": "```yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: limit-mem-cpu-per-pod\nspec:\n  limits:\n  - max:\n      cpu: \"2\"\n      memory: \"2Gi\"\n    type: Pod\n\n```"}
{"prompt": "Write YAML that creates a LimitRange named \"cpu-min-max-demo-lr\" of kind LimitRange. The LimitRange sets the maximum CPU limit to 800 milliCPU and the minimum CPU limit to 200 milliCPU for containers.", "completion": "```yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-min-max-demo-lr\nspec:\n  limits:\n  - max:\n      cpu: \"800m\"\n    min:\n      cpu: \"200m\"\n    type: Container\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"quota-mem-cpu-demo-2\" with one container named \"quota-mem-cpu-demo-2-ctr\" using the Redis image. This Pod has resource limits of 1GB of memory and 800 milliCPU, and resource requests of 700MB of memory and 400 milliCPU.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: quota-mem-cpu-demo-2\nspec:\n  containers:\n  - name: quota-mem-cpu-demo-2-ctr\n    image: redis\n    resources:\n      limits:\n        memory: \"1Gi\"\n        cpu: \"800m\"\n      requests:\n        memory: \"700Mi\"\n        cpu: \"400m\"\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Pod named busybox2. It has four containers named busybox-cnt01, busybox-cnt02, busybox-cnt03, and busybox-cnt04. Each container is based on the busybox:1.28 image and runs a command to continuously print a message every 10 seconds. \n\nThe first container (busybox-cnt01) has both resource requests and limits defined for memory and CPU. The requests are set to 100Mi memory and 100m CPU, while the limits are set to 200Mi memory and 500m CPU. \n\nThe second container (busybox-cnt02) has only resource requests defined with the same values as busybox-cnt01. \n\nThe third container (busybox-cnt03) has only resource limits defined, matching the values of busybox-cnt01. \n\nThe fourth container (busybox-cnt04) has no resource requests or limits specified.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox2\nspec:\n  containers:\n  - name: busybox-cnt01\n    image: busybox:1.28\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello from cnt01; sleep 10;done\"]\n    resources:\n      requests:\n        memory: \"100Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"200Mi\"\n        cpu: \"500m\"\n  - name: busybox-cnt02\n    image: busybox:1.28\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello from cnt02; sleep 10;done\"]\n    resources:\n      requests:\n        memory: \"100Mi\"\n        cpu: \"100m\"\n  - name: busybox-cnt03\n    image: busybox:1.28\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello from cnt03; sleep 10;done\"]\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"500m\"\n  - name: busybox-cnt04\n    image: busybox:1.28\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello from cnt04; sleep 10;done\"]\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod resource with the name \"default-cpu-demo\". The Pod specification includes a single container named \"default-cpu-demo-ctr\" running the nginx image.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: default-cpu-demo\nspec:\n  containers:\n  - name: default-cpu-demo-ctr\n    image: nginx\n\n```"}
{"prompt": "Write YAML that defines a Pod called \"busybox1\" with four containers. Each container is named \"busybox-cnt01\", \"busybox-cnt02\", \"busybox-cnt03\", and \"busybox-cnt04\", respectively. The containers use the \"busybox:1.28\" image and run a command to continuously output messages every 10 seconds. The first container has resource limits of 100Mi memory and 100m CPU, while the second container has resource requests of the same values. The third container has resource limits of 200Mi memory and 500m CPU. The fourth container has no specified resources.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox1\nspec:\n  containers:\n  - name: busybox-cnt01\n    image: busybox:1.28\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello from cnt01; sleep 10;done\"]\n    resources:\n      requests:\n        memory: \"100Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"200Mi\"\n        cpu: \"500m\"\n  - name: busybox-cnt02\n    image: busybox:1.28\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello from cnt02; sleep 10;done\"]\n    resources:\n      requests:\n        memory: \"100Mi\"\n        cpu: \"100m\"\n  - name: busybox-cnt03\n    image: busybox:1.28\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello from cnt03; sleep 10;done\"]\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"500m\"\n  - name: busybox-cnt04\n    image: busybox:1.28\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello from cnt04; sleep 10;done\"]\n\n```"}
{"prompt": "Write YAML that defines a LimitRange resource with the specified name and specifications. The LimitRange sets limits and defaults for CPU and memory resources for containers. The max, min, default, and defaultRequest fields specify the values for the CPU and memory resources and the type field indicates that these limits and defaults apply to containers.", "completion": "```yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: limit-mem-cpu-per-container\nspec:\n  limits:\n  - max:\n      cpu: \"800m\"\n      memory: \"1Gi\"\n    min:\n      cpu: \"100m\"\n      memory: \"99Mi\"\n    default:\n      cpu: \"700m\"\n      memory: \"900Mi\"\n    defaultRequest:\n      cpu: \"110m\"\n      memory: \"111Mi\"\n    type: Container\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"constraints-cpu-demo\" with a single container named \"constraints-cpu-demo-ctr\" using the nginx image. The Pod has CPU resource constraints specified, with a limit of 800 milliCPU and a request of 500 milliCPU. This YAML can be used to create the Pod with the specified CPU constraints in a Kubernetes cluster.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: constraints-cpu-demo\nspec:\n  containers:\n  - name: constraints-cpu-demo-ctr\n    image: nginx\n    resources:\n      limits:\n        cpu: \"800m\"\n      requests:\n        cpu: \"500m\"\n\n```"}
{"prompt": "Write YAML that deploys a Pod named \"constraints-mem-demo-3\" with a single container named \"constraints-mem-demo-3-ctr\". The container uses the nginx image and has memory resource limits set to 800Mi and memory resource requests set to 100Mi.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: constraints-mem-demo-3\nspec:\n  containers:\n  - name: constraints-mem-demo-3-ctr\n    image: nginx\n    resources:\n      limits:\n        memory: \"800Mi\"\n      requests:\n        memory: \"100Mi\"\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"default-cpu-demo-3\" with one container named \"default-cpu-demo-3-ctr\" using the Nginx image. The Pod specifies a resource request for CPU of \"0.75\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: default-cpu-demo-3\nspec:\n  containers:\n  - name: default-cpu-demo-3-ctr\n    image: nginx\n    resources:\n      requests:\n        cpu: \"0.75\"\n\n```"}
{"prompt": "Write YAML that defines a LimitRange object with the name \"storagelimits\". This LimitRange specifies limits for PersistentVolumeClaims, with a maximum storage of 2Gi and a minimum storage of 1Gi.", "completion": "```yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: storagelimits\nspec:\n  limits:\n  - type: PersistentVolumeClaim\n    max:\n      storage: 2Gi\n    min:\n      storage: 1Gi\n\n```"}
{"prompt": "Write YAML that defines a LimitRange resource named \"cpu-limit-range\" with a specification that contains limits for containers. The default limit for CPU is set to 1, and the default request for CPU is set to 0.5. This YAML can be used to create this resource in a Kubernetes cluster.", "completion": "```yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-limit-range\nspec:\n  limits:\n  - default:\n      cpu: 1\n    defaultRequest:\n      cpu: 0.5\n    type: Container\n\n```"}
{"prompt": "Write YAML that creates a PersistentVolumeClaim with the name \"pvc-limit-greater\". It has an access mode of \"ReadWriteOnce\" and requests a storage of 5 gigabytes. This YAML can be used to configure and provision a persistent volume claim in a Kubernetes cluster.", "completion": "```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-limit-greater\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n\n```"}
{"prompt": "Write YAML that creates a deployment named \"pod-quota-demo\" with 3 replicas. The deployment selects pods with the label \"purpose=quota-demo\" and sets up a template for creating pods with the same label. The template specifies a container named \"pod-quota-demo\" using the nginx image.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-quota-demo\nspec:\n  selector:\n    matchLabels:\n      purpose: quota-demo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        purpose: quota-demo\n    spec:\n      containers:\n      - name: pod-quota-demo\n        image: nginx\n\n```"}
{"prompt": "Write YAML that creates a PersistentVolumeClaim named \"pvc-quota-demo\" with a storage class named \"manual\" and allows for read-write access. The claim requests a storage size of 3 gigabytes. This YAML can be used to define the desired configuration for creating a persistent volume claim in a Kubernetes cluster.", "completion": "```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-quota-demo\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"busybox3\" with a single container named \"busybox-cnt01\". The container will use the \"busybox:1.28\" image and have resource limits of 300Mi of memory and resource requests of 100Mi of memory.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox3\nspec:\n  containers:\n  - name: busybox-cnt01\n    image: busybox:1.28\n    resources:\n      limits:\n        memory: \"300Mi\"\n      requests:\n        memory: \"100Mi\"\n\n```"}
{"prompt": "Write YAML that creates a Kubernetes Pod named \"constraints-cpu-demo-3\" with a single container named \"constraints-cpu-demo-3-ctr\" running the nginx image. The Pod has CPU resource constraints specified with a limit of 800 milliCPU and a request of 100 milliCPU. These constraints define the maximum and minimum amount of CPU resources that the container can use, respectively.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: constraints-cpu-demo-3\nspec:\n  containers:\n  - name: constraints-cpu-demo-3-ctr\n    image: nginx\n    resources:\n      limits:\n        cpu: \"800m\"\n      requests:\n        cpu: \"100m\"\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"default-mem-demo\" with one container. The container is named \"default-mem-demo-ctr\" and it uses the nginx image.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: default-mem-demo\nspec:\n  containers:\n  - name: default-mem-demo-ctr\n    image: nginx\n\n```"}
{"prompt": "Write YAML that defines a resource quota named \"pod-demo\" with a limit of two pods. This YAML code belongs to the v1 API version and represents a Kubernetes resource type called ResourceQuota. The hard section under the spec field specifies the maximum number of pods allowed for the defined quota.", "completion": "```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: pod-demo\nspec:\n  hard:\n    pods: \"2\"\n\n```"}
{"prompt": "Write YAML that defines a PersistentVolumeClaim object with the name \"pvc-quota-demo-2\". The storage class name is set to \"manual\", and it specifies that the volume can be accessed in read-write mode, but only by one pod at a time. The claim requests a storage size of 4 gigabytes.", "completion": "```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-quota-demo-2\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 4Gi\n\n```"}
{"prompt": "Write YAML that defines a PersistentVolumeClaim named \"pvc-limit-lower\". It specifies that the claim requires ReadWriteOnce access mode. The claim also requests 500Mi of storage.", "completion": "```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-limit-lower\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 500Mi\n\n```"}
{"prompt": "Write YAML that defines a Pod called \"quota-mem-cpu-demo\" with one container named \"quota-mem-cpu-demo-ctr\" running the nginx image. The Pod has resource limits set to maximum memory of 800Mi and maximum CPU usage of 800m, and resource requests set to minimum memory of 600Mi and minimum CPU usage of 400m.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: quota-mem-cpu-demo\nspec:\n  containers:\n  - name: quota-mem-cpu-demo-ctr\n    image: nginx\n    resources:\n      limits:\n        memory: \"800Mi\"\n        cpu: \"800m\"\n      requests:\n        memory: \"600Mi\"\n        cpu: \"400m\"\n\n```"}
{"prompt": "Write YAML that defines a Pod with the name \"constraints-mem-demo-4\" and the image \"nginx\". This Pod does not have any constraints set for memory or any other resources. It belongs to the \"v1\" API version and is of kind \"Pod\". The YAML is formatted in a hierarchical manner, with metadata and spec being the top-level fields.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: constraints-mem-demo-4\nspec:\n  containers:\n  - name: constraints-mem-demo-4-ctr\n    image: nginx\n\n\n```"}
{"prompt": "Write YAML that specifies a Kubernetes Pod named \"constraints-mem-demo\" with a container named \"constraints-mem-demo-ctr\" and an image of nginx. The Pod's container is assigned memory limits of 800Mi and memory requests of 600Mi.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: constraints-mem-demo\nspec:\n  containers:\n  - name: constraints-mem-demo-ctr\n    image: nginx\n    resources:\n      limits:\n        memory: \"800Mi\"\n      requests:\n        memory: \"600Mi\"\n\n```"}
{"prompt": "Write YAML that defines a ClusterRole named \"system:kube-scheduler\" with auto updating enabled and labeled as \"rbac-defaults\". The ClusterRole has three rules: 1) allow creating leases in the coordination.k8s.io API group, 2) allow getting and updating leases with the names \"kube-scheduler\" and \"my-scheduler\" in the coordination.k8s.io API group, and 3) allow deleting, getting, patching, and updating endpoints in the default API group with the names \"kube-scheduler\" and \"my-scheduler\".", "completion": "```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:kube-scheduler\nrules:\n  - apiGroups:\n      - coordination.k8s.io\n    resources:\n      - leases\n    verbs:\n      - create\n  - apiGroups:\n      - coordination.k8s.io\n    resourceNames:\n      - kube-scheduler\n      - my-scheduler\n    resources:\n      - leases\n    verbs:\n      - get\n      - update\n  - apiGroups:\n      - \"\"\n    resourceNames:\n      - kube-scheduler\n      - my-scheduler\n    resources:\n      - endpoints\n    verbs:\n      - delete\n      - get\n      - patch\n      - update\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod named \"annotation-second-scheduler\" with the label \"multischeduler-example\". The Pod has a spec with a desired scheduler named \"my-scheduler\" and a single container named \"pod-with-second-annotation-container\" running the \"pause:2.0\" image from the k8s.io registry.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: annotation-second-scheduler\n  labels:\n    name: multischeduler-example\nspec:\n  schedulerName: my-scheduler\n  containers:\n  - name: pod-with-second-annotation-container\n    image: registry.k8s.io/pause:2.0\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod called \"no-annotation\" with the label \"multischeduler-example\". The Pod has a single container named \"pod-with-no-annotation-container\" which uses the \"registry.k8s.io/pause:2.0\" image.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: no-annotation\n  labels:\n    name: multischeduler-example\nspec:\n  containers:\n  - name: pod-with-no-annotation-container\n    image: registry.k8s.io/pause:2.0\n```"}
{"prompt": "Write YAML that creates a ServiceAccount named \"my-scheduler\" in the \"kube-system\" namespace. \n\nWrite YAML that creates a ClusterRoleBinding named \"my-scheduler-as-kube-scheduler\" which binds the \"my-scheduler\" ServiceAccount to the \"system:kube-scheduler\" ClusterRole in the \"kube-system\" namespace. \n\nWrite YAML that creates a ConfigMap named \"my-scheduler-config\" with a data field containing a YAML configuration for the KubeScheduler, and also creates a Deployment named \"my-scheduler\" in the \"kube-system\" namespace, using the \"my-scheduler\" ServiceAccount, and mounting the \"my-scheduler-config\" ConfigMap as a volume in the container.", "completion": "```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-scheduler\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: my-scheduler-as-kube-scheduler\nsubjects:\n- kind: ServiceAccount\n  name: my-scheduler\n  namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:kube-scheduler\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: my-scheduler-as-volume-scheduler\nsubjects:\n- kind: ServiceAccount\n  name: my-scheduler\n  namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:volume-scheduler\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: my-scheduler-extension-apiserver-authentication-reader\n  namespace: kube-system\nroleRef:\n  kind: Role\n  name: extension-apiserver-authentication-reader\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: my-scheduler\n  namespace: kube-system\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-scheduler-config\n  namespace: kube-system\ndata:\n  my-scheduler-config.yaml: |\n    apiVersion: kubescheduler.config.k8s.io/v1beta2\n    kind: KubeSchedulerConfiguration\n    profiles:\n      - schedulerName: my-scheduler\n    leaderElection:\n      leaderElect: false\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: scheduler\n    tier: control-plane\n  name: my-scheduler\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      component: scheduler\n      tier: control-plane\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: scheduler\n        tier: control-plane\n        version: second\n    spec:\n      serviceAccountName: my-scheduler\n      containers:\n      - command:\n        - /usr/local/bin/kube-scheduler\n        - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml\n        image: gcr.io/my-gcp-project/my-kube-scheduler:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTPS\n          initialDelaySeconds: 15\n        name: kube-second-scheduler\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTPS\n        resources:\n          requests:\n            cpu: '0.1'\n        securityContext:\n          privileged: false\n        volumeMounts:\n          - name: config-volume\n            mountPath: /etc/kubernetes/my-scheduler\n      hostNetwork: false\n      hostPID: false\n      volumes:\n        - name: config-volume\n          configMap:\n            name: my-scheduler-config\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod resource with the API version set to \"v1\" and the kind set to \"Pod\". The metadata section includes the name of the Pod and labels for identification. The specification section includes the scheduler name and a container with a given name and image.", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: annotation-default-scheduler\n  labels:\n    name: multischeduler-example\nspec:\n  schedulerName: default-scheduler\n  containers:\n  - name: pod-with-default-annotation-container\n    image: registry.k8s.io/pause:2.0\n\n```"}
{"prompt": "Write YAML that defines a ServiceAccount called \"kube-dns-autoscaler\" in the \"kube-system\" namespace. \nWrite YAML that defines a ClusterRole called \"system:kube-dns-autoscaler\" with specific rules for accessing resources. \nWrite YAML that deploys a Deployment called \"kube-dns-autoscaler\" in the \"kube-system\" namespace, with a container running the \"cluster-proportional-autoscaler\" image and specific resource requests, command line arguments, and tolerations.", "completion": "```yaml\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: kube-dns-autoscaler\n  namespace: kube-system\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:kube-dns-autoscaler\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"replicationcontrollers/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments/scale\", \"replicasets/scale\"]\n    verbs: [\"get\", \"update\"]\n# Remove the configmaps rule once below issue is fixed:\n# kubernetes-incubator/cluster-proportional-autoscaler#16\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\", \"create\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:kube-dns-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: kube-dns-autoscaler\n    namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:kube-dns-autoscaler\n  apiGroup: rbac.authorization.k8s.io\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-dns-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns-autoscaler\n    kubernetes.io/cluster-service: \"true\"\nspec:\n  selector:\n    matchLabels:\n      k8s-app: kube-dns-autoscaler\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns-autoscaler\n    spec:\n      priorityClassName: system-cluster-critical\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n        supplementalGroups: [ 65534 ]\n        fsGroup: 65534\n      nodeSelector:\n        kubernetes.io/os: linux\n      containers:\n      - name: autoscaler\n        image: registry.k8s.io/cpa/cluster-proportional-autoscaler:1.8.4\n        resources:\n            requests:\n                cpu: \"20m\"\n                memory: \"10Mi\"\n        command:\n          - /cluster-proportional-autoscaler\n          - --namespace=kube-system\n          - --configmap=kube-dns-autoscaler\n          # Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base\n          - --target=<SCALE_TARGET>\n          # When cluster is using large nodes(with more cores), \"coresPerReplica\" should dominate.\n          # If using small nodes, \"nodesPerReplica\" should dominate.\n          - --default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true,\"includeUnschedulableNodes\":true}}\n          - --logtostderr=true\n          - --v=2\n      tolerations:\n      - key: \"CriticalAddonsOnly\"\n        operator: \"Exists\"\n      serviceAccountName: kube-dns-autoscaler\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Pod resource named \"busybox\" in the \"default\" namespace. The Pod contains a single container named \"busybox\" with the image \"busybox:1.28\". The container has a command to sleep for 3600 seconds and has an image pull policy of \"IfNotPresent\". The Pod has a restart policy of \"Always\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n  restartPolicy: Always\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"dnsutils\" in the \"default\" namespace. The Pod runs a container named \"dnsutils\" with the image \"registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3\". The container's command is set to \"sleep infinity\" and the image pull policy is set to \"IfNotPresent\". The restart policy for the Pod is set to \"Always\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dnsutils\n  namespace: default\nspec:\n  containers:\n  - name: dnsutils\n    image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3\n    command:\n      - sleep\n      - \"infinity\"\n    imagePullPolicy: IfNotPresent\n  restartPolicy: Always\n\n```"}
{"prompt": "Write YAML that defines a Pod named \"konnectivity-server\" in the \"kube-system\" namespace. The Pod has a priority class set to \"system-cluster-critical\" and is configured to use the host network. It contains a single container named \"konnectivity-server-container\" with a specified image and command. The container has liveness probes configured and exposes ports 8132, 8133, and 8134. It also mounts three volumes: \"k8s-certs\", \"kubeconfig\", and \"konnectivity-uds\".", "completion": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: konnectivity-server\n  namespace: kube-system\nspec:\n  priorityClassName: system-cluster-critical\n  hostNetwork: true\n  containers:\n  - name: konnectivity-server-container\n    image: registry.k8s.io/kas-network-proxy/proxy-server:v0.0.37\n    command: [\"/proxy-server\"]\n    args: [\n            \"--logtostderr=true\",\n            # This needs to be consistent with the value set in egressSelectorConfiguration.\n            \"--uds-name=/etc/kubernetes/konnectivity-server/konnectivity-server.socket\",\n            \"--delete-existing-uds-file\",\n            # The following two lines assume the Konnectivity server is\n            # deployed on the same machine as the apiserver, and the certs and\n            # key of the API Server are at the specified location.\n            \"--cluster-cert=/etc/kubernetes/pki/apiserver.crt\",\n            \"--cluster-key=/etc/kubernetes/pki/apiserver.key\",\n            # This needs to be consistent with the value set in egressSelectorConfiguration.\n            \"--mode=grpc\",\n            \"--server-port=0\",\n            \"--agent-port=8132\",\n            \"--admin-port=8133\",\n            \"--health-port=8134\",\n            \"--agent-namespace=kube-system\",\n            \"--agent-service-account=konnectivity-agent\",\n            \"--kubeconfig=/etc/kubernetes/konnectivity-server.conf\",\n            \"--authentication-audience=system:konnectivity-server\"\n            ]\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        host: 127.0.0.1\n        port: 8134\n        path: /healthz\n      initialDelaySeconds: 30\n      timeoutSeconds: 60\n    ports:\n    - name: agentport\n      containerPort: 8132\n      hostPort: 8132\n    - name: adminport\n      containerPort: 8133\n      hostPort: 8133\n    - name: healthport\n      containerPort: 8134\n      hostPort: 8134\n    volumeMounts:\n    - name: k8s-certs\n      mountPath: /etc/kubernetes/pki\n      readOnly: true\n    - name: kubeconfig\n      mountPath: /etc/kubernetes/konnectivity-server.conf\n      readOnly: true\n    - name: konnectivity-uds\n      mountPath: /etc/kubernetes/konnectivity-server\n      readOnly: false\n  volumes:\n  - name: k8s-certs\n    hostPath:\n      path: /etc/kubernetes/pki\n  - name: kubeconfig\n    hostPath:\n      path: /etc/kubernetes/konnectivity-server.conf\n      type: FileOrCreate\n  - name: konnectivity-uds\n    hostPath:\n      path: /etc/kubernetes/konnectivity-server\n      type: DirectoryOrCreate\n\n```"}
{"prompt": "Write YAML that specifies the API version being used is v1beta1 and the kind of resource being defined is EgressSelectorConfiguration. The configuration includes an egressSelections field with one egress selection named \"cluster\". The egress selection includes a connection field specifying that the proxy protocol is GRPC and the transport is UDS. The UDS configuration includes the udsName field specifying the UDS socket location.", "completion": "```yaml\napiVersion: apiserver.k8s.io/v1beta1\nkind: EgressSelectorConfiguration\negressSelections:\n# Since we want to control the egress traffic to the cluster, we use the\n# \"cluster\" as the name. Other supported values are \"etcd\", and \"controlplane\".\n- name: cluster\n  connection:\n    # This controls the protocol between the API Server and the Konnectivity\n    # server. Supported values are \"GRPC\" and \"HTTPConnect\". There is no\n    # end user visible difference between the two modes. You need to set the\n    # Konnectivity server to work in the same mode.\n    proxyProtocol: GRPC\n    transport:\n      # This controls what transport the API Server uses to communicate with the\n      # Konnectivity server. UDS is recommended if the Konnectivity server\n      # locates on the same machine as the API Server. You need to configure the\n      # Konnectivity server to listen on the same UDS socket.\n      # The other supported transport is \"tcp\". You will need to set up TLS \n      # config to secure the TCP transport.\n      uds:\n        udsName: /etc/kubernetes/konnectivity-server/konnectivity-server.socket\n\n```"}
{"prompt": "Write YAML that creates a ClusterRoleBinding named \"system:konnectivity-server\" with labels \"kubernetes.io/cluster-service: true\" and \"addonmanager.kubernetes.io/mode: Reconcile\". The ClusterRoleBinding should have a role reference to \"system:auth-delegator\" in the \"rbac.authorization.k8s.io\" API group and a subject with the kind \"User\" and the name \"system:konnectivity-server\" in the \"rbac.authorization.k8s.io\" API group. Additionally, write YAML that creates a ServiceAccount named \"konnectivity-agent\" in the \"kube-system\" namespace with labels \"kubernetes.io/cluster-service: true\" and \"addonmanager.kubernetes.io/mode: Reconcile\".", "completion": "```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:konnectivity-server\n  labels:\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: User\n    name: system:konnectivity-server\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: konnectivity-agent\n  namespace: kube-system\n  labels:\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n\n```"}
{"prompt": "Write YAML that defines a DaemonSet named \"konnectivity-agent\" in the \"kube-system\" namespace. The DaemonSet runs a container with the image \"us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent:v0.0.37\" and sets its command and arguments. It also mounts a volume for a service account token, configures a liveness probe, and assigns a service account named \"konnectivity-agent\" to the DaemonSet.\n\nThis YAML ensures that the \"konnectivity-agent\" container is deployed to every node in the cluster, as specified by the DaemonSet kind. It also sets various parameters for the container, such as the proxy server host and port, admin server port, and health server port. The use of a liveness probe ensures that the container's health is continuously monitored, and the service account allows the container to access necessary resources.", "completion": "```yaml\napiVersion: apps/v1\n# Alternatively, you can deploy the agents as Deployments. It is not necessary\n# to have an agent on each node.\nkind: DaemonSet\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    k8s-app: konnectivity-agent\n  namespace: kube-system\n  name: konnectivity-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: konnectivity-agent\n  template:\n    metadata:\n      labels:\n        k8s-app: konnectivity-agent\n    spec:\n      priorityClassName: system-cluster-critical\n      tolerations:\n        - key: \"CriticalAddonsOnly\"\n          operator: \"Exists\"\n      containers:\n        - image: us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent:v0.0.37\n          name: konnectivity-agent\n          command: [\"/proxy-agent\"]\n          args: [\n                  \"--logtostderr=true\",\n                  \"--ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\n                  # Since the konnectivity server runs with hostNetwork=true,\n                  # this is the IP address of the master machine.\n                  \"--proxy-server-host=35.225.206.7\",\n                  \"--proxy-server-port=8132\",\n                  \"--admin-server-port=8133\",\n                  \"--health-server-port=8134\",\n                  \"--service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token\"\n                  ]\n          volumeMounts:\n            - mountPath: /var/run/secrets/tokens\n              name: konnectivity-agent-token\n          livenessProbe:\n            httpGet:\n              port: 8134\n              path: /healthz\n            initialDelaySeconds: 15\n            timeoutSeconds: 15\n      serviceAccountName: konnectivity-agent\n      volumes:\n        - name: konnectivity-agent-token\n          projected:\n            sources:\n              - serviceAccountToken:\n                  path: konnectivity-agent-token\n                  audience: system:konnectivity-server\n\n```"}
{"prompt": "Write YAML that defines a DaemonSet named \"ssd-driver\" with the app label set to nginx. The DaemonSet consists of a template that includes a container named \"example-container\" running the \"example-image\". The template is configured to run only on nodes with the ssd label set to \"true\".", "completion": "```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ssd-driver\n  labels:\n    app: nginx\nspec:\n  selector:\n    matchLabels:\n      app: ssd-driver-pod\n  template:\n    metadata:\n      labels:\n        app: ssd-driver-pod\n    spec:\n      nodeSelector:\n        ssd: \"true\"\n      containers:\n        - name: example-container\n          image: example-image\n```"}
{"prompt": "Write YAML that creates a DaemonSet named \"fluentd-elasticsearch\" in the \"kube-system\" namespace with labels \"k8s-app: fluentd-logging\". The DaemonSet uses a RollingUpdate update strategy with a maxUnavailable of 1. The DaemonSet's template specifies tolerations for control plane nodes and master nodes, and includes a container named \"fluentd-elasticsearch\" using the image \"quay.io/fluentd_elasticsearch/fluentd:v2.5.2\". The container has volume mounts for \"/var/log\" and \"/var/lib/docker/containers\" and is set to read-only for the latter. The template also defines terminationGracePeriodSeconds of 30 and two volumes, one for \"/var/log\" and another for \"/var/lib/docker/containers\" using hostPath.", "completion": "```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd-elasticsearch\n  namespace: kube-system\n  labels:\n    k8s-app: fluentd-logging\nspec:\n  selector:\n    matchLabels:\n      name: fluentd-elasticsearch\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        name: fluentd-elasticsearch\n    spec:\n      tolerations:\n      # these tolerations are to have the daemonset runnable on control plane nodes\n      # remove them if your control plane nodes should not run pods\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n        effect: NoSchedule\n      containers:\n      - name: fluentd-elasticsearch\n        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n\n```"}
{"prompt": "Write YAML that defines a Kubernetes Job called \"pi\" in the `batch/v1` API version. The Job will run a Perl container with the image `perl:5.34.0`, executing the command `perl -Mbignum=bpi -wle 'print bpi(2000)'`. The Job has a restart policy of \"Never\" and a backoff limit of 4.", "completion": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  template:\n    spec:\n      containers:\n      - name: pi\n        image: perl:5.34.0\n        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n  backoffLimit: 4\n\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Deployment resource called \"nginx-deployment\" with 3 replicas. It selects pods with the label \"app: nginx\" and creates a pod template with a container named \"nginx\" using the image \"nginx:1.14.2\" on port 80.", "completion": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that defines a DaemonSet resource called \"fluentd-elasticsearch\" in the \"kube-system\" namespace with labels \"k8s-app: fluentd-logging\". It specifies a selector to match labels with the name \"fluentd-elasticsearch\" and a template for pods with the same labels. The pods have tolerations for control plane and master nodes, use the \"fluentd_elasticsearch/fluentd:v2.5.2\" image, resource limits and requests, volume mounts for \"/var/log\", and a termination grace period of 30 seconds. The pods also have a volume named \"varlog\" which is a host path to \"/var/log\".", "completion": "```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd-elasticsearch\n  namespace: kube-system\n  labels:\n    k8s-app: fluentd-logging\nspec:\n  selector:\n    matchLabels:\n      name: fluentd-elasticsearch\n  template:\n    metadata:\n      labels:\n        name: fluentd-elasticsearch\n    spec:\n      tolerations:\n      # these tolerations are to have the daemonset runnable on control plane nodes\n      # remove them if your control plane nodes should not run pods\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n        effect: NoSchedule\n      containers:\n      - name: fluentd-elasticsearch\n        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n\n```"}
{"prompt": "Write YAML that defines a ReplicationController named \"nginx\" with 3 replicas. The ReplicationController selects pods with the label \"app: nginx\". The pods are defined with a container named \"nginx\" running the nginx image on port 80.", "completion": "```yaml\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    app: nginx\n  template:\n    metadata:\n      name: nginx\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that describes a Kubernetes Job named \"job-pod-failure-policy-config-issue\" with a target of completing 8 instances and running 2 instances in parallel. The Job is defined to have a restart policy of \"Never\". It also has a container named \"main\" that uses an image from a non-existing repository. In case of pod failures, the Job will fail after 6 retries if the pod condition is a configuration issue.", "completion": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-pod-failure-policy-config-issue\nspec:\n  completions: 8\n  parallelism: 2\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: main\n        image: \"non-existing-repo/non-existing-image:example\"\n  backoffLimit: 6\n  podFailurePolicy:\n    rules:\n    - action: FailJob\n      onPodConditions:\n      - type: ConfigIssue\n\n```"}
{"prompt": "Write YAML that specifies the API version and kind as \"autoscaling/v1\" and \"HorizontalPodAutoscaler\" respectively. The metadata section includes the name of the HPA as \"frontend-scaler\". The spec section defines the scale target reference as a ReplicaSet with the name \"frontend\", sets the minimum replicas to 3, maximum replicas to 10, and target CPU utilization percentage to 50.", "completion": "```yaml\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: frontend-scaler\nspec:\n  scaleTargetRef:\n    kind: ReplicaSet\n    name: frontend\n  minReplicas: 3\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 50\n\n```"}
{"prompt": "Write YAML that describes a ReplicaSet resource named \"frontend\" with labels \"app: guestbook\" and \"tier: frontend\". The ReplicaSet should have 3 replicas and should select pods with the label \"tier: frontend\". The pods created by the ReplicaSet should have a container named \"php-redis\" with the image \"gcr.io/google_samples/gb-frontend:v3\".", "completion": "```yaml\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\nspec:\n  # modify replicas according to your case\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n\n```"}
{"prompt": "Write YAML that defines a Job object with the following properties: completions set to 12, parallelism set to 3, and restart policy set to Never. The Job runs a container named \"main\" with the image \"docker.io/library/bash:5\" and a command \"bash\". The container has an example command that triggers the \"FailJob\" action when it exits with an exit code of 42. The Job has a backoff limit of 6 and a pod failure policy that specifies failure action for a specific container exit code and disruption target.", "completion": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-pod-failure-policy-example\nspec:\n  completions: 12\n  parallelism: 3\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: main\n        image: docker.io/library/bash:5\n        command: [\"bash\"]        # example command simulating a bug which triggers the FailJob action\n        args:\n        - -c\n        - echo \"Hello world!\" && sleep 5 && exit 42\n  backoffLimit: 6\n  podFailurePolicy:\n    rules:\n    - action: FailJob\n      onExitCodes:\n        containerName: main      # optional\n        operator: In             # one of: In, NotIn\n        values: [42]\n    - action: Ignore             # one of: Ignore, FailJob, Count\n      onPodConditions:\n      - type: DisruptionTarget   # indicates Pod disruption\n\n```"}
{"prompt": "Write YAML that defines a Job resource with the apiVersion 'batch/v1' and the kind 'Job'. The Job will have a name of 'job-pod-failure-policy-ignore' and it will have 4 completions and a parallelism of 2. Inside the template, the restart policy is set to 'Never' and there is a container named 'main' that uses the 'docker.io/library/bash:5' image. The container's command is set to ['bash'] and the args specify to echo \"Hello world! I'm going to exit with 0 (success).\" sleep for 90 seconds, and then exit with a status code of 0. The backoffLimit is set to 0 and the podFailurePolicy is set to 'Ignore' with the condition type as 'DisruptionTarget'.", "completion": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-pod-failure-policy-ignore\nspec:\n  completions: 4\n  parallelism: 2\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: main\n        image: docker.io/library/bash:5\n        command: [\"bash\"]\n        args:\n        - -c\n        - echo \"Hello world! I'm going to exit with 0 (success).\" && sleep 90 && exit 0\n  backoffLimit: 0\n  podFailurePolicy:\n    rules:\n    - action: Ignore\n      onPodConditions:\n      - type: DisruptionTarget\n\n```"}
{"prompt": "Write YAML that defines a DaemonSet object named \"fluentd-elasticsearch\" in the \"kube-system\" namespace. It has labels and a selector to match containers with the name \"fluentd-elasticsearch\". The update strategy is set to RollingUpdate with maxUnavailable set to 1. The DaemonSet includes a pod template with tolerations for control plane nodes, a container named \"fluentd-elasticsearch\" using an image from quay.io, resource limits and requests, volume mounts for /var/log and /var/lib/docker/containers, and a termination grace period of 30 seconds.", "completion": "```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd-elasticsearch\n  namespace: kube-system\n  labels:\n    k8s-app: fluentd-logging\nspec:\n  selector:\n    matchLabels:\n      name: fluentd-elasticsearch\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        name: fluentd-elasticsearch\n    spec:\n      tolerations:\n      # these tolerations are to have the daemonset runnable on control plane nodes\n      # remove them if your control plane nodes should not run pods\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n        effect: NoSchedule\n      containers:\n      - name: fluentd-elasticsearch\n        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n\n```"}
{"prompt": "Write YAML that creates a ReplicaSet object in the apps/v1 API version. The ReplicaSet will have the name \"my-repset\" and have three replicas. It will select pods with the label \"pod-is-for: garbage-collection-example\" and create a pod template with the same label. The pod template will have a single container named \"nginx\" using the nginx image.", "completion": "```yaml\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: my-repset\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      pod-is-for: garbage-collection-example\n  template:\n    metadata:\n      labels:\n        pod-is-for: garbage-collection-example\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n\n```"}
{"prompt": "Write YAML that creates a ReplicationController named \"my-nginx-v4\" with 5 replicas. The ReplicationController will select pods with the labels \"app: nginx\" and \"deployment: v4\" and use a template with the same labels. Inside the template, there is a container named \"nginx\" with the image \"nginx:1.16.1\", running the command \"nginx -T\", and exposing port 80.", "completion": "```yaml\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: my-nginx-v4\nspec:\n  replicas: 5\n  selector:\n    app: nginx\n    deployment: v4\n  template:\n    metadata:\n      labels:\n        app: nginx\n        deployment: v4\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.16.1\n        args: [\"nginx\", \"-T\"]\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that defines a ReplicationController named \"my-nginx\" with 5 replicas. The ReplicationController will create pods using the specified template metadata and spec. The template defines a pod with the label \"app: nginx\" and a single container named \"nginx\" using the image nginx:1.14.2, exposing port 80.", "completion": "```yaml\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: my-nginx\nspec:\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n\n```"}
{"prompt": "Write YAML that creates a Job resource with 8 completions and a maximum of 2 parallel pods. The Job uses a container named \"main\" with the image \"docker.io/library/bash:5\" and runs a bash command that echoes a message, sleeps for 30 seconds, and exits with the code 42 to simulate a software bug. The podFailurePolicy states that if the container named \"main\" exits with an exit code of 42, the Job should fail.", "completion": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-pod-failure-policy-failjob\nspec:\n  completions: 8\n  parallelism: 2\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: main\n        image: docker.io/library/bash:5\n        command: [\"bash\"]\n        args:\n        - -c\n        - echo \"Hello world! I'm going to exit with 42 to simulate a software bug.\" && sleep 30 && exit 42\n  backoffLimit: 6\n  podFailurePolicy:\n    rules:\n    - action: FailJob\n      onExitCodes:\n        containerName: main\n        operator: In\n        values: [42]\n\n```"}
