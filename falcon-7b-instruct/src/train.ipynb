{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e478fa-d095-4145-9bd1-b4feec7bc4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6e5c22ad344dedbd2560125947b673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('tiiuae/falcon-7b-instruct')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1182e48e0246938a94490c05eac43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/model/saved/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, load_in_8bit=True, device_map={\"\":0}, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e1795-c783-4ddf-999e-f1de19258928",
   "metadata": {},
   "source": [
    "Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fafd16b-d8c9-47bf-9116-c27b1d43a019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-6ffb87a3d7edc3ab/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339c4c879e9b4af3a8f3e54d2f8ed3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "data_path = os.environ.get('DATA_PATH', \"../sample-data/k8s-instructions.jsonl\")\n",
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "525364c4-3e46-467e-b6ff-c8838fdb6671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 6,924,080,000 || trainable%: 0.03407378308742822\n"
     ]
    }
   ],
   "source": [
    "lora_config2 = LoraConfig(\n",
    " r=8,\n",
    " lora_alpha=32,\n",
    " target_modules=[\"query_key_value\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config2)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec8a1a9f-fe60-49c7-ab20-04034323df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt(instruction, output):\n",
    "    prompt = \"{0}\\n\\n{1}\\n{2}\\n\\n{3}\\n{4}\".format(\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n",
    "        \"### Instruction:\",\n",
    "        instruction,\n",
    "        \"### Response:\",\n",
    "        output\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5dd944b-e2bd-4bfd-a5fa-55bc90239926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 4\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenizing: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 4\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(data)\n",
    "data = data.map(lambda x: tokenizer(prompt(x[\"prompt\"], x[\"completion\"]),\n",
    "    max_length=1000, padding=True, truncation=True))\n",
    "print(\"After tokenizing:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b33e407a-9d4f-49f6-a74b-b80db8cc3a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.961900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.964500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"./outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"/model/trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dea4e68e-57a7-48bd-bad9-f03dfe3f8a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "total 9.1M\n",
      "4.0K drwxr-xr-x 1 root root 4.0K Jun 19 02:57 .\n",
      "8.0K drwxr-xr-x 1 root root 4.0K Jun 19 02:32 ..\n",
      "4.0K -rw-r--r-- 1 root root   27 Jun 19 03:00 README.md\n",
      "4.0K -rw-r--r-- 1 root root  406 Jun 19 03:00 adapter_config.json\n",
      "9.1M -rw-r--r-- 1 root root 9.1M Jun 19 03:00 adapter_model.bin\n",
      "4.0K -rw-r--r-- 1 root root 3.9K Jun 19 03:00 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "! ls -lash /model/trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78f67e37-4035-40ea-89a5-7570cae0b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "total 9.1M\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Jun 19 03:04 .\n",
      "8.0K drwxr-xr-x 1 root root 4.0K Jun 19 03:04 ..\n",
      "4.0K -rw-r--r-- 1 root root   27 Jun 19 03:04 README.md\n",
      "4.0K -rw-r--r-- 1 root root  406 Jun 19 03:04 adapter_config.json\n",
      "9.1M -rw-r--r-- 1 root root 9.1M Jun 19 03:04 adapter_model.bin\n",
      "4.0K -rw-r--r-- 1 root root 3.9K Jun 19 03:04 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "! ls -lash /model/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3b6511f-b8a5-4d2a-b61e-010c931dae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{\n",
      "  \"base_model_name_or_path\": \"/model/saved/\",\n",
      "  \"bias\": \"none\",\n",
      "  \"fan_in_fan_out\": false,\n",
      "  \"inference_mode\": true,\n",
      "  \"init_lora_weights\": true,\n",
      "  \"layers_pattern\": null,\n",
      "  \"layers_to_transform\": null,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dropout\": 0.05,\n",
      "  \"modules_to_save\": null,\n",
      "  \"peft_type\": \"LORA\",\n",
      "  \"r\": 8,\n",
      "  \"revision\": null,\n",
      "  \"target_modules\": [\n",
      "    \"query_key_value\"\n",
      "  ],\n",
      "  \"task_type\": \"CAUSAL_LM\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat /model/trainer/adapter_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "376294d6-e5c9-4cde-8d2f-8eafbbecdc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write the YAML files to deploy a docker registry on K8s\n",
      "\n",
      "To deploy a docker registry on k8s, you can follow these steps:\n",
      "\n",
      "1. Install the docker registry on the k8s cluster. You can use the official docker registry image or install it manually.\n",
      "\n",
      "2. Create a Kubernetes deployment for the registry. You can use the following YAML file to create the deployment:\n",
      "\n",
      "```\n",
      "apiVersion: v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: registry\n",
      "spec:\n",
      "  containers:\n",
      "  - name: registry\n",
      "    image: docker.io/registry:3.1.1\n",
      "    ports:\n",
      "    - containerPort: 5000\n",
      "  volumes:\n",
      "  - name: registry-data\n",
      "    path: /data/registry\n",
      "```\n",
      "\n",
      "This YAML file creates a deployment for the registry and maps the container port 5000 to the host port 5000.\n",
      "\n",
      "3. Create a Kubernetes service for the registry. You can use the following YAML file to create the service:\n",
      "\n",
      "```\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: registry\n",
      "spec:\n",
      "  selector:\n",
      "    app: registry\n",
      "  ports:\n",
      "  - name: registry\n",
      "    port: 5000\n",
      "  type: ClusterIP\n",
      "```\n",
      "\n",
      "This YAML file creates a service for the registry and maps the container port 5000 to the host port 5000.\n",
      "\n",
      "4. Create a Kubernetes secret for the registry. You can use the following YAML file to create the secret:\n",
      "\n",
      "```\n",
      "apiVersion: v1\n",
      "kind: Secret\n",
      "metadata:\n",
      "  name: registry-data\n",
      "data:\n",
      "  registry.yml:\n",
      "    registry.yml: |\n",
      "      username: registry\n",
      "      password: registry\n",
      "      host: registry.example.com\n",
      "      port: 5000\n",
      "```\n",
      "\n",
      "This YAML file creates a secret for the registry and maps the registry.yml file to the host registry.yml file.\n",
      "\n",
      "5. Create a Kubernetes deployment for the registry secret. You can use the following YAML file to create the deployment:\n",
      "\n",
      "```\n",
      "apiVersion: v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: registry-secret\n",
      "spec:\n",
      "  containers:\n",
      "  - name: registry-secret\n",
      "    image: registry.example.com/registry:3.1.1\n",
      "    ports:\n",
      "    - containerPort: 5000\n",
      "  volumes:\n",
      "  - name: registry-secret\n",
      "    path: /data/registry\n",
      "```\n",
      "\n",
      "This YAML file creates a deployment for the registry secret and maps the container port 5000 to the host port 5000.\n",
      "\n",
      "6. Create a Kubernetes service for the registry secret. You can use the following YAML file to create the service:\n",
      "\n",
      "```\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: registry-secret\n",
      "spec:\n",
      "  selector:\n",
      "    app: registry\n",
      "  ports:\n",
      "  - name: registry\n",
      "    port: 5000\n",
      "  type\n"
     ]
    }
   ],
   "source": [
    "#text = \">>QUESTION<<\\nWrite the YAML files to deploy a docker registry on K8s\\n>>ANSWER<<\\n\"\n",
    "#device = \"cuda:0\"\n",
    "\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "# inputs.pop(\"token_type_ids\")\n",
    "# outputs = model.generate(**inputs, max_new_tokens=200, top_p=0.95, top_k=50, temperature=0.8)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1053f3a-08e9-4bff-80c9-9c55b3727e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs  requirements.txt\tsample-data  save  saved  scripts  src\ttrained\n"
     ]
    }
   ],
   "source": [
    "! ls /model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ae9ce-261e-48e4-904d-7c3d2948308e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
